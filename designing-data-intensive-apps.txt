


Designing Data-Intensive Applications by Martin Kleppmann

(DIA) data intensive applications := the main challenge is the complexity of data that is processed

recent trends (between 2000-2020)
    - big data volumes
    - services supposed to be highly available
    - machines didn't become faster; but number of cpu cores increased and networks are faster -> more parallel processing.
    - distributed build systems (thanks to infrastructure as a service - IaaS) 
    - open source systems prevalent (noosql and message queues are often used tools)

says he makes sense of the new world and describes lasting principles.

Part I - fundamental ideas of data intensive apps

Chapter I - analyses the relevant concerns (Reliability, Scalability, Maintainability)

Reliability (continue to work even when things go wrong); 
    expectations for software:
    - system performs expected functions
    - can tolerate errors in data/user mistakes or some degree of errors in software components (fault tolerance)
    - can function under load
    - prevent unauthorized access

    Terminology:
        fault - 'one component deviating from the spec'
        Failure - system as a whole stops functioning.

        still there are 'systemic faults' in software - when the same fault causes unrelated machines to fail 
            examples: leap second bug in linux kernel, 
            memory leak will cause unrelated machines to have OOM (out of memory)
            cascading faults (one component causes others to fault, then others and others)
            component that other component depend on fails (like DNS) - thus creating wider impact

        ? how is a systemic fault different from a system failure?'

        Says they like to do continuous testing by simulating faults in production systems (like the chaos monkey that kills random processes)

        Also says continuous monitoring is needed to identify if the system is running correctly...


Scalability  - ability of system to cope with increased load
    
    - no one fit all characteristic to describe loads; the /load parameters/ depend on a given system.
      Load parameter examples:
        throughput/requests per seconds (batch systems care about this)
        response time (interactive systems care about this)
            - here they look at the average, median and nth percentile (entry in histogram of cases per percentage of latency).
            - service level agreements may state the required median and required 99% nth  percentile.
            - /amazon optimize for 99% percentile for latency: customers who get a low latency are very valuable - because they do a lot of transactions/
        latency - (duration that a requests waits to be handled) /more of an internal characteristic/ 
        ratio of read requests vs write requests.
        hit rate on cache.  
        number of concurrent active users.

    - example how load parameters as type of measurements change with the system: 
        twitter has two basic use cases: 'post tweet' and 'show timeline for a user'
            - at start showing the timeline for a users was select in tweet table join follower table (that wasn't very scalable)
            - then they started to cache the timeline for each active user, 
              when a person posts a message the change is pushed to the timeline objects of each active subscriberd
                (now for writes we have the measure of updating the 'fan out' - followers timelines.)
            - problem: some celebrities have millions of followers, so updating the timeline of each celebrity follower is too much work; (these are exempt from fan out, for their followers they must fetch celebrity tweets when building the cached timeline)
    

    - questions to ask:
        - if you increase the load (by relevant 'load parameter') - given the same setup, how does per performance decrease?
        - if you increase the load (by relevant 'load parameter') - How many machines do you need to add to keep the same performance?
            / order of magnitude changes may require a change of architecture/

    - types of scaling:
        - scale up (get faster, more powerful machine) /costs a lot but is easier (you try to scale up database installations)/ (you scale up 'pets')
        - scale out (get more machines) /adds lots of complexity on the system and may require change of architecture!/ (you scale out 'cattle')

Maintainability - (most costs are after development while maintaining a system)
    concerns
        - Operability - measures how easy it is for operations team to keep the system running
            For monitoring/problem solving/patching/upgrading/load planning 
        - simplicity - how easy it is to understand the internals of the system?
            avoiding accidental complexity (complexity arising from implementation details and not inherent in the problem (as seen by users): 
        - evolvability - (extensibility, modifiability, plasticity)


Chapter II - data model (modelling language and query language)

        - Many apps layer one data model on top of another one (like application level data object is layered on top of infrastructure data object layered on top of the database engine, etc)
- as usual: each layer hides complexity from the next layer on top to simplify the next layer on top (the different layers don't always fit - that creates an 'impedance mismatch')

- sql: says it was successful because the programmer doesn't have to think a lot how the data is represented by DBMS (? he has when tuning for performance ?) and RDBMS do transaction isolation well!
- nosql: became popular due to a set of requirements/options 
    - need for greater scalability (as compared to RDBMS) - very large datasets and throughput rates required for something like twitter.
    - lots of open source tools and solutions
    - frustration from sql (application objects don't always map to tables) 'desire for more dynamic and expressive data models'
    - specialized query operations (that can do without transactions)
    - in some way it's the return of hierarchical and network models; greetings from IMS (says SQL/RDBMS won that war because application code was much more simple as compared to the alternatives; the RDBMS query optimizer hides complexity of dealing with records directly/access paths to a record)

- with sql you have multiple choices for representations: non-indexed fields can be 1) in separate columns 2) in special json type field 3) as json document in TEXT field.
- schemaless json in database is often regarded as 'decreasing the impedance mismatch' (OMG) , what it does give you is the ability to handle collections of fields/records (at the expense of having to do validations in the application code)
- still separating common fields into separate table and linking them with foreign key (normalization) makes a lot of sense::: (? why were they bitching about sql in the first place?)
    - for localization (can translate a table into n languages without duplicating entries)
    - ability to query fields by value
    - duplication of values makes ambiguity and is bad.
- still foreign keys introduce many-to-one relations; the need for sub queries (or joins) ->  many-to-many relations are therefore difficult to handle (and are slow compared to other approaches - all due to sub queries); so we keep bitching.

- in any event where xml/json/xaml fields are stored: you most often don't have a schema (that means validation task is on the application code)
- however the irony is that lack of schema is sited as being easier to migrate data formats (as you don't have to modify/alter the data schema) - now that comes at the expense of even more messy application code...

- makes sense to keep data that is accessed together in the same xml/json document (never mind the cost to parse the whole thing)

- relational DBMS got a JSON field (with varying level of support) so that there is some convergence between document storage and RDBMS.

remaining chapters: tools and approachesS

DIA use building blocks (application code stitches these tools together)
Increasing overlap between tools & approaches (kafka message queue does store messages persistently) or reddis (data store also used as message queue)

    - datastores (sql,nosql) for persistance
    - caches to store intermediate results
    - search indexes 
    - stream processing - (allow messages to be sent to remote processes for asynchronous handling) - like message queues
    - batch processing pipelines

Minimizing human errors:
    - 'design system as to minimize the opportunity for error' and that 'encourage/make it easy to do the right thing and discourage to do the wrong thing' (i guess that's the rational of the 'opinionated' business that one sees all over the place)
        - (implies that this leads to an restrictive approach and that by overdoing it the user will be forced to work around that; so that 'the right balance is needed)
    - isolated sandbox environment for testing and training ( "Decouple the places where people make the most mistakes from the places wherethey  can  cause  failure")
    - make it easy to recover from human errors (fast rollback of configuration and code in production environments)
    - testing (unit testing/integration testing/system testing )
    - continuous monitoring of performance/throughput/response times/error rates (makes it possible to diagnose conditions)


Chapter II - data models & query languages

Application objects and database tables often need a translation layer that translates between database row between one or multiple sql tables, and the application object; for example one to many relationships have to be looked up into an application object collection. They call these difficulties 'object-relational mismatch'
    - on the other hand: if you have a 'normalized' relational DB (meaning all common data redirected to tables without duplication ) then that has a lot of good points
        - such as avoiding ambiguities (if you duplicate the 'location name' field then what do you do with places that have the same name?)
        - better localization support (just translate the string table, but references to entries in string table stay the same)
        - queries go well because of lack of ambiguity.

Application objects map more neatly to json documents (or json fields in sql database) but now the json data is not validated against a schema (well, there is sql schema, but I guess document db's don't support them when this book was written).

Issue of schema: Either you have the DMBS validate a schema, or the validation is in the application code while inserting/updating/querying values.
(that makes this debate similar to static type checking (with schema) vs dynamic (runtime) type checking).
Each approach has its good and bad things: for example lack of schema will have more flexibility as there is 'update table'/schema change overhead (also schema migration adds to system downtime (?)); (still there is more work the 'dynamic approach' overall, if you ask me) 

SQL and many-to-many relationships: SQL has recursive syntax since SQL 1999 - recursive common table expression (but that is not quite concise)

Query language:
- SQL has declarative query language (because you specify desired result in the query).
    - declarative query language is more concise than imperative query language where the query is done in code 
    - the order of query results can come out different 
    - declarative language is done behind the scene - the database engine could run it in parallel 
  Map reduce query language - is imperative ql (does map/filter/fold with a lambda expression function without side effects (assignment) that applies the filtering/processing); (mongo db query language looks even more declarative, but that's a disguise)
    - somewhere in between with flexibility/conciseness as compared with declarative vs imperative query language
  Imperative query language:    
    - code that loops through data and checks if item meet a coded condition.`` 
    
Graph databases - all data is a set of nodes and links between them; is ok when there are lots of many-to-many relationships in the data; Is supposed to work more natural rather than putting it into relational tables.
    - graph db models (directed graph, each of them)
        - property model : each node(vertex) and each link have a unique ID and each has a set of name   /value propertiesl 
        - triple store model; each facts stored as 3-tuples: (subject, predicate, object) ; subject and obect are nodes(vertexes); predicate is a directed link.
    - some products like NEO4j have their own declarative data manipulation and query language.


To sum it up; NoSQL come as following models (both don't enforce a schema, typically)
    - document db: data comes in self contained documents and relations between documents is rare (?)
    - graph db: any node can be potentially connected with every other node.

Chapter III - storage & retrieval 

talks bout how table indexes for lookup of record in table by key field are done in RDBMS (data there is organized from a log of binary records - that makes it easier to handle crashes, also append only handles writes quickly)

    - one approach is to have a hash map for lookup (?!) 
        - works for append only (log) data; map the key to the record offset.
        - can have segments of a file (each segment with it's own hash table; so as to accommodate limited length of offset field).
        - since date is append only: on deleting a record one has to append a 'this entry is deleted' record (also called tombstone)
        - problems:
            - index must fit into memory (having on disk hash index means a lot of disk seeks, that is bad).
            - can't do range queries (only lookup of entry by key)
    - SSTables (sorted string table) 
        - divides the file into segments, where each segment has a set of sequential entries sorted by key valued.
        - again need to divide a file into segments; `
        - merging two segments into one: easy as entries are sorted; if the same key appears in both segments then take the most recent record 
            (merging of two/several segments is done by compaction process)
        - uses smaller in-memory index (than hash map); still need the first and last record of each segment file (then search the partial segment when found)
        - for the batch of least recently used entries: maintain them in memory with a BTREE index (this in-memory structure is called MEMTABLE).
        - when the MEMTABLE grows too big then write sort it and write it to disk as a segment. 
        - Problem: need to keep a log of recently used entries; so that it can recover the last records (still in memory MEMTABLE) upon crash.

    - LSM-Tree (log structured merge tree, very suitable for append-only data)
        similar to SSTables; but devides ss-tables into 'levels'. Tables from lower levels are merged into larger tables that are put into the next 'level'.
            lots of tables are ordered into 'levels' (not clear what the difference between 'size tiered' and 'level tiered' compaction is). Each level contains tables with non-overlapping key ranges.
              - size tiered compaction  
              - level tiered compaction

            optimization: add in memory bloom filter to each segment (to check if entry is in key range, as there are tables in different tiers with overlapping key ranges)
              

       - BTree indexes (most often used and well known). Each node has a set of entries for keys k(1) .... k(n); and or each key k(i) it has a pointer p(i) that points to the node that will contain all keys in the range between k(i) and k(i+1). Very suitable as index for data that is updated in-place.
             - take care to pick records size so that disk block is filled properly.
             - they make an effort to keep it balanced, but sometimes they need to split nodes (to keep it balanced). Now if it crashes then the tree is corrupted. To fix that: keep a log of pending modifications (written first before applying the change); that's a write-ahead-log (WAL); Alternatively:: copy on write - prepare a modified page with the new btree entry; then update the parent page to point to the modified page.       
             ;                                                           

    comparing btree and lsm
    - BTree is faster for read, while LSM is faster or write (while reading LSM tree has to check multiple entries)
    - lsm tree: entry is not bound to block size, therefore can be compressed better and does less fragmentation - that occurs when splitting btree nodes.
    - lsm have less 'write amplification' - (that is a lsm write turns into less writes on disk on average) (? What about compaction ?)
    - lsm compaction may not keep up with the write rate (bad) in this case reads become slow - have to check for more segments.
    - btree performance is more uniform (less spikes) as each key is in one location.

    Indexes
        - You can have a 'clustered index' In this some columns get duplicated in the btree index - for faster read access. (now that brings all the perils of duplication and makes the dmbs transaction management more complicated)
            CREATE INDEX IX_test ON table1(column1) INCLUDE (column2)
        - multi-column indexes: most often they just concatenate all the index field and search over this value (that's called concatenated index); problem: you can't do range searches (combine x and y coordinates, you can't find { (x,y) |  x1 <x < x2 && y1 < y < y2 }
                - have more specialized structures for that :: R-Tree

    Free text search
        - keep a reverse index: for each word (that is not a stop word like 'and' 'or') keep a list of the document id's that it appears in
        - allow to search for synonyms of search term (or terms within given editing distance to make up for typos)

    In memory database - is fast because doesn't need to encode the data in log format (for persistence)
       says "OLTP Through the Looking Glass, and What We Found There" Stavros Harizopoulos, Daniel J. Abadi,    Michael Stonebraker 


    OLTP (online transaction processing) vs Analytics (data warehourses).
    OLTP - needs low latency (interactive queries) and high availability.

    Analytics need to run batch queries over a wide range of records for their reports, this access pattern can interfere with transaction processing (walking over all records and accessing a few columns may add lock contention); Therefore analytics get their own separate database called 'data warehouse' (sometimes that's just a snapshot of the real OLTP database) 
    But it can also be a separate product that is tuned to access patterns of analytic queries!
        -- Analytics may also want to keep historic data that is not relevant to OLTP, so there is much more data to look at than with OLTP.

    Data is often translated into different formats for purpose of analytics:: 
        - fact table is a fat table for each 'fact' transaction (may have hundreds of columns!), with many foreign key references to additional tables (called dimension table)
    Often a single analytics query is reading just a few columns from the fact table; therefore they may keep a table for a single column (column centric storage) - such a table has lots of similar information, and can often be compressed!
        - also in column centric storage and append only write access leads to choice of LSM tree for storage.

    Other concerns: like to sort rows by the column that acts as frequent dimension in queries; May have multiple such indexes! 
    Sorting order of rows can also affect compression, as similar data is placed nearby.

    Common sums are cached in special tables called 'materialized views' (now they have the headache of invalidating stuff)
    when 'material views' go multidimensional they are called 'data cube'

 Part II - challenges in building distributed systems (across multiple machines) 
    
        Part III - derived datasets (no one single database)
