

This book gives you a good overview over a wide range of topics in data processing/web apps; if one sits in his own niche then chances are that you don't know any other areas too well; this book gives an wider overview of problems that have to be dealt with across the stack; therefore you get a better grasp on the architecture solutions that cloud based stuff is adopting.

----

Designing Data-Intensive Applications by Martin Kleppmann

(DIA) data intensive applications := the main challenge is the complexity of data that is processed

Recent trends (between 2000-2020)
    - big data volumes
    - services supposed to be highly available
    - machines didn't become faster; but number of cpu cores increased and networks are faster -> more parallel processing.
    - distributed build systems (thanks to infrastructure as a service - IaaS) 
    - open source systems prevalent (noosql and message queues are often used tools)

Says he makes sense of the new world and describes lasting principles.

Part I - fundamental ideas of data intensive apps

Chapter I - analyses the relevant concerns (Reliability, Scalability, Maintainability)

Reliability (continue to work even when things go wrong); 
    expectations for software:
    - system performs expected functions
    - can tolerate errors in data/user mistakes or some degree of errors in software components (fault tolerance)
    - can function under load
    - prevent unauthorized access

    Terminology:
        fault - 'one component deviating from the spec'
        Failure - system as a whole stops functioning.

        still there are 'systemic faults' in software - when the same fault causes unrelated machines to fail 
            examples: leap second bug in linux kernel, 
            memory leak will cause unrelated machines to have OOM (out of memory)
            cascading faults (one component causes others to fault, then others and others)
            component that other component depend on fails (like DNS) - thus creating wider impact

        ? how is a systemic fault different from a system failure?'

        Says they like to do continuous testing by simulating faults in production systems (like the chaos monkey that kills random processes)

        Also says continuous monitoring is needed to identify if the system is running correctly...


Scalability  - ability of system to cope with increased load
    
    - no one fit all characteristic to describe loads; the /load parameters/ depend on a given system.
      Load parameter examples:
        throughput/requests per seconds (batch systems care about this)
        response time (interactive systems care about this)
            - here they look at the average, median and nth percentile (entry in histogram of cases per percentage of latency).
            - service level agreements may state the required median and required 99% nth  percentile.
            - /amazon optimize for 99% percentile for latency: customers who get a low latency are very valuable - because they do a lot of transactions/
        latency - (duration that a requests waits to be handled) /more of an internal characteristic/ 
        ratio of read requests vs write requests.
        hit rate on cache.  
        number of concurrent active users.

    - example how load parameters as type of measurements change with the system: 
        twitter has two basic use cases: 'post tweet' and 'show timeline for a user'
            - at start showing the timeline for a users was select in tweet table join follower table (that wasn't very scalable)
            - then they started to cache the timeline for each active user, 
              when a person posts a message the change is pushed to the timeline objects of each active subscriberd
                (now for writes we have the measure of updating the 'fan out' - followers timelines.)
            - problem: some celebrities have millions of followers, so updating the timeline of each celebrity follower is too much work; (these are exempt from fan out, for their followers they must fetch celebrity tweets when building the cached timeline)
    

    - questions to ask:
        - if you increase the load (by relevant 'load parameter') - given the same setup, how does per performance decrease?
        - if you increase the load (by relevant 'load parameter') - How many machines do you need to add to keep the same performance?
            / order of magnitude changes may require a change of architecture/

    - types of scaling:
        - scale up (get faster, more powerful machine) /costs a lot but is easier (you try to scale up database installations)/ (you scale up 'pets')
        - scale out (get more machines) /adds lots of complexity on the system and may require change of architecture!/ (you scale out 'cattle')

Maintainability - (most costs are after development while maintaining a system)
    concerns
        - Operability - measures how easy it is for operations team to keep the system running
            For monitoring/problem solving/patching/upgrading/load planning 
        - simplicity - how easy it is to understand the internals of the system?
            avoiding accidental complexity (complexity arising from implementation details and not inherent in the problem (as seen by users): 
        - evolvability - (extensibility, modifiability, plasticity)


Chapter II - data model (modelling language and query language)

        - Many apps layer one data model on top of another one (like application level data object is layered on top of infrastructure data object layered on top of the database engine, etc)
- as usual: each layer hides complexity from the next layer on top to simplify the next layer on top (the different layers don't always fit - that creates an 'impedance mismatch')

- sql: says it was successful because the programmer doesn't have to think a lot how the data is represented by DBMS (? he has when tuning for performance ?) and RDBMS do transaction isolation well!
- nosql: became popular due to a set of requirements/options 
    - need for greater scalability (as compared to RDBMS) - very large datasets and throughput rates required for something like twitter.
    - lots of open source tools and solutions
    - frustration from sql (application objects don't always map to tables) 'desire for more dynamic and expressive data models'
    - specialized query operations (that can do without transactions)
    - in some way it's the return of hierarchical and network models; greetings from IMS (says SQL/RDBMS won that war because application code was much more simple as compared to the alternatives; the RDBMS query optimizer hides complexity of dealing with records directly/access paths to a record)

- with sql you have multiple choices for representations: non-indexed fields can be 1) in separate columns 2) in special json type field 3) as json document in TEXT field.
- schemaless json in database is often regarded as 'decreasing the impedance mismatch' (OMG) , what it does give you is the ability to handle collections of fields/records (at the expense of having to do validations in the application code)
- still separating common fields into separate table and linking them with foreign key (normalization) makes a lot of sense::: (? why were they bitching about sql in the first place?)
    - for localization (can translate a table into n languages without duplicating entries)
    - ability to query fields by value
    - duplication of values makes ambiguity and is bad.
- still foreign keys introduce many-to-one relations; the need for sub queries (or joins) ->  many-to-many relations are therefore difficult to handle (and are slow compared to other approaches - all due to sub queries); so we keep bitching.

- in any event where xml/json/xaml fields are stored: you most often don't have a schema (that means validation task is on the application code)
- however the irony is that lack of schema is sited as being easier to migrate data formats (as you don't have to modify/alter the data schema) - now that comes at the expense of even more messy application code...

- makes sense to keep data that is accessed together in the same xml/json document (never mind the cost to parse the whole thing)

- relational DBMS got a JSON field (with varying level of support) so that there is some convergence between document storage and RDBMS.

remaining chapters: tools and approachesS

DIA use building blocks (application code stitches these tools together)
Increasing overlap between tools & approaches (kafka message queue does store messages persistently) or reddis (data store also used as message queue)

    - datastores (sql,nosql) for persistance
    - caches to store intermediate results
    - search indexes 
    - stream processing - (allow messages to be sent to remote processes for asynchronous handling) - like message queues
    - batch processing pipelines

Minimizing human errors:
    - 'design system as to minimize the opportunity for error' and that 'encourage/make it easy to do the right thing and discourage to do the wrong thing' (i guess that's the rational of the 'opinionated' business that one sees all over the place)
        - (implies that this leads to an restrictive approach and that by overdoing it the user will be forced to work around that; so that 'the right balance is needed)
    - isolated sandbox environment for testing and training ( "Decouple the places where people make the most mistakes from the places wherethey  can  cause  failure")
    - make it easy to recover from human errors (fast rollback of configuration and code in production environments)
    - testing (unit testing/integration testing/system testing )
    - continuous monitoring of performance/throughput/response times/error rates (makes it possible to diagnose conditions)


Chapter II - data models & query languages

Application objects and database tables often need a translation layer that translates between database row between one or multiple sql tables, and the application object; for example one to many relationships have to be looked up into an application object collection. They call these difficulties 'object-relational mismatch'
    - on the other hand: if you have a 'normalized' relational DB (meaning all common data redirected to tables without duplication ) then that has a lot of good points
        - such as avoiding ambiguities (if you duplicate the 'location name' field then what do you do with places that have the same name?)
        - better localization support (just translate the string table, but references to entries in string table stay the same)
        - queries go well because of lack of ambiguity.

Application objects map more neatly to json documents (or json fields in sql database) but now the json data is not validated against a schema (well, there is sql schema, but I guess document db's don't support them when this book was written).

Issue of schema: Either you have the DMBS validate a schema, or the validation is in the application code while inserting/updating/querying values.
(that makes this debate similar to static type checking (with schema) vs dynamic (runtime) type checking).
Each approach has its good and bad things: for example lack of schema will have more flexibility as there is 'update table'/schema change overhead (also schema migration adds to system downtime (?)); (still there is more work the 'dynamic approach' overall, if you ask me) 

SQL and many-to-many relationships: SQL has recursive syntax since SQL 1999 - recursive common table expression (but that is not quite concise)

Query language:
- SQL has declarative query language (because you specify desired result in the query).
    - declarative query language is more concise than imperative query language where the query is done in code 
    - the order of query results can come out different 
    - declarative language is done behind the scene - the database engine could run it in parallel 
  Map reduce query language - is imperative ql (does map/filter/fold with a lambda expression function without side effects (assignment) that applies the filtering/processing); (mongo db query language looks even more declarative, but that's a disguise)
    - somewhere in between with flexibility/conciseness as compared with declarative vs imperative query language
  Imperative query language:    
    - code that loops through data and checks if item meet a coded condition.`` 
    
Graph databases - all data is a set of nodes and links between them; is ok when there are lots of many-to-many relationships in the data; Is supposed to work more natural rather than putting it into relational tables.
    - graph db models (directed graph, each of them)
        - property model : each node(vertex) and each link have a unique ID and each has a set of name   /value propertiesl 
        - triple store model; each facts stored as 3-tuples: (subject, predicate, object) ; subject and obect are nodes(vertexes); predicate is a directed link.
    - some products like NEO4j have their own declarative data manipulation and query language.


To sum it up; NoSQL come as following models (both don't enforce a schema, typically)
    - document db: data comes in self contained documents and relations between documents is rare (?)
    - graph db: any node can be potentially connected with every other node.

Chapter III - storage & retrieval 

How are indexes for lookup of record in table by key field are done in RDBMS? (data there is organized from a log of binary records - that makes it easier to handle crashes, also append only handles writes quickly)

    - one approach is to have a hash map for lookup (?! chained hash, not linear hash) 
        - works for append only (log) data; map the key to the record offset.
        - can have segments of a file (each segment with it's own hash table; so as to accommodate limited length of offset field).
        - since date is append only: on deleting a record one has to append a 'this entry is deleted' record (also called tombstone)
        - problems:
            - index must fit into memory (having on disk hash index means a lot of disk seeks, that is bad).
            - can't do range queries (only lookup of entry by key)
    - SSTables (sorted string table) 
        - divides the file into segments, where each segment has a set of sequential entries sorted by key valued.
        - again need to divide a file into segments; `
        - merging two segments into one: easy as entries are sorted; if the same key appears in both segments then take the most recent record 
            (merging of two/several segments is done by compaction process)
        - uses smaller in-memory index (than hash map); still need the first and last record of each segment file (then search the partial segment when found)
        - for the batch of least recently used entries: maintain them in memory with a BTREE index (this in-memory structure is called MEMTABLE).
        - when the MEMTABLE grows too big then write sort it and write it to disk as a segment. 
        - Problem: need to keep a log of recently used entries; so that it can recover the last records (still in memory MEMTABLE) upon crash.

    - LSM-Tree (log structured merge tree, very suitable for append-only data)
        similar to SSTables; but has multiple ss-tables grouped into 'levels'. Tables from lower levels are merged into larger tables that are put into the next 'level'.
            Lots of tables are ordered into 'levels' (not clear what the difference between 'size tiered' and 'level tiered' compaction is). Each level contains tables with non-overlapping key ranges.
              - size tiered compaction  
              - level tiered compaction

            Optimization: add in memory bloom filter to each segment (to check if entry is in key range, as there are tables in different tiers with overlapping key ranges)
              

       - BTree indexes (most often used and well known). Each node has a set of entries for keys k(1) .... k(n); and or each key k(i) it has a pointer p(i) that points to the node that will contain all keys in the range between k(i) and k(i+1). Very suitable as index for data that is updated in-place.
             - take care to pick records size so that disk block is filled properly.
             - they make an effort to keep it balanced, but sometimes they need to split nodes (to keep it balanced). Now if it crashes then the tree is corrupted. To fix that: keep a log of pending modifications (written first before applying the change); that's a write-ahead-log (WAL); Alternatively:: copy on write - prepare a modified page with the new btree entry; then update the parent page to point to the modified page.       
             ;                                                           

    Comparing btree and lsm
    - BTree is faster for read, while LSM is faster or write (while reading LSM tree has to check multiple entries)
    - lsm tree: entry is not bound to block size, therefore can be compressed better and does less fragmentation - that occurs when splitting btree nodes.
    - lsm have less 'write amplification' - (that is a lsm write turns into less writes on disk on average) (? What about compaction ?)
    - lsm compaction may not keep up with the write rate (bad) in this case reads become slow - have to check for more segments.
    - btree performance is more uniform (less spikes) as each key is in one location.

    Indexes
        - You can have a 'clustered index' In this some columns get duplicated in the btree index - for faster read access. (now that brings all the perils of duplication and makes the dmbs transaction management more complicated)
            CREATE INDEX IX_test ON table1(column1) INCLUDE (column2)
        - multi-column indexes: most often they just concatenate all the index field and search over this value (that's called concatenated index); problem: you can't do range searches (combine x and y coordinates, you can't find { (x,y) |  x1 <x < x2 && y1 < y < y2 }
                - have more specialized structures for that :: R-Tree

    Free text search
        - keep a reverse index: for each word (that is not a stop word like 'and' 'or') keep a list of the document id's that it appears in
        - allow to search for synonyms of search term (or terms within given editing distance to make up for typos)

    In memory database - is fast because doesn't need to encode the data in log format (for persistence)
       says "OLTP Through the Looking Glass, and What We Found There" Stavros Harizopoulos, Daniel J. Abadi,    Michael Stonebraker 


    OLTP (online transaction processing) vs Analytics (data warehourses).
    OLTP - needs low latency (interactive queries) and high availability.

    Analytics need to run batch queries over a wide range of records for their reports, this access pattern can interfere with transaction processing (walking over all records and accessing a few columns may add lock contention); Therefore analytics get their own separate database called 'data warehouse' (sometimes that's just a snapshot of the real OLTP database) 
    But it can also be a separate product that is tuned to access patterns of analytic queries!
        -- Analytics may also want to keep historic data that is not relevant to OLTP, so there is much more data to look at than with OLTP.

    Data is often translated into different formats for purpose of analytics:: 
        - fact table is a fat table for each 'fact' transaction (may have hundreds of columns!), with many foreign key references to additional tables (called dimension table)
    Often a single analytics query is reading just a few columns from the fact table; therefore they may keep a table for a single column (column centric storage) - such a table has lots of similar information, and can often be compressed!
        - also in column centric storage and append only write access leads to choice of LSM tree for storage.

    Other concerns: like to sort rows by the column that acts as frequent dimension in queries; May have multiple such indexes! 
    Sorting order of rows can also affect compression, as similar data is placed nearby.

    Common sums are cached in special tables called 'materialized views' (now they have the headache of invalidating stuff)
    when 'material views' go multidimensional they are called 'data cube'

Chapter IV - how applications deal with schema changes

- server apps do rolling upgrades (upgrade on a few nodes and see if it still works, then proceed with some more nodes,...)
- so new and older version must deal with the same data! the requirements are
    - backward compatibility - new versions of the system can read data written by older versions
    - forward compatibility - older versions of the system can read data written by newer versions. (older versions must ignore data extensions of newer versions!)

language specific serialization (like java.io.Serializable and python pickle) have problems:
    - not portable between languages. (that's why they often prefer xml, json, etc)
    - security problems as the mechanism needs to be able to instantiate arbitrary classes (and/or carry code with the data).
    - hard to deal well with versioning 
    - cpu intensive. (could be more efficient)

problems with textual encoding (json, xml, csv)
   - ambiguity how language implementations deal with number formats (just store numbers as strings in json)
   - binary data needs to be translated to base64 strings (makes the data bigger by a third)
   - most apps don't bother with schemas, as they have to serialize the data, and the parsing step is the implicit validation process.

        binary encoding - binary encodings are sometimes preferred for compactness. (lots of binary encodings for json! MessagePack, BSON, BJSON, UBJSON, BISON, Smile .. there is also json-b, json-c to add to the confusion (json-b does string tables to avoid repeating field names)
    

google protocol buffers, apache thrift, apache avro - RPC libaries have the same problem of binary representation, so they too can be used for binary serialization (protobuf seems to be the most common one now)
    - in proto definition: each message field has an integer tag (assigned by author of proto file) and fields are optional by default. 
    - integer tag value of fields enables forward and backward compatibility - if new fields get new tag numbers (old code just ignores data fields with tags it doesn't know!) You can change an optional field into a repeated field with protobuf, as the binary format it gets the same markers.

Apache avro 
    - they don't have tags for fields; instead each modification gets a version number; and the runtime has to find a transformation between the version of the written data and the readers schema version. One can add/remove fields with a default value - this preserves compatibility between versions
    - avro has different options of encoding the schema with written data
        - put the schema in the header; for large files
        - keep the version number with each record, allows to mix different versions in one file.
        - negotiate schema version on connection setup for rpc usage.
     - says avro approach is better for dynamically generated schemas; when a new version of the schema is generated: in protobuf you would need to update integer tags and take care that new fields get new tag numbers.

Databases and backward compatibility.
    - database may have had schema change, but old code may access the DB with new schema! 
        - DB client is backward/forward compatible; 
        - but application now get the following semantic problem: Old client reads new schema that got fields added. New field gets ignores; now old code updates the record - do the added fields of modified record get null values? If the app just copies the DB fields into an application object and then does the update query based on app object data then you have a problem.

database evolution: when a db table new fields, then dbms avoids to rewrite its table (costs time); instead it has to deal with rows written when the old schema was active! Now you have a schema migration problem - linked in Espresso DB uses Avro to address this problem.

Web services 
 - like to use http based mechanism - SOAP (has fallen out of favor) or REST (use OpenAPI/swagger for interface definition) ; but these are slower than RPC.
 - rpc call is very different from calling local function; no use to try to make it look the same because :: Network timeout - you don't know if server crashed - there is no equivalent concept in local function calls. What happens when request or response get lost due to Network problems? is a failed request resent? Network can have very different latencies because congestion. Client and server in different language - don't have the same number primitives.
 - says REST is popular because 'it doesn't try to look too much like RPC; REST advantages: you can test it easily with curl, don't have to deal with code generation; easy to experiment and there are lots of tools around
 - RPC is evolving: newer version of RPC care about schema compatibility; in REST addition of parameters & return values are seen as non breaking changes. gRPC has streams (for sending/receiving sets of objects)
 - RPC advantage: it is much faster then REST (no text parsing, which is slow)
 - handling versioning in the API: REST may have version as part of URL or HTTP accept header. With RPC you may have admin interface or version as param.
 - says REST is preferred for talking between different organizations (therefore REST interface need to preserve backw. compatibility longer); RPC is preferred for talking within the same organization.

Message queues - asynchronous and, one way communication (client publishes message to queue/topic, server is reading from that queue/topic), isn't waiting for response). more flexible than RPC:
- server can be down temporarily, server crash does not lead to client timeout (the queue is a buffer) therefore messages don't get lost in transit.
- client needs to know queue topic, don't need to know the server address. There is more decoupling: client publishes message.
- unlike RPC: the message is just a sequence of bytes; no provisions for versioning - that's the problem of the application. /typically more tight coupling of clients vd servers version is assumed/ 
- for rolling upgrades: apps have to choose something like gRPC messsages as data instead of default format.
a
Part II - challenges in building distributed systems (across multiple machines) 
  
Vertical scaling  - get server with more CPU/RAM (costs go up non-linearly) - fault tolerance suffers (everything is down with the big server). vertical scaling for data: Shared disks accessed by many machines - problems with locks.

Horizonal scaling - get more machines (assumes shared nothing between components) - problem: more work with the software

- Data Replication (keep copies of the same data on different machines)

- Data partitioning  (different subsets of the data kept on different machines)


Part III - derived datasets (no one single database)


- leader based replication (active/passed based replication; master-slave replication) :: one instance is instance write mode, it can store data (aka. master, leader, primary) all other instances are read-only and will receive the change log for the data and change their data accordingly (read instances also known as: read replicas, slaves, secondaries, hot standbys.

- synchronous replication: server processes a write request; the writeable instance acknowledges the write to the client only after all readonly instances have acknowledged that they received and processed the replication message.`Common setup: changes are replicated synchronoysly to one readonly instance (that one is in hot standby to take up when the current writable instance fails); all others are replicated asynchronously (this way request latency is kept in limits for the client).
- if all replication is done asynchronously then there is a potential problem with data loss, says they still do it it keep latency down (if many readonly instances or instances are far away)

New readOnly instance setup: need to get a full snapshot from the leader; then ask the leader to send all the diffs since that snapshot, process them and go online after that.
- similar recovery step needed if a readonly instance is taken offline for some time (for maintenance).

- upon write instance/leader failure the cluster needs to elect a new leader.
    - determine that leader failed (the pings time out, is not failsafe)
    - elect new leader (either by consensus or a controller node apoints it; now if there is one instance only that is replicated synchronously then that would be the candidate in both cases)

Shit can happen during replication (complicated failure scenarios during failover)
- with async. replication - a instance without full backup is elected, then the old leader comes back, what happens with newer data in old leader?
- a replica without full data becomes leader; now some of the assigned primary keys get reused. What happens if these keys are being referred to by outside systems?
- at the same time two nodes conclude they are the leader - can lead to data corruption as both masters are assigning primary keys.
- too short leader timeouts can be due to congestion/load and can lead to these failure cases; too long timeouts can lead to long delays :: what is the right leader timeout?

Mechanisms of replication
- Statement based replication: the current leader can send sql statements to the followers; problem: SQL statements that call random(), rely on current DB state, etc will end up having different results in each follower.
0 
- WAL (write-ahead log shipment) :: The SQL DB maintains a write log of all transactions, send that to the follower. Problem: the write log is tied to the db version; you need the master/follower instances to be of the same version
- Logical row based replication :: some DB's have a logical row log for replication purposes that is upward compatible.
- trigger based replication :: (obscure, used when you only need to replicate a subset of the data) have a DB stored procedure trigger run when a table changes, make it record changes in a special table, and have that table replicated.

- replication for performance reasons (read scaling): you end up with many asynchronously replicated followers; each one with out-of-date data. Now a client can get out of date data. Welcome to the world of 'eventual consistency' (many applications don't need transactions) Now there are several levels of consistency and one has to consider what kind of service is acceptable:

    Problem: Application reads its own writes; if subsequent reads come from any replicated follower they may return old data.
    Read after write consistency := requirement to read your own write consistently. How to do that?
     - probably you need this guarantee only for your own data (like the same users profile) - as that's the data he has changed himself previously; Always read that data from the leader; read other data from any other replica
     - alternative: track time of record update, if record is too old then re-read from the leader.
     - check if replica is up to date by reading your own most recent modification; if you don't get it then either wait or move to another replica.


    cross device read after write consistency := can get your previously written data from any client machine.
     - now the tricks with checking your own last writes don't work.
     - need to route all requests to the same datacenter (us-east) Harder to do that with mobile devices.
     
    Monotonic read ::= guarantee that a user will not see older data on a later read (can happen if read 2 goes to a replica with older data). Solve this by routing DB reads to the same replica for the same user.

    Consistent prefix write := if a sequence of records have been written, then every reader gets the records by their correct sequence.


Multi-leader replication (master-master, active-active) :=  multiple writable nodes in the system.

Why would you do that?
- if you have multiple geographic data centers; each data center has a leader that gets the writes for it's region; now they need to sync in order to work on the same global dataset. Now conflicts can get complex if the same entries get overwritten in multiple data centers.
- client that allow offline operations; when offline each client is its own DB; when they get back online they need to sync with the global DB.
- collaborative editing: you can't have a lock on globals state, instead each client has to sync with the global state.

Problem: conflicts must be resolved - two writes of the same data succeeded on two different masters; during replication you find that they are in conlict (modify the same data differently)
    - one can avoid it: writes to data are routed to the leader that is in the region that owns the data of a user
    - alternative to avoidance - detect & resolve conflicts
        - detecting conflicts: Kinds of conflicts:
            - two concurrent writes change the same record; now have two versions of them
            - application specific conflicts: scheduling the same room for two different groups at the same time (more difficult to spot: depends on known state at scheduling state)
        - resolving conflicts:    
            - each write gets a time stamp, upon conflict the write with highest conflict wins.
            - have both results (possible in collaborative editing apps)
            - have a data structure with all versions, let the user later resolve them (by prompting him in UI)
            - resolve conflicts on read: store both conflicting versions, when the user reads his data than prompt him to resolve the conflict.
            - have some mechanism that resolves conflicts automatically (somehow) 
                - when they are identified during replication: either application level script; 
                - when a conflicting values is used: let the user choose in prompt (need to preserve all conflicting versions for that
                - conlict-free datatypes (says still in research); or Mergeable persistent data structures (keep changelog and allow three-way merge as in git); or operational transformation (what googledocs does for conflict resolution on simultaneous editing)

Topologies for multi-leader replication
   - circular - each change has tag of origin node, is propagated to the next in the ring; when it get back to the origin node it is no longer passed on. Problem: one node falls off the ring (would have to reconfigure and skip the failed node)
   - all-to-all replication - problem: different speed of links leads to wrong causality of events. The replication protocol would have to add versioning and figure this out (says not always good support for that, be careful)
   - star, all-to-all.

Leaderless replication - every instance can do writes, every instance replicates with each other. Used with NoSQL (DyanmoDB, Cassandra, etc)
  - conflict resulution strategies:
      - each record has a version sequence number that is always incremented; You have to read from several instances and take the result with highest sequence number (great, what do they do when a stale instance gets a write and it becomes inconsistent?)
 
  - simultaneous writes :: conflict resolved as follows - client detects that read returns stale data; now the client writes the correct data back to that instance. 
      - client writes to all leaders concurrently; succeeds if a quorum size (variable w) returns success..
      - read repair :: read from all leaders and (variable r) of them return the same data 
      - quorum reads/writes::: if r + w > numer-of-total-instances then you are sure that the next read will get the right data
            - n=3 w=2 r=2 - tolerate one node down
            - n=5 w=3 r=3 - tolerate two node down
      - shit can happen when w is low (when errors accumulate)
  - anti entropy process :: the server has a background process that replicates from other instances.
  - avoid concurrent writes :: client writes to one instance and that instance has to replicate the write to the other instance.
  
