======

Jenkins: life-saving tip:

    Say you have a failed build like:   

        https://jenkins-host/job/TESTS-JOB/2680/

    Where is the log file as tet ????

    Here is the full log as a text file, just add /consoleText to the URL 

        https://jenkins-host/job/TESTS-JOB/2680/consoleText

    Thanks to https://devops.stackexchange.com/questions/6085/how-can-we-get-the-jenkins-console-output-in-a-text-file

======

Kubernetes:

    -- show name and image for pods
        kubectl get pods -o=custom-columns='NAME:metadata.name,IMAGE:status.containerStatuses[0].imageID'

    - show current context details

        kubectl config view --minify --flatten

    - which deployment/replicaset/etc is controlling a given pod?

        kubectl describe pod <pod-name>

        - result has line that starts with 'Controlled by' - this names which entity is controlling the pod (usually a replicaset, and that one is controlled by a pod)
         

    - get pods sorted by start time

        kubectl get pods --sort-by=.status.startTime


    - extracting interesting fields from a set of pods:


         Pod name and pod IP addreess

         kubectl get pods -o custom-columns=NAME:metadata.name,IP:status.podIP

         An overview of what the containers in pods are running: (name,image,command,arguments of commands, environment) - and pod IP

         kubectl get pods -o json | jq '.items[] | {name: .metadata.name, image: .spec.containers[].image, command: .spec.containers[].command, args: .spec.containers[].args, env: .spec.containers[].env, status.podIP, podIP: .status.podIP }'

    - list all ingresses (with ip addresses)

        kubectl get ingress --all-namespaces

    - https://github.com/ahmetb/kubectx 
        contains utils: 
            kubectx - for switching kubectl context (each kubectl context defines how to access a particular cluster)
                      !! kubectl configuration for switching clusters is defined in $HOME/.kube/config !!
                      !! if you want to switch using a different cluster with kubectl command line (specify configuration directory with --kubeconfig)
                            kubectl --kubeconfig DIFFERENT_DIR_FOR_KUBE_CONFIG !!

            kubens - setting the default kubectl namespace 

     - configuring kubectl access to other clusters
            a bit involved, see here: https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

            use kubectx !!!

     - list configured clusteres

            kubectl config get-contexts

            # kubectx - a utility that helps with contexts
            kubectx 

     - switch to another context

            kubectl config use-context CONTEXT_NAME
    
            # ... or with kubectx
            kubectx CONTEXT_NAME

      - switch to previous context (kubectl can't do that)
            kubectx - 

      - run a shell in a pod (pod-name - listed with $(kubectl get pods)

          kubectl -it pod-name -- bash

          kubectl -it pod-name -- sh

      - run a shell in a container cnt of a given pod (get list of containers for pod with $(kubectl describe pod pod-name)

          kubectl -it pod-name -c container-name -- bash

      - list all namespaces 

            kubectl get ns

      - switching namespaces and cluster context: (there is an nice utility for that: kubectx + kubens)

            https://github.com/ahmetb/kubectx


=======

Show logs of all pods (name of pod/container is in each line) - and grep through them

    kubectl get pods | tail -n +2  | awk '{ print $1 }' | xargs -I{} kubectl logs --all-containers=true --ignore-errors=true --prefix=true {} | grep -i error

    kubectl get pods | tail -n +2  | awk '{ print $1 }' | xargs -I{} kubectl logs --prefix=true {} | grep -i error


=====

Need to know which node a pod is running on?
    
    # -o wide shows a column with the node
    kubectl get pods -o wide

======

Docker reference

https://docs.docker.com/engine/reference/commandline/docker/

---

How to build with the Dockerfile in the current directory

    docker build --progress=plain --no-cache . -t resulting-image-tag
       --progress=plain : show output on the screen! old-style! you see what is happening!
       --no-cache       : don't use the cache to pick up something old
       .                : current dir
       -t blalba        : tag the resulting image as blabla

---

    # don't use cached layer - best used to create the 'official build'
    Docker build --no-cache 

---

docker image ls     :: shows docker images (except for intermediate images)
                    :: what is an intermediate image? Each step in docker build creates an intermediate images (these are cached to improve build time!)

docker image ls -a  :: shows all docker images (including intermediate images)


    docker image ls
        REPOSITORY               TAG       IMAGE ID       CREATED        SIZE
        docker/getting-started   latest    157095baba98   5 months ago   27.4MB


    # you have more fields in the json output (no idea what they all mean).
    docker image ls --format='{{json .}}'
        {"Containers":"N/A","CreatedAt":"2022-04-11 18:25:34 +0300 IDT","CreatedSince":"5 months ago","Digest":"\u003cnone\u003e","ID":"157095baba98","Repository":"docker/getting-started","SharedSize":"N/A","Size":"27.4MB","Tag":"latest","UniqueSize":"N/A","VirtualSize":"27.37MB"}



---
running a container - with a given image (if image is not present then they try to pull it from the docker registry)

    docker run fedora:latest ls / 

        - run $(ls /) command in the container, and print the output of the command.
        - the command exits and the container is no longer running

            docker ps -a    
            CONTAINER ID   IMAGE           COMMAND   CREATED              STATUS                          PORTS     NAMES
            d35b0813ce79   fedora:latest   "ls /"    About a minute ago   Exited (0) About a minute ago             eager_mcclintock

        - the entry of the exited docker container remains - until it is cleaned up by running $(docker container prune -f) 

    docker run --rm fedora:latest ls /  

        - same as before, just that the entry for the exited docker container gets cleaned up, when the docker exits. (--rm says so)


    # or running a shell in the thing
    docker run -it fedora:latest  bash 

    # force specific platform
    docker run --platform linux/amd64  -ti alpine:3.17   sh

    docker run --platform linux/amd64  -ti ubuntu:latest   sh

    # sometimes a docker image comes with an entry point, so you need to override the entry point to run a shell
    # sad but true
    docker run -ti --entrypoint sh docker-image-with-explicit-entrypoint 


    # if the image has an entry point then you can override it
    docker run -it --entrypoint bash fedora:latest

    # if you need to copy files between docker and file system: map home dir to /myhome in container  -v $PWD:/myhome
    docker run -v $PWD:/myhome -it fedora:latest  bash 

    # or running a shell in the thing - and force it to run as root
    docker run -u 0 -it fedora:latest  bash 



    docker run -d fedora:latest ls /
8bd31381ae93712414e02c58589a8562be9739ccc01d0d9baf93f39a6ab32505

        -d option says not to show the docker command output to screen, instead it prints the id of the docker container. You can now use that id to stop/resume/kill the container (or to $(docker wait CONTAINER_ID) until it exits)


    docker run -d -p 9000:8000 -v $PWD:/mnt/loc fedora:latest /bin/sleep infinity

        -d                  : runs as daemon (you get the docker container id in stdout)
        -p 9000:8000        : maps tcp port 9000 (outside the container) to port 8000 (inside the container)
        -v $PWD:/mnt/loc    : mounts current directory to /mnt/loc inside container

        /bin/sleep infinity : the container does nothing, just keeps running (but you can attach a terminal to it that runs in the container, and it enjoys the mapped port and mounted directory!)

        ## !!! sleep infinity - works on Linux, this is a GNU extension. !!!

    # attach to the running container (it will not loose it's stuff - while the container is running!) 
    # (you can't attach to a stopped container - naturally)

    docker exec -it fedora:latest bash

    # may use the following to run a container, maybe that's more general.
    # sh -c 'while [ true ]; do sleep 5m; done'
    
    docker run -d -p 9000:8000 -v $PWD:/mnt/loc alpine:latest  sh -c 'while [ true ]; do sleep 5m; done'  

        for bridge network mode: -p 9000:8000 - port 9000 on the host is mapped to port 8000 inside the container. (listening port inside container can accept connections from the outside!)
        -v $PWD:/mnt/loc    :: map the home directory on host to /mnt/loc inside the container


---

Running a container, and overriding the entry point of the container (--entrypoint can tell what to run, provided the process is installed on the image)

    docker run  -it --entrypoint /bin/sh php:7.4-cli

In addition: mount the current directory on the host into the container - to path /mnt/loc ; ls /mnt/loc - just like the local directory when running the command!

    docker run  -v $PWD:/mnt/loc -it --entrypoint /bin/sh php:7.4-cli

You can run the whole thing in detached mode ! (and attach to it later)
This has the advantage that the docker keeps running, until the system is rebooted. (you can get a similar effect with tmux :- ) 

    docker run -d  -v $PWD:/mnt/loc -it --entrypoint /bin/sh php:7.4-cli


Run me a linux, with my stuff:

    docker run  -v $HOME/mystuff/:/mystuff -it fedora:latest   /bin/bash

!!!!! run a docker inside docker DIND !!!!
    
    # problem - elevated privileges 

    docker run --privileged -d --name dind-test docker:dind

    docker exec -it dind-test /bin/sh

    !!! you can do on linux and run DIND without elevated privileges !!!! (needs sysbox docker runtimer) / you can't do that on osx
    ??? wonder why sysbox runtime is not part of docker proper (probably because it doesn't work on all kernels - out of the box) 
    
    https://devopscube.com/run-docker-in-docker/
    https://github.com/nestybox/sysbox#installing-sysbox

? identifying docker containers by label / ensure that only one is running.

    - if the container has a name - docker doesn't allow you to run two instances with the same name at a time

    - now problem left: you can have the container hanging in some strange stage like 'Stopped' or 'Paused'
      The fix is to use labels.

      When starting the container add some unique label
            docker run ..... -l docker-php-admin

      Before starting the container: find by label, check if not running (exit if true) - else: stop it and prune it.

        STATE=$(docker ps -a --filter 'label=docker-php-admin'  --format='{{.State}}')
        if [[ $STATE == "running" ]]; then
            echo "server is already running"
            exit 1
        fi
        if [[ $STATE != "" ]]; then
            # force stop and clean up
            ID=$(docker ps -a --filter 'label=docker-php-admin' --format='{{.ID}}')
            if [[ $ID != "" ]]; then
                docker kill "$ID"
                docker container prune -f --filter 'label=docker-php-admin'
            fi
        fi



---

docker exec -it <container name|container id> /bin/sh   :: The classic: running a shell in a running container 

[Docker guides] See:  https://docs.docker.com/get-started/overview/) It's all hidden under: Running your app in production / Configure containers 


----

docker ps     ::: shows only running containers
docker ps -a  ::: shows running and stopped containers !!!

docker container ls ::: exactly the same as docker ps 

Docker commands can display it's stuff as json!!

    docker ps --format='{{json .}}'
        {"Command":"\"/docker-entrypoint.…\"","CreatedAt":"2022-10-08 07:45:28 +0300 IDT","ID":"f99693d80146","Image":"docker/getting-started","Labels":"maintainer=NGINX Docker Maintainers \u003cdocker-maint@nginx.com\u003e","LocalVolumes":"0","Mounts":"","Names":"keen_darwin","Networks":"bridge","Ports":"0.0.0.0:80-\u003e80/tcp","RunningFor":"6 minutes ago","Size":"1.09kB (virtual 27.4MB)","State":"running","Status":"Up 6 minutes"}

-----

## moving a docker image to another machine

    docker save FULL_SHA -o tarfile.tar

    /COPY THE TARFILE.TAR to the other machine/

    docker load FULL_SHA -i tarfile.tar

    /Note that it often doesn't move the TAGS!!! so you need to tag the resulting image on your own/

    docker tag FULL_SHA tag_name


## shows properties of the docker engine

docker info         

## shows more stuff in json output (again, no idea what they all mean) 

docker info --format='{{json .}}' 

-----

### shows container logs (shows both stdout and stderr)

docker logs <container name or id>  

### shows logs with timestamp

docker logs  --timestamps <container name or id> 

### shows both stdout and stderr (they need to be sorted by timestamp, otherwise things get mixed up)

docker logs  --timestamps <container name or id>  2>&1 | sort -k 1  

### !!! Attention !!! docker logs writes to both STDOUT and STDERR. Error info is supposed to be written to stderr!!! ###

-----
## spills out details of the object. (all different format, depending on object type)

docker inspect <container-id>|<container-name>|<image-id>|<image-name>      


# You can extract several fields from the result!

docker inspect --format='{{json .Id}} {{json .Config.Cmd}}' docker/getting-started
    "sha256:157095baba98513dfef4ea00423767d8dae10edfeb629e9d39ea456e53f51e6a" ["nginx","-g","daemon off;"]

# strings in the template are shown as is (between the {{...}} query construct)

docker inspect --format='{{json .Id}} <=> {{json .Config.Cmd}}' docker/getting-started
"sha256:157095baba98513dfef4ea00423767d8dae10edfeb629e9d39ea456e53f51e6a" <=> ["nginx","-g","daemon off;"]


Listing all tags/info for an image:

    https://www.googlinux.com/how-to-list-all-tags-of-a-docker-image/

    curl 'https://registry.hub.docker.com/v2/repositories/library/debian/tags/'|jq . 


------

docker swarms  - they have their own cluster thing (i think docker tries to be some alternative to Kubernetes; that's more than docker-compose - which runs on a single node only)


--------
Get info on a specific image and tag - from dockerhub (including the os/platforms of all possible images)

    docker manifest inspect php:8.2-apache

------

PORTS

    - you can expose ports from a container -  that does nothing (it's a form of documenting which ports will be listened at within the container)
        - either by having EXPOSE in the Dockerfile (during build)
        - $(docker run --expose <port> )

    - to access a daemon running within the container: you need to map the port from outside the network to a port within the network  (docker run -p 8000:80)

    - you can do $(docker run -P .... ) - this will create a mapping between all 'exposed' ports to a temporarily assigned port number outside the network.

------

    You can connect from default docker to localhost -  host.docker.internal
        from docker  - that dns name resolves to the local host !!!
        > ping host.docker.internal

------

Trying to run a subprocess $(docker exec -ti <docker_id> /bin/bash) and passed the process pipes for the stdin/stdout/stderr streams.
Got error "the input device is not a TTY".
 
Now there i a lower level docker REST API! https://github.com/docker-php/docker-php 


https://docs.docker.com/engine/api/sdk/examples/ 
    For $(docker ps)
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json`
That means write raw HTTP to a unix domain socket, genius!


https://stackoverflow.com/questions/53781590/how-to-use-docker-api-engine-to-exec-cmd-in-containera

    - first create an "exec instance"
        POST /containers/{{id/name}}/exec
        {
            "AttachStdin": true,
            "AttachStdout": true,
            "AttachStderr": true,
            "DetachKeys": "ctrl-p,ctrl-q",
            "Tty": true,
            "Cmd": [
            "bin/bash","-c","touch appa.py"
            ],
            "Env": [
            "FOO=bar",
            "BAZ=quux"
            ],
            "Privileged":true,
            "User":"root"
        }


    = messages received after upgrade

        ```go
        header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}
        ```

        `STREAM_TYPE` can be:

        - 0: `stdin` (is written on `stdout`)
        - 1: `stdout`
        - 2: `stderr`

        `SIZE1, SIZE2, SIZE3, SIZE4` are the four bytes of the `uint32` size
        encoded as big endian.

        Following the header is the payload, which is the specified number of
        bytes of `STREAM_TYPE`.


    - then 

----
Mapping some docker commands to the docker engine rest api

Beware! The json output of curl vs docker commands is very different!

    docker ps --format='{{json .}}

    # doesn't show all containers (only running ones)
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json | jq . | less

    # show all containers
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json?all=true | jq . | less

    # show all containers and the size of containers field.
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json?size=true\&all=true


    ---

    docker image ls -a --format='{{json .}}'

    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/images/json?all=true\&digest=true | jq .

---

Docker layered file system

https://jessicagreben.medium.com/digging-into-docker-layers-c22f948ed612#:~:text=What%20are%20the%20layers%3F,during%20the%20Docker%20image%20build. 

========================

on OSX and linux - the same output for:

    docker info --format='{{json .Runtimes}}' | jq .
    {
      "io.containerd.runc.v2": {
        "path": "runc"
      },
      "io.containerd.runtime.v1.linux": {
        "path": "runc"
      },
      "runc": {
        "path": "runc"
      }
    }



---

git/github

        # github search cheat sheet 
            https://cheatography.com/cpriest/cheat-sheets/github-search-syntax/

        # github search: limit search of term "miau" in files with file extension:*.py
        path:*.py miau

        # github serach: limit to root level of repository (can use that to limit to subdirectories)
        path:/

        # 'deleted by us' during merge or cherry-pick
            # file x was added in the commit being cherry-picked
            # file x was deleted in the branch that you cherry-pick into.
                git add x  # if you want to keep the file
                git rm  x  # if you want to delete the file.

        # 'deleted by us' during rebase - now 'us' and 'them' is reversed !!!!
            # file x was added in the branch that is being rebased
            # file x was removed in the branch that is brought in
            

        # shallow clone of linux kernel (--depth 1 - get commit history of the last commit) (much faster to clone)
        git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git linux-git --depth 1

        # sort files by extension (rev - reverts each line of the output, so that the extension (in reverse) comes first)
        git ls-files | rev | sort | rev

        # Get all commits by user (that can be useful during performance reviews...)
        # select all commits done by user

        git log --author=<user name that appears in commit messages>

        # get the commit message of all commits by user.
        git log --author=<user name that appears in commit messages> --format=%s 

        # in case that the first string is the name of the issue - get all the issues that the user did.
        git log --author=<user name that appears in commit messages> --format=%s | awk '{print $1}' | sort | uniq


        # github.com ::: open a pull request on some other repository https://github.com/AwesomeUser/AwesomeProject

        - on original repo page: create a fork (press the 'fork' button)
        - after forking: you have that repository under your own github user  https://github.com/YourOwnUser/AwesomeProject

        - clone that forked repository.
        - create a new branch of the main branch.
            git branch -m my_new_pr
        - add your changes and commit
        - set the UPSTREAM of the branch: 
            git remote add upstream https://github.com/AwesomeUser/AwesomeProject
        - push the new branch:
             git push -u origin my_new_pr
        - push command writes a long text to the console, that text includes a link to open the new pull request.   
    
vimdiff
     - space insensitive comparison:

        :set diffopt+=iwhiteall


Mac/OSX tricks



? remove colorcodes from text ? 

    # kubectl get logs  - this one is returning text with color codes (i think that's very confusing). 
    # kubectl logs POD_NAME - color escape sequences everywhere...

    cat blabla.txt | sed -e 's/\x1b\[[0-9;]*m//g'

? built-in something is very different from what you expect ?

    # very true for sed - will install gsed 
    brew install gnu-sed

    gsed -i 's/blabla/blublu/g' foo.txt

    # to set it up as sed - instead of gsed (still figure the path)
    brew install --default-names gnu-sed

? force quit an application

    command + option + escape - brings up a 'task manager'

    'Activity monitor' - a better 'task manager' that shows cpu utilization, etc. (or use top)

? can't install an old php version - because it's not supported by brew ?

    brew install shivammathur/php/php@7.4

    this installs it based on https://github.com/shivammathur/homebrew-php (got a list of supported versions).

? Always show the scroll bar ?
    In the Menu bar, click Apple Menu > System Preferences.
    Click General.
    Next to the "Show scroll bars" heading, select "Always."

? Mirror displays - on Monterey ?
    
    Apple > System Preferences > Displays > select Display settings > click on Use as: "Choose Mirror As"

? who is listening on port ?

    # on Mnterey (from here: https://stackoverflow.com/questions/4421633/who-is-listening-on-a-given-tcp-port-on-mac-os-x )

    sudo lsof -iTCP -sTCP:LISTEN -n -P

? osx - find all executable blabla files (not symlinks ?

    find . -perm +111 -type f  -name blabla

? osx - find all executable blabla files or symlinks ?

    find . -perm +111 -type f -or -type l -name blabla

? sort processes by virtual memory consumption /low memory does strange things on the mac.../

   
    On Osx:
        # can use Activity Monitor - a gui app.

        # top - start and sort by memory column
        top -o mem 

        # top - batch mode: write the table once and exit
        top -b

    On Linux:
        Better: inside top, press M to sort by memory 
        press c to show command line arguments of processes.


? regular expression editor (online) ?

    https://regex101.com/

? show process tree (on both osx and linux) ?

    htop -t

? netstat - show process names, who is listening on a given port ?

    sudo lsof -iTCP -sTCP:LISTEN -n -P
    sudo lsof -iUDP -sTCP:LISTEN -n -P

? disk space
    
    Show disk usage:
        df -a -H

    Show disk usage in directory
        du -ah  /usr/ | tail -1


    ? https://apple.stackexchange.com/questions/267165/why-is-devfs-full    
        devfs - is always full

? show memory usage in friendly terms

        # option -h is for Human friendly output !!!
        free -h

? http server with nc

    in bash shell:
        bash -c 'while true; do (echo -e "HTTP/1.1 200 OK\r\nConnection: close\r\n\r\nMy website has date function\t$(date)\n") | nc -l 8081; done'
        curl http://localhost:8081/

? hosted in docker
    with-nc : my docker image with bash and nc :
       
    Dockerfile-with-nc 
        FROM alpine:latest

        RUN apk update && apk upgrade --no-cache
        RUN apk add -y --no-cache bash netcat-openbsd 
            
    docker build -f Dockerfile-with-nc -t with-nc .

    docker run --rm  -p 8082:8081 -it with-nc /bin/bash -c 'while true; do (echo -e "HTTP/1.1 200 OK\r\nConnection: close\r\n\r\nMy website has date function\t$(date)\n") | nc -l 8081; done'
    curl http://localhost:8082/ 

   
    now curl doesn't exit - it doesn't get the FIN ?

? ssh access keys ?

    # generate the keypair. (entery empty keyphrase
    ssh-keygen -t rsa -b 4096 -f id_rsa -C "mmoser@perforce.com"
    # you get id_rsa (private key) id_rsa.pub (public key)

    # put the public key in authorized keys
    cat id_rsa.pub >>${HOME}/.ssh/authorized_keys

    # check that permission is 0600
    stat $HOME/.ssh/authorized_keys

    # save the keys somewhere in your reach. (id_rsa and id_rsa.pub)

    # write down the ip admin address - (gcp tells that in the console), hostname and user $USER

    # can connect with 
    ssh -i id_rsa <user>@<admin_ip>

? install docker on ubuntu ?  https://docs.docker.com/engine/install/ubuntu/

    sudo apt-get update -y
    sudo apt-get install -y \
        ca-certificates \
        curl \
        gnupg \
        lsb-release

    # docker gpg key    
    sudo mkdir -p /etc/apt/keyrings
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

    # set up the repository:
    echo \
 "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \

    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

    # make current user able to access docker 
    sudo setfacl -m user:$USER:rw /var/run/docker.sock

    #check that it is alive from non-root user
    docker run hello-world
    docker ps -a
    docker container prune -f

? list installed brew packages

    brew list

    # show the stuff with a gui
    brew list --cask

? install a second version of node

    brew install node@18

    # where is the installed version?

    /opt/homebrew/opt/node@18/bin/node
    Welcome to Node.js v18.15.0.
    Type ".help" for more information.
    >

    # or set up as the default version
    brew link --overwrite node@18
    node --version
    v18.15.0


? tcpdump ?

    # just show the packets on screen (without payload)
    sudo tcpdump -i any port 9010

    # -X adds payload
    sudo tcpdump -X -i any port 9010

    # dump packets into pcap file (can view them in wireshark, or read the file with tcpdump)
    sudo tcpdump -i any port 9010 -s 65535 -w out.pcap


? wireshark ?
    don't install via brew (goes wrong) - there is an installer on https://www.wireshark.org/


Linux
    # show kernel / cpu architecture / etc.
    uname -a

    # show which distro/distribution is on the current machine
    ls  /etc/*-release

    # get netstat on ubuntu
    apt-get install net-tools

    # fedora: they all call that package net-tools!
    dnf install net-tools

    # get external ip address
    dig +short txt ch whoami.cloudflare @1.0.0.1

    #get external ipv6
    dig -6 TXT +short o-o.myaddr.l.google.com @ns1.google.com


    # check if running under docker?

        apk add virt-what
        virt-what

        # or check for presence of files: /.dockerenv or /.dockerinit


    # what to do in a system without core-utils? (you get that a lot on containers - they like to have a limited runtime system, and you are not allowed to change too much... :-( )

        - no ps? list processes?

            ls -l /proc/*/exe

        - show command line for given PID

            cat /proc/PID/cmdline | xargs --null

           or if no xargs:

            cat /proc/PID/cmdline | tr \\0 \_

           ... and put in space instead of _



OS agnostic

    # also gets external ip address
    curl ifconfig.me
    curl icanhazip.com
    curl ipecho.net/plain
    curl ident.me
    curl bot.whatismyipaddress.com
    curl https://diagnostic.opendns.com/myip
    curl http://checkip.amazonaws.com
    curl http://whatismyip.akamai.com




======
Linux

ALPINE - list all installed packages

    apk info -vv

ALPINE - install rust with rustup
 
 # Newest upgradeables for used base image ... 
 apk upgrade --no-cache

 # libgcc - compatibility layer for libc (otherwise it searches for stuff that is not present on alpine - this one didn't work without it)
 apk add curl libgcc
 
 # using -y option, so that it does not ask any questions...
 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s  -- -y \

 # install specific version of rust ( --defualt-toolchain)
 curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s  -- -y  --default-toolchain=1.65.0 


=======
Ansible

    https://www.digitalocean.com/community/tutorial_series/how-to-write-ansible-playbooks

Terminology:
    Playbook: yaml file that defines the script of what is being done. Includes a sequence of Tasks (unit of execution)

    Modules: reusable entities for doing common tasks

    Tasks: you write these to specify what the playbook is doing.

Examples:

    Playbook yaml that sets up variables, the first task prints them ('hosts: all' - the task act on all defined 'hosts' - these are machines that we can control)

        ---
        - hosts: all
          vars:
            - username: sammy
            - home: /home/sammy   
          tasks:
            - name: print variables
              debug:
                msg: "Username: {{ username }}, Home dir: {{ home }}"


======

Get all artifact names for a public repository using the GITHUB rest api

    USER="robxu9"
    REPO="bash-static"

    curl -L -s -H "Accept: application/vnd.github+json" https://api.github.com/repos/${USER}/${REPO}/actions/artifacts | jq '.artifacts[].name' | sort | uniq

Get names of all artifacts that are not of expired state

    curl -L -s -H "Accept: application/vnd.github+json" https://api.github.com/repos/${USER}/${REPO}/actions/artifacts | jq --raw-output '.artifacts[] | select(.expired==false) | .name'


Get the latest release for a public repository

    curl -L -s  -H "Accept: application/json" https://github.com/${USER}/${REPO}/releases/latest | jq --raw-output .tag_name

====================


https://www.mongodb.com/docs/manual/reference/command/

Mongo (just like Elasticsearch) is a document oriented database. 

Mongo DB - holds a set of "collections"
    DB identified by name 
        - may not have following chars in it:  /, \, ., ", *, <, >, :, |, ?, $, space, \0, 
        - limited to 64 chars length
        - case sensitive

Mongo "collection" - similar to SQL table (but you don't have to define a schema in Mongo).
     - "collection" identified by its name ( "" is not a valid collection name, no $ chars or \0 chars in the name; may not start with prefix "system.")
     - can store very different json documents in a collection, but that's not a good idea!
     - Recommendation: store json docs of similar structure in the same collection (this allows you to do search)
        
    
A "document" in Mongo is an entry in a "collection" (it's a JSon file)- equivalent to a db "Row"; 
    - each "document" gets it's generated special _id field (12 bytes long - binary value)
    - id structure: 
            [4 bytes timestamp]
            [4 bytes - machine id]
            [2 bytes - pid]
            [3 bytes - incremented counter]
    - id field is always indexed, so you can lookup a document by it's id value!

Didn't know that mongo-db is used for search; so in what sense there is a equivalence with ElasticSearch ?

        - https://cloud.netapp.com/blog/cvo-blg-elasticsearch-vs-mongodb-6-key-differences

            - inserts are faster in mongo,  (mongo is written in C++, whereas elasticsearch is java)
                - better suited for document structured data (? don't you have a schema in elasticsearch too?)
            - text search is faster in elasticsearch

- mongo daemon (by default it listens on port 27017) 
  HTTP admin UI is listening port + 1000 (default: 28017)

- mongo shell: can enter expressions in json - that's very natural, as you can look at json documents as javacript maps/arrays. 
    - the document _id is represented as ObjectId class,.

    (or you can use alternative GUI based clients - like "Robo 3T")

    - shell commands
        var coll = db.getCollection("collectionName");  // lookup of collection object; or you can do as db.collectionName - but get an error if hte collection does not exist.
        
        - collection thing has all sorts of manipulation methods 
                - coll.insert({"person","Bob", "profession" : "Drummer"})  // insert doc, _id field is added upon insertion
                - var existingDoc = coll.findOne({"person" : "Bob"});    // find one doc, given a field
                - coll.update({"person":"Bob"}, { '$set' : { "profession":"Teacher" }}) ;  // first argument: query for doc like this, second argument: how to change the entry, once it was found. 
                - coll.find({}) // find all of them
    
        - update expressions can have all sorts of operators,
                $set - set the value of a field

                $inc, 

                $rename (rename field) 

                $push (adding to array)
                    coll.update({"person","Aob"}, {"$push", {"children" : { "Andrew" } } });

                $addToSet - like push, but prevents duplication of entries

                {$pop : { "key": 1}} - remove element from beginning of array "key", {$pop : { "key": -1}} - remove from the end 
                
                $pull - remove matching documents

                $each - push more than one entry (in an array)

                $slice - limit the size of an array to maximum n elements (or remove n - if n is negative)
                
                $sort - sort array by given key entry (provided that array has objects where all objects have the given key attribute)

            https://www.mongodb.com/docs/manual/reference/operator/update/
            https://www.mongodb.com/docs/manual/reference/operator/

        - find queries can have a lot of these (that one tries to be similar to sql SELECT...)  https://www.mongodb.com/docs/manual/reference/operator/query/    

    - Example session:

====

mongosh - the mongo shell

Accessing mongo: use the mogosh shell (mongo - that is the old shell, it was deprecated)
        (installation instructions: https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/ )

- in mongosh every map on a collection will return a Cursor object. You can convert it into a javascript array with toArray (big difference!!!)
    - but toArray() will take a lot of time - the cursor is evaluated just as it is needed, whereas toArray() will walk the whole query!!!
      (i had to do it, when translating an index from one db into another one, they were using the same index field values, so I had to get them all...)

Reference on db.collection.find - query, projection, etc. etc.

    https://www.mongodb.com/docs/manual/reference/method/db.collection.find/

mongosh

    working with really large datasets

    - makes sense to save a result to file (use node trick)
        fs.writeFileSync('file_big_array.json', JSON.stringify( variable_you_want_to_store ) )

    - in the new shell mongosh
        a=fs.readFileSync('file_big_array.json').toString()

---

    # show db names - select a particular db name as the current one
        show dbs
        use <dbname>

    # show name of all collections in current db.
        show collections

        db.getCollectionNames() // this one returns an iterator - won't show all of them.

    
    # other stuff when working in shell.

---

> c=db.createCollection("tst")
{ "ok" : 1 }

> c=db.getCollection("tst")

> c.update({_id: 123}, { '$addToSet': {'a': 1, 'b': 2, 'c' :3} }, { 'upsert': 'true'})
WriteResult({ "nMatched" : 0, "nUpserted" : 1, "nModified" : 0, "_id" : 123 })

> c.find({_id: 123})
{ "_id" : 123, "a" : [ 1 ], "b" : [ 2 ], "c" : [ 3 ] }

> c.update({_id: 123}, { '$addToSet': {'d': 4, 'e': 5, 'e' :5} }, { 'upsert': 'true'})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })

> c.find({_id: 123})
{ "_id" : 123, "a" : [ 1 ], "b" : [ 2 ], "c" : [ 3 ], "d" : [ 4 ], "e" : [ 5 ] }

---

Aggregation queries: (that's what they do for 'join')
/these get complex, but are terribly important/
    
    https://www.mongodb.com/docs/manual/core/aggregation-pipeline/#std-label-aggregation-pipeline


-----

Interesting detail: they often add a field that tells if an entry has been deleted or not.
That looks crazy - in SQL one would always delete a record, instead of adding a 'isactive' or 'isdeleted' field.


It seems that this is a kind of historical artifact: - from prior to 2019

https://stackoverflow.com/questions/6086245/is-remove-an-expensive-operation-in-mongodb

    
    this answer is a historical artifact from 2011 and it refers to the old MMAPv1 engine which was removed in 2019.

    MongoDB stores the data in a double linked list and so removing results is adjusting two links, the next link of the previous document and the previous link of the next document. There is no autocompacting. 
    Updating, if you have a value already stored, happens in place, changing one value. Now... you think, great, update one int instead of two pointers, surely faster! Not so -- you now need to index on this flag and creating indexes is "slow".


Culture, my ass...


-----

A function that shows all collections in all DB instances into a file - can run it from the mongo shell.

    function showAllCollectionsInAllDBS() {

        const fs=require("node:fs");

        let dbs = db.adminCommand(
           {
             listDatabases: 1
           }
        );


        let lstIt = ""

        for(let i=0; i < dbs.databases.length; ++i) {

            try {
                let dbname = dbs.databases[i].name;

                let dbnow = db.getSiblingDB(dbname);

                lstIt += "databaseName: " + dbname + "\n";

                let colNames = dbnow.getCollectionNames();

                for(let j=0; j< colNames.length;++j) {

                    lstIt += "\tcollection: " + colNames[j] + "\n";
                }
            } catch(er) {
                lstIt += "error: " + er;
            }
        }

        fs.writeFileSync("listdbs.txt", lstIt);

        console.log("see file llistdbs.tx for list of dbs an collections...");
    }

==========================

Reddis
    - does all sort of stuff (not just key value storage), 
        - they do work queues, and locks (with a specified time to life period). /what about reliability when usingg the work queue thing?/

        - https://github.com/resque/php-resque - that's a php library on top of redis lists (port of ruby library), Requests are queued as json objects. 
            Library has as a queue runner that listens on the list for jobs (maintained by github)



# port forwarding to service in k8s (inner expression finds pod name from some role name that pod adheres to 
# pod in cluster has redis running on port 6379 - local port is 6378 (other commands will use that number)

kubectl port-forward "$(kubectl get pods -l role=ROLE_NAME_OF_POD_IN_CLUSTER -o jsonpath="{.items[0].metadata.name}")" 6378:6379

# find a specific key

echo 'keys *' | redis-cli -p 6378 | grep  blabla
BLABLA-KEY-NAME

# check the type (this one is a list)

echo 'type BLABLA-KEY-NAME' | redis-cli -p 6378
list

# show how many elements are in list

echo 'LLEN BLABLA-KEY-NAME' | redis-cli -p 6378

# show the elements of the list between index 0 and 1 (not including 1)
echo 'LRANGE BLABLA-KEY-NAME 0 1' | redis-cli -p 6378


    
===========================

https://en.wikipedia.org/wiki/WebSocket

    - bi-directional communication
         https://www.rfc-editor.org/rfc/rfc6455 - the doc

    - initially over http, then client sends header: 'Connection: Upgrade' 'Upgrade: websocket' + additional headers, in order to change from http to websocket protocol.

        Request:

            GET /chat HTTP/1.1
            Host: server.example.com
            Upgrade: websocket
            Connection: Upgrade
            Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
            Origin: http://example.com
            Sec-WebSocket-Protocol: chat, superchat
            Sec-WebSocket-Version: 13

        Response:  Sec-WebSocket-Accept value is some sha signature over the request value Sec-WebSocket-Key + <some uuid>
    
            HTTP/1.1 101 Switching Protocols
            Upgrade: websocket
            Connection: Upgrade
            Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=

 
        Further requests in WebSocket are in frames

        - php Ratchet: 

            // bad news: you can't just upgrade a http connection of you php web request handler. You need to make a standalone script that listens to incoming WebSocket request.

            // simple chat server (that's how they do event driven stuff)

            $server =  Ratchet\Server\IoServer::factory(     // creates listening socke0wwt and event loop wrapper (IoServer), 
                                                             // the connect events are handled and create IoConnection)
                new Ratchet\Http\HttpServer(                 // parses http messages             
                    new Ratchet\WebSocket\WsServer(          // parsing of websocket handshake and frame messages 
                        new Chat()                           // application level class that handles messages
                    )
                ),
                8080
             );
             $server->run(); // run event loop.

            // server side action goes here
            class WsServer implements HttpServerInterface {

                
                // ComponentInterface - the application that consumes messages
                public function __construct(ComponentInterface $component) {

                public function onOpen(ConnectionInterface $conn, RequestInterface $request = null) {

                    // handle handshake
                    \Ratchet\RFC6455\Handshake\ServerNegotiator - handshake method ::: does the handshake.


        
        - Ratchet: working with sockets:
            
            Ratchet\Server\IoConnection - asynchronous socket handler
                - constructor receives derived class of \\React\\Socket\\ConnectionInterface

                - \React\Socket\Connection // wraps a stream/socket so that it does the ConnectionInterface!
                        public function __construct($resource, LoopInterface $loop)

        - every server passed to a ratched event loop handles:

            interface MessageInterface {

                function onMessage(ConnectionInterface $clientConnection, $msg);

----

    List of libraries for working with the docker engine api  : https://docs.docker.com/engine/api/sdk/

----
Python has integer division operator ... I would always divide 

    python
    >>> 9 / 5
    1.8
    >>> 9 // 5
    1

    >>> 10 / 3
    3.3333333333333335
    >>> 10 // 3
    3
    
    # this one also works...
    >>> int(10/3)
    3

----

https://realpython.com/pytest-python-testing/ (and others)

    - standard library comes with unittest framework ( https://docs.python.org/3/library/unittest.html ) - very much like JUnit (with setUp and tearDown methods)

        - but they have pytest : 
        - here you don't have to import/derive from base class and call it's assertion methods; most simple test is a function with test_ prefix and calls assert
        - pytest is an exe, it's the test runner - it gathers files with test_ prefix and calls all the test.

    - so how do I set up the test?
        - test function has parameter, the parameter is a function that is declared with @pytest.fixture decorator ; FIXTURE MEANS it produces a value used for the test.

        import pytest

        @pytest.fixture
        def example_fixture():
            return 1

        def test_with_fixture(example_fixture):
            assert example_fixture == 1

    - now functions with the @pytest.fixture annotation can also have parameters, these are most probably nested fixtures, so they call themselves in turn upon the setup of the value that is returned by the fixture.

    - great, now how do I tear down the test? (here things get complicated - you tear down the fixture!) generators!

      One way of tearing the fixture down.

        import pytest

        @pytest.fixture()
        def resource():
            print("setup")
            yield "resource"
            print("teardown")

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

        
    Another way: register a callback!

        @pytest.fixture()
        def resource(request):
            print("setup")

            def teardown():
                print("teardown")
            request.addfinalizer(teardown)
            
            return "resource"

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

    - lifetime of a fixture return value.

        When you use a fixture in several tests, then the returned value may be cached (similar to unittest - you don't want to have a setUp per test!)
        Now the value returned by the fixture has a lifetime, depending on the scope parameter of the fixture annotation!!!

        
        # scope="function" - if you use the resource parameter as a fixture, then the nested teardown method will be called FOR THE LAST TEST OF THIS SOURCE FILE!
        # scope="session"  - teardown called after the end of the run of the pytest test runner !
        # scope="module"   - teardown called after the last test in the current module
        # scope="package"  - teardown called after the last test in the current package.

        !!! BECAUSE OF ALL THE CACHING: YOU CAN'T INVOKE A FIXTURE TWICE FOR A TEST !!!
       
        @pytest.fixture(scope="function")
        def resource(request):
            print("setup")

            def teardown():
                print("teardown")
            request.addfinalizer(teardown)
            
            return "resource"

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

    - can parametrize tests. Each entry in the array will be passed in a separate test run of test_in_palindrome
      YOU ALSO PASS PAREMTERS TO FIXTURES THIS WAY !!!

        @pytest.mark.parametrize("palindrome", [
            "",
            "a",
            "Bob",
            "Never odd or even",
            "Do geese see God?",
        ])
        def test_is_palindrome(palindrome):
            assert is_palindrome(palindrome)
                
    - multiple parametrize annotations: will result in all combinations (cartesian product) of all test values!
        
            @pytest.mark.parametrize('foo', ['a', 'b', 'c'])
            @pytest.mark.parametrize('bar', [1, 2, 3])
            def test_things(foo, bar):
                assert foo in ['a', 'b', 'c']
                assert bar in [1, 2, 3]

      test report now looks as:

            test_foo.py::test_things[1-a] PASSED
            test_foo.py::test_things[1-b] PASSED
            test_foo.py::test_things[1-c] PASSED
            test_foo.py::test_things[2-a] PASSED
            test_foo.py::test_things[2-b] PASSED
            test_foo.py::test_things[2-c] PASSED
            test_foo.py::test_things[3-a] PASSED
            test_foo.py::test_things[3-b] PASSED
            test_foo.py::test_things[3-c] PASSED


-----
BNF grammars 

    Python got it's syntax in the reference
        https://docs.python.org/3/reference/grammar.html

    BNF grammars for javascript?

    You got a lot of them in ANTLR!
        - https://github.com/antlr/grammars-v4/tree/master/javascript/javascript

    C syntax BNF
        - https://cs.wmich.edu/~gupta/teaching/cs4850/sumII06/The%20syntax%20of%20C%20in%20Backus-Naur%20form.htm

    Pascal syntax BNF
        - https://condor.depaul.edu/ichu/csc447/notes/wk2/pascal.html

===

Javascript - iterating over map entries (node session)

    > a={a:1,b:2,c:3}
    { a: 1, b: 2, c: 3 }
    > Object.entries(a).map(([k,v],i) => { console.log("k: " + k + " v: " +v + " i :" +i); return k } )
    k: a v: 1 i :0
    k: b v: 2 i :1
    k: c v: 3 i :2
    [ 'a', 'b', 'c' ]

Javascript generators/iterators - they are needed to provide values for a 'for' loop.

Here is how you do an iterable object (that's an object that has special function to return an iterator, so that it can be used in a for loop)

    class Foo {
         constructor() {
             this.data = [1,2,3];
         }
    }

    //
    // class Foo is 'iterable' if it has a member named Symbol.iterator - that returns an iterator object.
    //
    // need to add the function named as special symbol: Symbol.iterator to protoype
    // can't just define a member function with that name ???!
    // now a new Foo object will bet 'iterable' as it has an iterator.
    //
    Foo.prototype[Symbol.iterator] = function() {
         let pos = 0;
         let data = this.data;

         // iterator is an object with the next function, that returns an object upon each call
         // returned object/dict has done: - are we done iterating? and value: - value to return to for loop
         return {
             next: function() {
                     return {
                         done: !(pos in data), // are we done with the iteration?
                         value: data[pos++]    // value returned by iterator
                     }
             }
         }
     }


    f = new Foo();
    for(let c of f) {
        console.log(c);
    }

Here is how you do a generator

    function *range(from,to,step) {
        let val = from;
        while(val < to) {
            yield val;
            val += step;
        }
    }

    for(let i of range(1,10,2)) {
        console.log(i);
    }

Calling generators directly

    > a=range(1,3,1)
    Object [Generator] {}
    > a.next()
    { value: 1, done: false }
    > a.next()
    { value: 2, done: false }
    > a.next()
    { value: undefined, done: true }
    > a.next()
    { value: undefined, done: true }


Generators can call other generators - but they have to do the call via yield* 

    function *blabla() {
        return 3;
    }

    function* func1() {
      yield 42;
    }

    function regFunc() {
        return 43;
    }

    function* func2() {

        // can call regular function from generator - works as expected
        let bl = regFunc();
        console.log(bl);

        // calling a generator function returns a generator object
        let er = blabla();
        console.log("blabla returns: " + er);

        // this way it returns 3
        er = yield *blabla();
        console.log("blabla returns: " + er);

        yield* func1();
        yield* func1();
        yield* func1();
    }

    for(a of func2()) {
        console.log(a);
    }


It turns out that python has a similar thing to delegate form one generator to the other. Welcome to 'yield from' - beginning with python 3.8

    def my_range(f,t,st):
        while f < t:
            yield f
            f += st

    # yield from - to delegate to another generator !!! (from python 3.8)
    def range_three(f,t):
        yield from my_range(f,t,3)

    for n in my_range(1,10,1):
        print(n)

    print("generator calling generator")

    for n in range_three(1,10):
        print(n)


===

    javascript express for http server:

        const express = require("express"),
              app = express()

        app.get("/", (req, res) => {
            res.send("Hello, TREND OCEANS!")
            console.log("finished handling request");
        })

        console.log("before listen");
        app.listen(3000, console.log(`Server started on port 3000`))
        console.log("after listen");

    output:

        node t1.js
        before listen
        Server started on port 3000
        after listen
        finished handling request

=== 

    javascript sleep:

        function onTm() {
            console.log("onTm");
        }

        console.log("before sleep");
        setTimeout(onTm,1000);
        console.log("after sleep");

    output:
        node t.js
        before sleep
        after sleep
        onTm

====
http in node:
    node has in-built http module
        

    express - server
        http://expressjs.com/en/5x/api.html


====

NodeJS - event loop

    https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/

    Phases of the event loop
        - timers: (timers are best effort - not exactly scheduled)
 
----

Binary data in nodejs
(The javascript standard later added it's own stuff:  ArrayBuffer, Uint8array, Float64array etc.. but they don't use that in node)

> let b= new Buffer.alloc(10); // allocates zero allocated buffer
undefined

> typeof(a)
'object'
> a.constructor.name
'Buffer'

> b
<Buffer 00 00 00 00 00 00 00 00 00 00>

> b[2]
0

> b[2]=1
1

> b[2]
1

> b
<Buffer 00 00 01 00 00 00 00 00 00 00>

    
> b.length
10
-----

npm - creating package

    - search if package exists in: https://www.npmjs.com/search?q=prs
    - npm init new-package-name  # makes a default package.json ( package.json reference: https://docs.npmjs.com/cli/v9/configuring-npm/package-json )

    - npm adduser  # use this to log into your npm account. before publishing: need to be logged into your account. 

    - set NPM_TOKEN environment variable to an npm account API token with publish privs.

    - npm publish --access public # from directory that has package.json - and all of the files mentioned in package.json !!!

    - each publish needs to increment the package version number (in package.json)

Show root folder where globally installed npm modules are:

    npm root -g 

install YAML package into the global dir
    npm i yaml -g

    To use globally installed package:

    To use locally installed package:
        need to set environment variable: 
        export NODE_PATH=$(npm root -g)

install YAML locally
    npm i yaml

    module is installed to $PWD/node_modules
    
    To use locally installed package:
        need to set environment variable: 
        export NODE_PATH=$PWD

If you are part of an installed package, then you don't need to do this!!! (can include anything in the current scope)


List all globally installed packages

    npm list -g 


--- 

Viewing ssh logs:

With systemd

    journalctl -fu ssh

otherwise 

    sudo grep sshd /var/log/auth.log

In /etc/ssh/sshd_config put

    LogLevel VERBOSE


======

Formatting text in whatsapp messages

https://faq.whatsapp.com/539178204879377

    ```...```` - makes it fixed font.

======

Python virtual environments

    # create the virtual env
    python -m venv myvenv

    # activate the virtual env
    source myvenv/bin/activate
    #(prompt changes after that)

    # if there is a requirements.txt file:
    pip install -r requirements.txt

    # create requirements (after having installed some pip packages)
    pip freeze > requirements.txt

    Benefit: pip install puts up everything in /site/packages - shared thing, can get clogged up. This way every environment is kind of isolated.

    # deactivate virtual environment myvenv
    deactivate


Pip show installed package version of numpy
     pip show numpy


Show all packages
     pip list

Show all packages as a tree (which one requires which one)

   # need to install the thing
   pip install pipdeptree

   pipdeptree


   # extremely usefull thing - i wonder why they don't have something like that as part of pip, without having to install some package, that is.


Force upgrade a package to latest?

    pip install --upgrade --force-reinstall cryptography

=========

Workign with binary data.

(Perl and Python are similar to this respect - they pack/unpack the fields - according to some printf-like text spec (the spec is called 'the template')
Nodejs/Javascript doesn't do that - they have java like readUint8, writeUint8 functions for binary de-serialisation.


https://perldoc.perl.org/perlpacktut


==

GITHUB

Want to submit a pull request to some repository other than your own stuff?

    https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork

    - need to fork that repository first.


--------

AWS lambdas

    - tutorial: https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html



----

Integration testing approaches :  testing interaction of groups of software models / components.

    https://en.wikipedia.org/wiki/Integration_testing

    Big bang - create a complete system (consisting of all components) ; run test against this configuration

        Problem: 
            the complete system hardly works during development (lot's of downtime for the developers)

            https://www.tutorialspoint.com/software_testing_dictionary/big_bang_testing.htm

            - Defects present at the interfaces of components are identified at very late stage as all components are integrated in one shot.
            - It is very difficult to isolate the defects found.
            - There is high probability of missing some critical defects, which might pop up in the production environment
            - It is very difficult to cover all the cases for integration testing without missing even a single scenario.

    Bottom up - test lower level modules in isolation, then combine them with next upper level - until you test the whole system

        needs test drivers : to call the lower level models (in the absence of a complete system)

        But it is easier to test the system!  https://testsigma.com/blog/difference-between-top-down-and-bottom-up-integration-testing/

            - easier to find problems when limited amount of components are interacting
            - easier to come up with a more complete test scenario
            - less downtime than with big bang system

        Problems
            - you need to develop an army of these test drivers
            - more time to run the test?


    Top - down - reverse direction.

        first test the higher level models, note that lower level models are substituted by test stubs (that simulate the working of the lower level models)

        (says it's more suitable for procedural systems, less so for Object Oriented systems)
        
            
    Sandwich level - combine bottom-up and top-down

        (interesting, which companies use this one? Sounds like a lot of work)

----

Dynamic programming - can be used to problems that have 'optimal substructure' and 'Overlapping subproblems'


    optimal substructure - optimal solution to the problem can be constructed from the optimal solutions of its subproblems. 

    Overlapping subproblems refer - a problem can be broken down into smaller subproblems, and the solutions to these subproblems are used repeatedly to solve the original problem


Solution can be bottom up - table driven (to store the repeated solutions) 
 or top-down - recursive (but need memoization - a hash table to store results of previously solved common subproblems)

- doing the recursive solution with memoization is easier, but the table driven solution is more efficient during runtime.

Doing some examples as test cases here:  https://github.com/MoserMichael/jscriptparse/tree/main/leetcode

Also lots of stuff here: https://en.wikipedia.org/wiki/Category:Dynamic_programming

===============

CHATGTP on it's api parameters:

    - The "temperature" field in a ChatGPT API request is a parameter that determines the level of randomness or creativity in the generated response.

    Temperature is a measure of the entropy of the system, and in the context of natural language processing, it determines the level of variation in the model's responses. A higher temperature value will result in more varied and unpredictable responses, while a lower temperature value will produce more conservative and predictable responses.

    In the context of a ChatGPT API request, the temperature value can be set to a value between 0 and 1, with 0 being the most conservative and 1 being the most creative. A temperature value of around 0.5 is often used as a default, as it strikes a balance between creativity and coherence in the generated responses.

    Note that the actual temperature value used in generating the response can be slightly different from the value specified in the API request, as the model may adjust the temperature value based on the context and other factors.


    - The maxTokens parameter is important because it allows you to control the length and specificity of the model's output. If you set maxTokens to a low value, the generated text will be relatively short and focused on the most salient aspects of the prompt. If you set maxTokens to a high value, the generated text will be longer and more detailed, potentially covering a wider range of topics and ideas.

    However, setting maxTokens too high can also lead to the model generating text that is unfocused or irrelevant to the prompt. Therefore, it is important to experiment with different values of maxTokens to find the optimal balance between length and relevance for your specific application.

    It is worth noting that different engines may have different maximum values for maxTokens


    >Is there an equation that returns the number of api tokens consumed by the request, depending on the max_token input parameter of the api?

    Yes, there is an equation that can be used to estimate the number of tokens consumed by an API request to ChatGPT, based on the max_tokens parameter that you set in your request.

    The equation is:

        tokens = max(max_tokens, 4) + ceil(log2(num_samples))

        num_samples is the number of samples or responses that you request from the API

        It's worth noting that this equation is an estimate, and the actual number of tokens consumed by a request may vary depending on factors such as the length and complexity of the generated text, as well as the specific parameters and settings used in the request.


-----
IntelliJ

    Command+L  - goto line/column 
        12:5     - move to line 12 column 5
        12         move to line 12 column 1

    Option+Fn+F7  - or from context menu 'Find Usage' - find where symbol under cursor is used in this project


Visual Studio Code

    
    Run commands in terminal
        Command + Shift + P ; type 'Shell Command install 'code' command in path'

        View / Terminal

    Fn+F12  : Goto definition of symbol. (very valuable thing!!!)

    Ctrl+G  : Goto line

-- learning how to do somthing in minecraft: (also looking at mineflayer - nodejs module for bots)

(i am an idiot should have looked it up years ago: 'Parent's guide to minecraft' https://www.youtube.com/watch?v=t9eJ6TLtfd8 ... )

Modes: can switch modes
    Creative mode: the monsters can't hurt you
    Survival mode: start with nothing, have to collect and build stuff. Monsters can hurt you. Have earn stuff.
    Adventure mode: can't place/destroy blocks (need tools). 


How to use it
Navigation (just like in vim ;-)
    a d  : left,right
    w s  : up down
    swipe mouse : turn around

    space space  : (in creative mode) - start flying
    shift : gets you down again

Building
    e : bring up the 'inventory window' - the stuff that you can build with.
        
        chose type of block and put them into the 'inventory' (you can build with it)
        
        press number to switch between the selected slot in the inventory (or mouse wheel)


Messages 
    / - bring up the chat window

    # now the bot will get a 'whisper' event. (the bot will get it!)
    /msg BotyBot fly
        
    # public chat on minecraft with say (they don't seem to have that in creative mode without microsoft authentication)
    # this one results in a event for the following
    # bot.on('chat', (username, text) => { } )

    /say BotyBot fly


Coordinates in minecraft
    https://minecraft.fandom.com/wiki/Coordinates#:~:text=The%20y%2Daxis%20indicates%20how,block%20equals%201%20cubic%20meter

    (x,y,z) 
        x,z - coorinates on the plane
            x-axis indicates the player's distance east (positive) or west (negative) - longitude
            z-axis indicates the player's distance south (positive) or north (negative) of the origin point—i.e., the latitude,

        y   - the elevation from surface (

In game:
    F3 (mac fn-F3) : dumps all sort of stuff on the screen (including players coordinates)



--------

Not entirely sure: GPT-4 is still inventing stuff, fixing it may be harder than they admit - for example you can still trick an ML vision model with a random image. (also: there must be *some* FUD in the marketing...) See:

https://www.youtube.com/watch?v=SjhIlw3Iffs&t=1068s

--------
https://philbooth.me/blog/nine-ways-to-shoot-yourself-in-the-foot-with-postgresql

Postgress perfrmance tips
    
    - it has work_mem parameter (size of memory reserved for a DB query)
    - if it's too small it starts paging ON EACH QUERY (OMG)
    - says the following formula does the approximation:

        work_mem = ($YOUR_INSTANCE_MEMORY * 0.8 - shared_buffers) / $YOUR_ACTIVE_CONNECTION_COUNT 


--------

https://www.infoq.com/news/2023/04/virtual-threads-arrives-jdk21/

JEP 444: Virtual Threads Arrive in JDK 21
    
    - idea is to add co-thread to JDK (now golang is lossing it's big advantage over JDK - goroutines?)

    - JEP444 is now 'Proposal' status in JDK21 (used to be candidate in JDK20)

    - 'The JDK can now run up to 10,000 concurrent virtual threads on a small number of operating system (OS) threads'
    
    - 

-----

Python turtle package on OSX

    - install python via brew

    brew install python3

    - get version
    
    /opt/homebrew/bin/python3 --version
    Python 3.11.4

    - install python tkinter binding for the same version!

    brew install python-tk@3.11

--

Most member functions of the Turtle class (and Screen class) are also available as global functions. Here is how they do it in the turtle module:

Interesting form of metaprogramming in python: see https://github.com/python/cpython/blob/3.11/Lib/turtle.py
 

    - pass names of functions to _make_global_funcs  

_tg_turtle_functions = ['back', 'backward', 'begin_fill', 'begin_poly', 'bk',
        'circle', 'clear', 'clearstamp', 'clearstamps', 'clone', 'color',
        'degrees', 'distance', 'dot', 'down', 'end_fill', 'end_poly', 'fd',
        'fillcolor', 'filling', 'forward', 'get_poly', 'getpen', 'getscreen', 'get_shapepoly',
        'getturtle', 'goto', 'heading', 'hideturtle', 'home', 'ht', 'isdown',
        'isvisible', 'left', 'lt', 'onclick', 'ondrag', 'onrelease', 'pd',
        'pen', 'pencolor', 'pendown', 'pensize', 'penup', 'pos', 'position',
        'pu', 'radians', 'right', 'reset', 'resizemode', 'rt',
        'seth', 'setheading', 'setpos', 'setposition', 'settiltangle',
        'setundobuffer', 'setx', 'sety', 'shape', 'shapesize', 'shapetransform', 'shearfactor', 'showturtle',
        'speed', 'st', 'stamp', 'tilt', 'tiltangle', 'towards',
        'turtlesize', 'undo', 'undobufferentries', 'up', 'width',
        'write', 'xcor', 'ycor']

_make_global_funcs(_tg_turtle_functions, Turtle,
                   'Turtle._pen', 'Turtle()', _turtle_docrevise)

        - _make_global_funcs  formats the code of the global forwarding functions as a string
        - the generated function chesk if the global variable Turtle._pen is set, if not then it creates a turtle and puts it into this variable
        - then calls the method with the same name as that of generated global with parameters / forwards the call.
        - the parameter list is is retrieved from the class objects (getmethod / getmethparlist)
        - exec( VARIABLE_WITH_FUNCTION_CODE )  - interprets the function.
        - some fudging with the docstrings.

__func_body = """\
def {name}{paramslist}:
    if {obj} is None:
        if not TurtleScreen._RUNNING:
            TurtleScreen._RUNNING = True
            raise Terminator
        {obj} = {init}
    try:
        return {obj}.{name}{argslist}
    except TK.TclError:
        if not TurtleScreen._RUNNING:
            TurtleScreen._RUNNING = True
            raise Terminator
        raise
"""

def _make_global_funcs(functions, cls, obj, init, docrevise):
    for methodname in functions:
        method = getattr(cls, methodname)
        pl1, pl2 = getmethparlist(method)
        if pl1 == "":
            print(">>>>>>", pl1, pl2)
            continue
        defstr = __func_body.format(obj=obj, init=init, name=methodname,
                                    paramslist=pl1, argslist=pl2)
        exec(defstr, globals())
        globals()[methodname].__doc__ = docrevise(method.__doc__)






