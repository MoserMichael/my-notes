git/github

        # 'deleted by us' during merge or cherry-pick
            # file x was added in the commit being cherry-picked
            # file x was deleted in the branch that you cherry-pick into.
                git add x  # if you want to keep the file
                git rm  x  # if you want to delete the file.

        # 'deleted by us' during rebase - now 'us' and 'them' is reversed !!!!
            # file x was added in the branch that is being rebased
            # file x was removed in the branch that is brought in
            

        # shallow clone of linux kernel (--depth 1 - get commit history of the last commit) (much faster to clone)
        git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git linux-git --depth 1

        # sort files by extension (rev - reverts each line of the output, so that the extension (in reverse) comes first)
        git ls-files | rev | sort | rev

        # Get all commits by user (that can be useful during performance reviews...)
        # select all commits done by user

        git log --author=<user name that appears in commit messages>

        # get the commit message of all commits by user.
        git log --author=<user name that appears in commit messages> --format=%s 

        # in case that the first string is the name of the issue - get all the issues that the user did.
        git log --author=<user name that appears in commit messages> --format=%s | awk '{print $1}' | sort | uniq


        # github.com ::: open a pull request on some other repository https://github.com/AwesomeUser/AwesomeProject

        - on original repo page: create a fork (press the 'fork' button)
        - after forking: you have that repository under your own github user  https://github.com/YourOwnUser/AwesomeProject

        - clone that forked repository.
        - create a new branch of the main branch.
            git branch -m my_new_pr
        - add your changes and commit
        - set the UPSTREAM of the branch: 
            git remote add upstream https://github.com/AwesomeUser/AwesomeProject
        - push the new branch:
             git push -u origin my_new_pr
        - push command writes a long text to the console, that text includes a link to open the new pull request.   
    
vimdiff
     - space insensitive comparison:

        :set diffopt+=iwhiteall


Mac/OSX tricks

? built-in something is very different from what you expect ?

    # very true for sed - will install gsed 
    brew install gnu-sed

    gsed -i 's/blabla/blublu/g' foo.txt

    # to set it up as sed - instead of gsed (still figure the path)
    brew install --default-names gnu-sed

? force quit an application

    command + option + escape - brings up a 'task manager'

    'Activity monitor' - a better 'task manager' that shows cpu utilization, etc. (or use top)

? can't install an old php version - because it's not supported by brew ?

    brew install shivammathur/php/php@7.4

    this installs it based on https://github.com/shivammathur/homebrew-php (got a list of supported versions).

? Always show the scroll bar ?
    In the Menu bar, click Apple Menu > System Preferences.
    Click General.
    Next to the "Show scroll bars" heading, select "Always."

? Mirror displays - on Monterey ?
    
    Apple > System Preferences > Displays > select Display settings > click on Use as: "Choose Mirror As"

? who is listening on port ?

    # on Mnterey (from here: https://stackoverflow.com/questions/4421633/who-is-listening-on-a-given-tcp-port-on-mac-os-x )

    sudo lsof -iTCP -sTCP:LISTEN -n -P

? osx - find all executable blabla files (not symlinks ?

    find . -perm +111 -type f  -name blabla

? osx - find all executable blabla files or symlinks ?

    find . -perm +111 -type f -or -type l -name blabla

? sort processes by virtual memory consumption /low memory does strange things on the mac.../

   
    On Osx:
        # can use Activity Monitor - a gui app.

        # top - start and sort by memory column
        top -o mem 

        # top - batch mode: write the table once and exit
        top -b

    On Linux:
        Better: inside top, press M to sort by memory 
        press c to show command line arguments of processes.


? regular expression editor (online) ?

    https://regex101.com/

? show process tree (on both osx and linux) ?

    htop -t

? netstat - show process names, who is listening on a given port ?

    sudo lsof -iTCP -sTCP:LISTEN -n -P
    sudo lsof -iUDP -sTCP:LISTEN -n -P

? disk space
    
    Show disk usage:
        df -a -H

    Show disk usage in directory
        du -ah  /usr/ | tail -1


    ? https://apple.stackexchange.com/questions/267165/why-is-devfs-full    
        devfs - is always full

? show memory usage in friendly terms

        # option -h is for Human friendly output !!!
        free -h

? http server with nc

    in bash shell:
        bash -c 'while true; do (echo -e "HTTP/1.1 200 OK\r\nConnection: close\r\n\r\nMy website has date function\t$(date)\n") | nc -l 8081; done'
        curl http://localhost:8081/

? hosted in docker
    with-nc : my docker image with bash and nc :
       
    Dockerfile-with-nc 
        FROM alpine:latest

        RUN apk update && apk upgrade --no-cache
        RUN apk add -y --no-cache bash netcat-openbsd 
            
    docker build -f Dockerfile-with-nc -t with-nc .

    docker run --rm  -p 8082:8081 -it with-nc /bin/bash -c 'while true; do (echo -e "HTTP/1.1 200 OK\r\nConnection: close\r\n\r\nMy website has date function\t$(date)\n") | nc -l 8081; done'
    curl http://localhost:8082/ 
    
    now curl doesn't exit - it doesn't get the FIN ?

? ssh access keys ?

    # generate the keypair. (entery empty keyphrase
    ssh-keygen -t rsa -b 4096 -f id_rsa -C "mmoser@perforce.com"
    # you get id_rsa (private key) id_rsa.pub (public key)

    # put the public key in authorized keys
    cat id_rsa.pub >>${HOME}/.ssh/authorized_keys

    # check that permission is 0600
    stat $HOME/.ssh/authorized_keys

    # save the keys somewhere in your reach. (id_rsa and id_rsa.pub)

    # write down the ip admin address - (gcp tells that in the console), hostname and user $USER

    # can connect with 
    ssh -i id_rsa <user>@<admin_ip>

? install docker on ubuntu ?  https://docs.docker.com/engine/install/ubuntu/

    sudo apt-get update -y
    sudo apt-get install -y \
        ca-certificates \
        curl \
        gnupg \
        lsb-release

    # docker gpg key    
    sudo mkdir -p /etc/apt/keyrings
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

    # set up the repository:
    echo \
 "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \

    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

    # make current user able to access docker 
    sudo setfacl -m user:$USER:rw /var/run/docker.sock

    #check that it is alive from non-root user
    docker run hello-world
    docker ps -a
    docker container prune -f

? list installed brew packages

    brew list

    # show the stuff with a gui
    brew list --cask

? install a second version of node

    brew install node@18

    # where is the installed version?

    /opt/homebrew/opt/node@18/bin/node
    Welcome to Node.js v18.15.0.
    Type ".help" for more information.
    >

    # or set up as the default version
    brew link --overwrite node@18
    node --version
    v18.15.0


? tcpdump ?

    # just show the packets on screen (without payload)
    sudo tcpdump -i any port 9010

    # -X adds payload
    sudo tcpdump -X -i any port 9010

    # dump packets into pcap file (can view them in wireshark, or read the file with tcpdump)
    sudo tcpdump -i any port 9010 -s 65535 -w out.pcap


? wireshark ?
    don't install via brew (goes wrong) - there is an installer on https://www.wireshark.org/





Linux
    # show kernel / cpu architecture / etc.
    uname -a

    # show which distro/distribution is on the current machine
    ls  /etc/*-release

    # get netstat on ubuntu
    apt-get install net-tools

    # fedora: they all call that package net-tools!
    dnf install net-tools

    # get external ip address
    dig +short txt ch whoami.cloudflare @1.0.0.1

    #get external ipv6
    dig -6 TXT +short o-o.myaddr.l.google.com @ns1.google.com


    # check if running under docker?

        apk add virt-what
        virt-what

        # or check for presence of files: /.dockerenv or /.dockerinit

OS agnostic

    # also gets external ip address
    curl ifconfig.me
    curl icanhazip.com
    curl ipecho.net/plain
    curl ident.me
    curl bot.whatismyipaddress.com
    curl https://diagnostic.opendns.com/myip
    curl http://checkip.amazonaws.com
    curl http://whatismyip.akamai.com




======
Linux

ALPINE - list all installed packages

    apk info -vv

=======
Ansible

    https://www.digitalocean.com/community/tutorial_series/how-to-write-ansible-playbooks

Terminology:
    Playbook: yaml file that defines the script of what is being done. Includes a sequence of Tasks (unit of execution)

    Modules: reusable entities for doing common tasks

    Tasks: you write these to specify what the playbook is doing.

Examples:

    Playbook yaml that sets up variables, the first task prints them ('hosts: all' - the task act on all defined 'hosts' - these are machines that we can control)

        ---
        - hosts: all
          vars:
            - username: sammy
            - home: /home/sammy   
          tasks:
            - name: print variables
              debug:
                msg: "Username: {{ username }}, Home dir: {{ home }}"


======

Get all artifact names for a public repository using the GITHUB rest api

    USER="robxu9"
    REPO="bash-static"

    curl -L -s -H "Accept: application/vnd.github+json" https://api.github.com/repos/${USER}/${REPO}/actions/artifacts | jq '.artifacts[].name' | sort | uniq

Get names of all artifacts that are not of expired state

    curl -L -s -H "Accept: application/vnd.github+json" https://api.github.com/repos/${USER}/${REPO}/actions/artifacts | jq --raw-output '.artifacts[] | select(.expired==false) | .name'


Get the latest release for a public repository

    curl -L -s  -H "Accept: application/json" https://github.com/${USER}/${REPO}/releases/latest | jq --raw-output .tag_name
======

Kubernetes:
    - extracting interesting fields from a set of pods:


         Pod name and pod IP addreess

         kubectl get pods -o custom-columns=NAME:metadata.name,IP:status.podIP

         An overview of what the containers in pods are running: (name,image,command,arguments of commands, environment) - and pod IP

         kubectl get pods -o json | jq '.items[] | {name: .metadata.name, image: .spec.containers[].image, command: .spec.containers[].command, args: .spec.containers[].args, env: .spec.containers[].env, status.podIP, podIP: .status.podIP }'

    - list all ingresses (with ip addresses)

        kubectl get ingress --all-namespaces

    - https://github.com/ahmetb/kubectx 
        contains utils: 
            kubectx - for switching kubectl context (each kubectl context defines how to access a particular cluster)
                      !! kubectl configuration for switching clusters is defined in $HOME/.kube/config !!
                      !! if you want to switch using a different cluster with kubectl command line (specify configuration directory with --kubeconfig)
                            kubectl --kubeconfig DIFFERENT_DIR_FOR_KUBE_CONFIG !!

            kubens - setting the default kubectl namespace 

     - configuring a cluster
            a bit involved, see here: https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/

     - list configured clusteres

            kubectl config get-contexts

            # kubectx - a utility that helps with contexts
            kubectx 

     - switch to another context

            kubectl config use-context CONTEXT_NAME
    
            # ... or with kubectx
            kubectx CONTEXT_NAME

      - switch to previous context (kubectl can't do that)
            kubectx - 

      - run a shell in a pod (pod-name - listed with $(kubectl get pods)

          kubectl -it pod-name bash

          kubectl -it pod-name sh


=======

Show logs of all pods (name of pod/container is in each line) - and grep through them

    kubectl get pods | tail -n +2  | awk '{ print $1 }' | xargs -I{} kubectl logs --all-containers=true --ignore-errors=true --prefix=true {} | grep -i error

    kubectl get pods | tail -n +2  | awk '{ print $1 }' | xargs -I{} kubectl logs --prefix=true {} | grep -i error


=====

Need to know which node a pod is running on?
    
    # -o wide shows a column with the node
    kubectl get pods -o wide

======

Docker reference

https://docs.docker.com/engine/reference/commandline/docker/

---

How to build with the Dockerfile in the current directory

    docker build --progress=plain --no-cache . -t resulting-image-tag
       --progress=plain : show output on the screen! old-style! you see what is happening!
       --no-cache       : don't use the cache to pick up something old
       .                : current dir
       -t blalba        : tag the resulting image as blabla

---

docker image ls     :: shows docker images (except for intermediate images)
                    :: what is an intermediate image? Each step in docker build creates an intermediate images (these are cached to improve build time!)

docker image ls -a  :: shows all docker images (including intermediate images)


    docker image ls
        REPOSITORY               TAG       IMAGE ID       CREATED        SIZE
        docker/getting-started   latest    157095baba98   5 months ago   27.4MB


    # you have more fields in the json output (no idea what they all mean).
    docker image ls --format='{{json .}}'
        {"Containers":"N/A","CreatedAt":"2022-04-11 18:25:34 +0300 IDT","CreatedSince":"5 months ago","Digest":"\u003cnone\u003e","ID":"157095baba98","Repository":"docker/getting-started","SharedSize":"N/A","Size":"27.4MB","Tag":"latest","UniqueSize":"N/A","VirtualSize":"27.37MB"}



---
running a container - with a given image (if image is not present then they try to pull it from the docker registry)

    docker run fedora:latest ls / 

        - run $(ls /) command in the container, and print the output of the command.
        - the command exits and the container is no longer running

            docker ps -a    
            CONTAINER ID   IMAGE           COMMAND   CREATED              STATUS                          PORTS     NAMES
            d35b0813ce79   fedora:latest   "ls /"    About a minute ago   Exited (0) About a minute ago             eager_mcclintock

        - the entry of the exited docker container remains - until it is cleaned up by running $(docker container prune -f) 

    docker run --rm fedora:latest ls /  

        - same as before, just that the entry for the exited docker container gets cleaned up, when the docker exits. (--rm says so)


    docker run -d fedora:latest ls /
8bd31381ae93712414e02c58589a8562be9739ccc01d0d9baf93f39a6ab32505

        -d option says not to show the docker command output to screen, instead it prints the id of the docker container. You can now use that id to stop/resume/kill the container (or to $(docker wait CONTAINER_ID) until it exits)


    docker run -d -p 9000:8000 -v $PWD:/mnt/loc fedora:latest /bin/sleep infinity

        -d                  : runs as daemon (you get the docker container id in stdout)
        -p 9000:8000        : maps tcp port 9000 (outside the container) to port 8000 (inside the container)
        -v $PWD:/mnt/loc    : mounts current directory to /mnt/loc inside container

        /bin/sleep infinity : the container does nothing, just keeps running (but you can attach a terminal to it that runs in the container, and it enjoys the mapped port and mounted directory!)

        ## !!! sleep infinity - works on Linux, this is a GNU extension. !!!

    # attach to the running container (it will not loose it's stuff - while the container is running!) 
    # (you can't attach to a stopped container - naturally)

    docker exec -it fedora:latest bash

    # may use the following to run a container, maybe that's more general.
    # sh -c 'while [ true ]; do sleep 5m; done'
    
    docker run -d -p 9000:8000 -v $PWD:/mnt/loc alpine:latest  sh -c 'while [ true ]; do sleep 5m; done'  

        for bridge network mode: -p 9000:8000 - port 9000 on the host is mapped to port 8000 inside the container. (listening port inside container can accept connections from the outside!)
        -v $PWD:/mnt/loc    :: map the home directory on host to /mnt/loc inside the container


---

Running a container, and overriding the entry point of the container (--entrypoint can tell what to run, provided the process is installed on the image)

    docker run  -it --entrypoint /bin/sh php:7.4-cli

In addition: mount the current directory on the host into the container - to path /mnt/loc ; ls /mnt/loc - just like the local directory when running the command!

    docker run  -v $PWD:/mnt/loc -it --entrypoint /bin/sh php:7.4-cli

You can run the whole thing in detached mode ! (and attach to it later)
This has the advantage that the docker keeps running, until the system is rebooted. (you can get a similar effect with tmux :- ) 

    docker run -d  -v $PWD:/mnt/loc -it --entrypoint /bin/sh php:7.4-cli


Run me a linux, with my stuff:

    docker run  -v $HOME/mystuff/:/mystuff -it fedora:latest   /bin/bash

!!!!! run a docker inside docker DIND !!!!
    
    # problem - elevated privileges 

    docker run --privileged -d --name dind-test docker:dind

    docker exec -it dind-test /bin/sh

    !!! you can do on linux and run DIND without elevated privileges !!!! (needs sysbox docker runtimer) / you can't do that on osx
    ??? wonder why sysbox runtime is not part of docker proper (probably because it doesn't work on all kernels - out of the box) 
    
    https://devopscube.com/run-docker-in-docker/
    https://github.com/nestybox/sysbox#installing-sysbox

? identifying docker containers by label / ensure that only one is running.

    - if the container has a name - docker doesn't allow you to run two instances with the same name at a time

    - now problem left: you can have the container hanging in some strange stage like 'Stopped' or 'Paused'
      The fix is to use labels.

      When starting the container add some unique label
            docker run ..... -l docker-php-admin

      Before starting the container: find by label, check if not running (exit if true) - else: stop it and prune it.

        STATE=$(docker ps -a --filter 'label=docker-php-admin'  --format='{{.State}}')
        if [[ $STATE == "running" ]]; then
            echo "server is already running"
            exit 1
        fi
        if [[ $STATE != "" ]]; then
            # force stop and clean up
            ID=$(docker ps -a --filter 'label=docker-php-admin' --format='{{.ID}}')
            if [[ $ID != "" ]]; then
                docker kill "$ID"
                docker container prune -f --filter 'label=docker-php-admin'
            fi
        fi



---

docker exec -it <container name|container id> /bin/sh   :: The classic: running a shell in a running container 

[Docker guides] See:  https://docs.docker.com/get-started/overview/) It's all hidden under: Running your app in production / Configure containers 


----

docker ps     ::: shows only running containers
docker ps -a  ::: shows running and stopped containers !!!

docker container ls ::: exactly the same as docker ps 

Docker commands can display it's stuff as json!!

    docker ps --format='{{json .}}'
        {"Command":"\"/docker-entrypoint.â€¦\"","CreatedAt":"2022-10-08 07:45:28 +0300 IDT","ID":"f99693d80146","Image":"docker/getting-started","Labels":"maintainer=NGINX Docker Maintainers \u003cdocker-maint@nginx.com\u003e","LocalVolumes":"0","Mounts":"","Names":"keen_darwin","Networks":"bridge","Ports":"0.0.0.0:80-\u003e80/tcp","RunningFor":"6 minutes ago","Size":"1.09kB (virtual 27.4MB)","State":"running","Status":"Up 6 minutes"}

-----

## moving a docker image to another machine

    docker save FULL_SHA -o tarfile.tar

    /COPY THE TARFILE.TAR to the other machine/

    docker load FULL_SHA -i tarfile.tar

    /Note that it often doesn't move the TAGS!!! so you need to tag the resulting image on your own/

    docker tag FULL_SHA tag_name


## shows properties of the docker engine

docker info         

## shows more stuff in json output (again, no idea what they all mean) 

docker info --format='{{json .}}' 

-----

### shows container logs (shows both stdout and stderr)

docker logs <container name or id>  

### shows logs with timestamp

docker logs  --timestamps <container name or id> 

### shows both stdout and stderr (they need to be sorted by timestamp, otherwise things get mixed up)

docker logs  --timestamps <container name or id>  2>&1 | sort -k 1  

### !!! Attention !!! docker logs writes to both STDOUT and STDERR. Error info is supposed to be written to stderr!!! ###

-----
## spills out details of the object. (all different format, depending on object type)

docker inspect <container-id>|<container-name>|<image-id>|<image-name>      


# You can extract several fields from the result!

docker inspect --format='{{json .Id}} {{json .Config.Cmd}}' docker/getting-started
    "sha256:157095baba98513dfef4ea00423767d8dae10edfeb629e9d39ea456e53f51e6a" ["nginx","-g","daemon off;"]

# strings in the template are shown as is (between the {{...}} query construct)

docker inspect --format='{{json .Id}} <=> {{json .Config.Cmd}}' docker/getting-started
"sha256:157095baba98513dfef4ea00423767d8dae10edfeb629e9d39ea456e53f51e6a" <=> ["nginx","-g","daemon off;"]


Listing all tags/info for an image:

    https://www.googlinux.com/how-to-list-all-tags-of-a-docker-image/

    curl 'https://registry.hub.docker.com/v2/repositories/library/debian/tags/'|jq . 


------

docker swarms  - they have their own cluster thing (i think docker tries to be some alternative to Kubernetes; that's more than docker-compose - which runs on a single node only)


--------
Get info on a specific image and tag - from dockerhub (including the os/platforms of all possible images)

    docker manifest inspect php:8.2-apache

------

PORTS

    - you can expose ports from a container -  that does nothing (it's a form of documenting which ports will be listened at within the container)
        - either by having EXPOSE in the Dockerfile (during build)
        - $(docker run --expose <port> )

    - to access a daemon running within the container: you need to map the port from outside the network to a port within the network  (docker run -p 8000:80)

    - you can do $(docker run -P .... ) - this will create a mapping between all 'exposed' ports to a temporarily assigned port number outside the network.

------

    You can connect from default docker to localhost -  host.docker.internal
        from docker  - that dns name resolves to the local host !!!
        > ping host.docker.internal

------

Trying to run a subprocess $(docker exec -ti <docker_id> /bin/bash) and passed the process pipes for the stdin/stdout/stderr streams.
Got error "the input device is not a TTY".
 
Now there i a lower level docker REST API! https://github.com/docker-php/docker-php 


https://docs.docker.com/engine/api/sdk/examples/ 
    For $(docker ps)
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json`
That means write raw HTTP to a unix domain socket, genius!


https://stackoverflow.com/questions/53781590/how-to-use-docker-api-engine-to-exec-cmd-in-containera

    - first create an "exec instance"
        POST /containers/{{id/name}}/exec
        {
            "AttachStdin": true,
            "AttachStdout": true,
            "AttachStderr": true,
            "DetachKeys": "ctrl-p,ctrl-q",
            "Tty": true,
            "Cmd": [
            "bin/bash","-c","touch appa.py"
            ],
            "Env": [
            "FOO=bar",
            "BAZ=quux"
            ],
            "Privileged":true,
            "User":"root"
        }


    = messages received after upgrade

        ```go
        header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}
        ```

        `STREAM_TYPE` can be:

        - 0: `stdin` (is written on `stdout`)
        - 1: `stdout`
        - 2: `stderr`

        `SIZE1, SIZE2, SIZE3, SIZE4` are the four bytes of the `uint32` size
        encoded as big endian.

        Following the header is the payload, which is the specified number of
        bytes of `STREAM_TYPE`.


    - then 

----
Mapping some docker commands to the docker engine rest api

Beware! The json output of curl vs docker commands is very different!

    docker ps --format='{{json .}}

    # doesn't show all containers (only running ones)
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json | jq . | less

    # show all containers
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json?all=true | jq . | less

    # show all containers and the size of containers field.
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json?size=true\&all=true


    ---

    docker image ls -a --format='{{json .}}'

    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/images/json?all=true\&digest=true | jq .

---

Docker layered file system

https://jessicagreben.medium.com/digging-into-docker-layers-c22f948ed612#:~:text=What%20are%20the%20layers%3F,during%20the%20Docker%20image%20build. 

========================

on OSX and linux - the same output for:

    docker info --format='{{json .Runtimes}}' | jq .
    {
      "io.containerd.runc.v2": {
        "path": "runc"
      },
      "io.containerd.runtime.v1.linux": {
        "path": "runc"
      },
      "runc": {
        "path": "runc"
      }
    }


====================



https://www.mongodb.com/docs/manual/reference/command/

Mongo (just like Elasticsearch) is a document oriented database. 

Mongo DB - holds a set of "collections"
    DB identified by name 
        - may not have following chars in it:  /, \, ., ", *, <, >, :, |, ?, $, space, \0, 
        - limited to 64 chars length
        - case sensitive

Mongo "collection" - similar to SQL table (but you don't have to define a schema in Mongo).
     - "collection" identified by its name ( "" is not a valid collection name, no $ chars or \0 chars in the name; may not start with prefix "system.")
     - can store very different json documents in a collection, but that's not a good idea!
     - Recommendation: store json docs of similar structure in the same collection (this allows you to do search)
        
    
A "document" in Mongo is an entry in a "collection" (it's a JSon file)- equivalent to a db "Row"; 
    - each "document" gets it's generated special _id field (12 bytes long - binary value)
    - id structure: 
            [4 bytes timestamp]
            [4 bytes - machine id]
            [2 bytes - pid]
            [3 bytes - incremented counter]
    - id field is always indexed, so you can lookup a document by it's id value!

Didn't know that mongo-db is used for search; so in what sense there is a equivalence with ElasticSearch ?

        - https://cloud.netapp.com/blog/cvo-blg-elasticsearch-vs-mongodb-6-key-differences

            - inserts are faster in mongo,  (mongo is written in C++, whereas elasticsearch is java)
                - better suited for document structured data (? don't you have a schema in elasticsearch too?)
            - text search is faster in elasticsearch

- mongo daemon (by default it listens on port 27017) 
  HTTP admin UI is listening port + 1000 (default: 28017)

- mongo shell: can enter expressions in json - that's very natural, as you can look at json documents as javacript maps/arrays. 
    - the document _id is represented as ObjectId class,.

    (or you can use alternative GUI based clients - like "Robo 3T")

    - shell commands
        var coll = db.getCollection("collectionName");  // lookup of collection object; or you can do as db.collectionName - but get an error if hte collection does not exist.
        
        - collection thing has all sorts of manipulation methods 
                - coll.insert({"person","Bob", "profession" : "Drummer"})  // insert doc, _id field is added upon insertion
                - var existingDoc = coll.findOne({"person" : "Bob"});    // find one doc, given a field
                - coll.update({"person":"Bob"}, { '$set' : { "profession":"Teacher" }}) ;  // first argument: query for doc like this, second argument: how to change the entry, once it was found. 
                - coll.find({}) // find all of them
    
        - update expressions can have all sorts of operators,
                $set - set the value of a field

                $inc, 

                $rename (rename field) 

                $push (adding to array)
                    coll.update({"person","Aob"}, {"$push", {"children" : { "Andrew" } } });

                $addToSet - like push, but prevents duplication of entries

                {$pop : { "key": 1}} - remove element from beginning of array "key", {$pop : { "key": -1}} - remove from the end 
                
                $pull - remove matching documents

                $each - push more than one entry (in an array)

                $slice - limit the size of an array to maximum n elements (or remove n - if n is negative)
                
                $sort - sort array by given key entry (provided that array has objects where all objects have the given key attribute)

            https://www.mongodb.com/docs/manual/reference/operator/update/
            https://www.mongodb.com/docs/manual/reference/operator/

        - find queries can have a lot of these (that one tries to be similar to sql SELECT...)  https://www.mongodb.com/docs/manual/reference/operator/query/    

    - Example session:

mongo 

> c=db.createCollection("tst")
{ "ok" : 1 }

> c=db.getCollection("tst")

> c.update({_id: 123}, { '$addToSet': {'a': 1, 'b': 2, 'c' :3} }, { 'upsert': 'true'})
WriteResult({ "nMatched" : 0, "nUpserted" : 1, "nModified" : 0, "_id" : 123 })

> c.find({_id: 123})
{ "_id" : 123, "a" : [ 1 ], "b" : [ 2 ], "c" : [ 3 ] }

> c.update({_id: 123}, { '$addToSet': {'d': 4, 'e': 5, 'e' :5} }, { 'upsert': 'true'})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })

> c.find({_id: 123})
{ "_id" : 123, "a" : [ 1 ], "b" : [ 2 ], "c" : [ 3 ], "d" : [ 4 ], "e" : [ 5 ] }

    - mongo operatator reference (very important link) https://www.mongodb.com/docs/manual/reference/operator/

    - now mongo is not the real shell (old and obsolete. use mongosh !!!!
        (installation instructions: https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/ )

    - in mongosh every map on a collection will return a Cursor object. You can convert it into a javascript array with toArray (big difference!!!)
        - but toArray() will take a lot of time - the cursor is evaluated just as it is needed, whereas toArray() will walk the whole query!!!

        - (i had to do it, when translating an index from one db into another one, they were using the same index field values, so I had to get them all...)

Reference on db.collection.find - query, projection, etc. etc.

    https://www.mongodb.com/docs/manual/reference/method/db.collection.find/

mongosh
    working with really large datasets

    - makes sense to save a result to file.
        fs.writeFileSync('file_big_array.json', JSON.stringify( variable_you_want_to_store ) )


    - reading the file back (in the old shell: mongo)
        a=cat('file_big_array.json')
        big_arr = eval(a)

    - in the new shell
        a=fs.readFileSync('file_big_array.json').toString()


Aggregation queries: (that's what they do for 'join')
    
    https://www.mongodb.com/docs/manual/core/aggregation-pipeline/#std-label-aggregation-pipeline


-----

Interesting detail: they often add a field that tells if an entry has been deleted or not.
That looks crazy - in SQL one would always delete a record, instead of adding a 'isactive' or 'isdeleted' field.


It seems that this is a kind of historical artifact: - from prior to 2019

https://stackoverflow.com/questions/6086245/is-remove-an-expensive-operation-in-mongodb

    
    this answer is a historical artifact from 2011 and it refers to the old MMAPv1 engine which was removed in 2019.

    MongoDB stores the data in a double linked list and so removing results is adjusting two links, the next link of the previous document and the previous link of the next document. There is no autocompacting. 
    Updating, if you have a value already stored, happens in place, changing one value. Now... you think, great, update one int instead of two pointers, surely faster! Not so -- you now need to index on this flag and creating indexes is "slow".


Culture, my ass...


-----

A function that shows all collections in all DB instances into a file - can run it from the mongo shell.

    function showAllCollectionsInAllDBS() {

        const fs=require("node:fs");

        let dbs = db.adminCommand(
           {
             listDatabases: 1
           }
        );


        let lstIt = ""

        for(let i=0; i < dbs.databases.length; ++i) {

            try {
                let dbname = dbs.databases[i].name;

                let dbnow = db.getSiblingDB(dbname);

                lstIt += "databaseName: " + dbname + "\n";

                let colNames = dbnow.getCollectionNames();

                for(let j=0; j< colNames.length;++j) {

                    lstIt += "\tcollection: " + colNames[j] + "\n";
                }
            } catch(er) {
                lstIt += "error: " + er;
            }
        }

        fs.writeFileSync("listdbs.txt", lstIt);

        console.log("see file llistdbs.tx for list of dbs an collections...");
    }

==========================

Reddis
    - does all sort of stuff (not just key value storage), 
        - they do work queues, and locks (with a specified time to life period). /what about reliability when usingg the work queue thing?/

        - https://github.com/resque/php-resque - that's a php library on top of redis lists (port of ruby library), Requests are queued as json objects. 
            Library has as a queue runner that listens on the list for jobs (maintained by github)



# port forwarding to service in k8s (inner expression finds pod name from some role name that pod adheres to 
# pod in cluster has redis running on port 6379 - local port is 6378 (other commands will use that number)

kubectl port-forward "$(kubectl get pods -l role=ROLE_NAME_OF_POD_IN_CLUSTER -o jsonpath="{.items[0].metadata.name}")" 6378:6379

# find a specific key

echo 'keys *' | redis-cli -p 6378 | grep  blabla
BLABLA-KEY-NAME

# check the type (this one is a list)

echo 'type BLABLA-KEY-NAME' | redis-cli -p 6378
list

# show how many elements are in list

echo 'LLEN BLABLA-KEY-NAME' | redis-cli -p 6378

# show the elements of the list between index 0 and 1 (not including 1)
echo 'LRANGE BLABLA-KEY-NAME 0 1' | redis-cli -p 6378


    
===========================

https://en.wikipedia.org/wiki/WebSocket

    - bi-directional communication
         https://www.rfc-editor.org/rfc/rfc6455 - the doc

    - initially over http, then client sends header: 'Connection: Upgrade' 'Upgrade: websocket' + additional headers, in order to change from http to websocket protocol.

        Request:

            GET /chat HTTP/1.1
            Host: server.example.com
            Upgrade: websocket
            Connection: Upgrade
            Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
            Origin: http://example.com
            Sec-WebSocket-Protocol: chat, superchat
            Sec-WebSocket-Version: 13

        Response:  Sec-WebSocket-Accept value is some sha signature over the request value Sec-WebSocket-Key + <some uuid>
    
            HTTP/1.1 101 Switching Protocols
            Upgrade: websocket
            Connection: Upgrade
            Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=

 
        Further requests in WebSocket are in frames

        - php Ratchet: 

            // bad news: you can't just upgrade a http connection of you php web request handler. You need to make a standalone script that listens to incoming WebSocket request.

            // simple chat server (that's how they do event driven stuff)

            $server =  Ratchet\Server\IoServer::factory(     // creates listening socke0wwt and event loop wrapper (IoServer), 
                                                             // the connect events are handled and create IoConnection)
                new Ratchet\Http\HttpServer(                 // parses http messages             
                    new Ratchet\WebSocket\WsServer(          // parsing of websocket handshake and frame messages 
                        new Chat()                           // application level class that handles messages
                    )
                ),
                8080
             );
             $server->run(); // run event loop.

            // server side action goes here
            class WsServer implements HttpServerInterface {

                
                // ComponentInterface - the application that consumes messages
                public function __construct(ComponentInterface $component) {

                public function onOpen(ConnectionInterface $conn, RequestInterface $request = null) {

                    // handle handshake
                    \Ratchet\RFC6455\Handshake\ServerNegotiator - handshake method ::: does the handshake.


        
        - Ratchet: working with sockets:
            
            Ratchet\Server\IoConnection - asynchronous socket handler
                - constructor receives derived class of \\React\\Socket\\ConnectionInterface

                - \React\Socket\Connection // wraps a stream/socket so that it does the ConnectionInterface!
                        public function __construct($resource, LoopInterface $loop)

        - every server passed to a ratched event loop handles:

            interface MessageInterface {

                function onMessage(ConnectionInterface $clientConnection, $msg);

----

    List of libraries for working with the docker engine api  : https://docs.docker.com/engine/api/sdk/

----

https://realpython.com/pytest-python-testing/ (and others)

    - standard library comes with unittest framework ( https://docs.python.org/3/library/unittest.html ) - very much like JUnit (with setUp and tearDown methods)

        - but they have pytest : 
        - here you don't have to import/derive from base class and call it's assertion methods; most simple test is a function with test_ prefix and calls assert
        - pytest is an exe, it's the test runner - it gathers files with test_ prefix and calls all the test.

    - so how do I set up the test?
        - test function has parameter, the parameter is a function that is declared with @pytest.fixture decorator ; FIXTURE MEANS it produces a value used for the test.

        import pytest

        @pytest.fixture
        def example_fixture():
            return 1

        def test_with_fixture(example_fixture):
            assert example_fixture == 1

    - now functions with the @pytest.fixture annotation can also have parameters, these are most probably nested fixtures, so they call themselves in turn upon the setup of the value that is returned by the fixture.

    - great, now how do I tear down the test? (here things get complicated - you tear down the fixture!) generators!

      One way of tearing the fixture down.

        import pytest

        @pytest.fixture()
        def resource():
            print("setup")
            yield "resource"
            print("teardown")

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

        
    Another way: register a callback!

        @pytest.fixture()
        def resource(request):
            print("setup")

            def teardown():
                print("teardown")
            request.addfinalizer(teardown)
            
            return "resource"

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

    - lifetime of a fixture return value.

        When you use a fixture in several tests, then the returned value may be cached (similar to unittest - you don't want to have a setUp per test!)
        Now the value returned by the fixture has a lifetime, depending on the scope parameter of the fixture annotation!!!

        
        # scope="function" - if you use the resource parameter as a fixture, then the nested teardown method will be called FOR THE LAST TEST OF THIS SOURCE FILE!
        # scope="session"  - teardown called after the end of the run of the pytest test runner !
        # scope="module"   - teardown called after the last test in the current module
        # scope="package"  - teardown called after the last test in the current package.

        !!! BECAUSE OF ALL THE CACHING: YOU CAN'T INVOKE A FIXTURE TWICE FOR A TEST !!!
       
        @pytest.fixture(scope="function")
        def resource(request):
            print("setup")

            def teardown():
                print("teardown")
            request.addfinalizer(teardown)
            
            return "resource"

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

    - can parametrize tests. Each entry in the array will be passed in a separate test run of test_in_palindrome
      YOU ALSO PASS PAREMTERS TO FIXTURES THIS WAY !!!

        @pytest.mark.parametrize("palindrome", [
            "",
            "a",
            "Bob",
            "Never odd or even",
            "Do geese see God?",
        ])
        def test_is_palindrome(palindrome):
            assert is_palindrome(palindrome)
                
    - multiple parametrize annotations: will result in all combinations (cartesian product) of all test values!
        
            @pytest.mark.parametrize('foo', ['a', 'b', 'c'])
            @pytest.mark.parametrize('bar', [1, 2, 3])
            def test_things(foo, bar):
                assert foo in ['a', 'b', 'c']
                assert bar in [1, 2, 3]

      test report now looks as:

            test_foo.py::test_things[1-a] PASSED
            test_foo.py::test_things[1-b] PASSED
            test_foo.py::test_things[1-c] PASSED
            test_foo.py::test_things[2-a] PASSED
            test_foo.py::test_things[2-b] PASSED
            test_foo.py::test_things[2-c] PASSED
            test_foo.py::test_things[3-a] PASSED
            test_foo.py::test_things[3-b] PASSED
            test_foo.py::test_things[3-c] PASSED


-----
BNF grammars 

    Python got it's syntax in the reference
        https://docs.python.org/3/reference/grammar.html

    BNF grammars for javascript?

    You got a lot of them in ANTLR!
        - https://github.com/antlr/grammars-v4/tree/master/javascript/javascript

    C syntax BNF
        - https://cs.wmich.edu/~gupta/teaching/cs4850/sumII06/The%20syntax%20of%20C%20in%20Backus-Naur%20form.htm

    Pascal syntax BNF
        - https://condor.depaul.edu/ichu/csc447/notes/wk2/pascal.html

===

Javascript - iterating over map entries (node session)

    > a={a:1,b:2,c:3}
    { a: 1, b: 2, c: 3 }
    > Object.entries(a).map(([k,v],i) => { console.log("k: " + k + " v: " +v + " i :" +i); return k } )
    k: a v: 1 i :0
    k: b v: 2 i :1
    k: c v: 3 i :2
    [ 'a', 'b', 'c' ]

Javascript generators/iterators - they are needed to provide values for a 'for' loop.

Here is how you do an iterable object (that's an object that has special function to return an iterator, so that it can be used in a for loop)

    class Foo {
         constructor() {
             this.data = [1,2,3];
         }
    }

    //
    // class Foo is 'iterable' if it has a member named Symbol.iterator - that returns an iterator object.
    //
    // need to add the function named as special symbol: Symbol.iterator to protoype
    // can't just define a member function with that name ???!
    // now a new Foo object will bet 'iterable' as it has an iterator.
    //
    Foo.prototype[Symbol.iterator] = function() {
         let pos = 0;
         let data = this.data;

         // iterator is an object with the next function, that returns an object upon each call
         // returned object/dict has done: - are we done iterating? and value: - value to return to for loop
         return {
             next: function() {
                     return {
                         done: !(pos in data), // are we done with the iteration?
                         value: data[pos++]    // value returned by iterator
                     }
             }
         }
     }


    f = new Foo();
    for(let c of f) {
        console.log(c);
    }

Here is how you do a generator

    function *range(from,to,step) {
        let val = from;
        while(val < to) {
            yield val;
            val += step;
        }
    }

    for(let i of range(1,10,2)) {
        console.log(i);
    }

Calling generators directly

    > a=range(1,3,1)
    Object [Generator] {}
    > a.next()
    { value: 1, done: false }
    > a.next()
    { value: 2, done: false }
    > a.next()
    { value: undefined, done: true }
    > a.next()
    { value: undefined, done: true }


Generators can call other generators - but they have to do the call via yield* 

    function *blabla() {
        return 3;
    }

    function* func1() {
      yield 42;
    }

    function regFunc() {
        return 43;
    }

    function* func2() {

        // can call regular function from generator - works as expected
        let bl = regFunc();
        console.log(bl);

        // calling a generator function returns a generator object
        let er = blabla();
        console.log("blabla returns: " + er);

        // this way it returns 3
        er = yield *blabla();
        console.log("blabla returns: " + er);

        yield* func1();
        yield* func1();
        yield* func1();
    }

    for(a of func2()) {
        console.log(a);
    }


It turns out that python has a similar thing to delegate form one generator to the other. Welcome to 'yield from' - beginning with python 3.8

    def my_range(f,t,st):
        while f < t:
            yield f
            f += st

    # yield from - to delegate to another generator !!! (from python 3.8)
    def range_three(f,t):
        yield from my_range(f,t,3)

    for n in my_range(1,10,1):
        print(n)

    print("generator calling generator")

    for n in range_three(1,10):
        print(n)


===

    javascript express for http server:

        const express = require("express"),
              app = express()

        app.get("/", (req, res) => {
            res.send("Hello, TREND OCEANS!")
            console.log("finished handling request");
        })

        console.log("before listen");
        app.listen(3000, console.log(`Server started on port 3000`))
        console.log("after listen");

    output:

        node t1.js
        before listen
        Server started on port 3000
        after listen
        finished handling request

=== 

    javascript sleep:

        function onTm() {
            console.log("onTm");
        }

        console.log("before sleep");
        setTimeout(onTm,1000);
        console.log("after sleep");

    output:
        node t.js
        before sleep
        after sleep
        onTm

====
http in node:
    node has in-built http module
        

    express - server
        http://expressjs.com/en/5x/api.html


====

NodeJS - event loop

    https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/

    Phases of the event loop
        - timers: (timers are best effort - not exactly scheduled)
 
----

Binary data in nodejs
(The javascript standard later added it's own stuff:  ArrayBuffer, Uint8array, Float64array etc.. but they don't use that in node)

> let b= new Buffer.alloc(10); // allocates zero allocated buffer
undefined

> typeof(a)
'object'
> a.constructor.name
'Buffer'

> b
<Buffer 00 00 00 00 00 00 00 00 00 00>

> b[2]
0

> b[2]=1
1

> b[2]
1

> b
<Buffer 00 00 01 00 00 00 00 00 00 00>

    
> b.length
10
-----

npm - creating package

    - search if package exists in: https://www.npmjs.com/search?q=prs
    - npm init new-package-name  # makes a default package.json ( package.json reference: https://docs.npmjs.com/cli/v9/configuring-npm/package-json )

    - npm adduser  # use this to log into your npm account. before publishing: need to be logged into your account. 

    - set NPM_TOKEN environment variable to an npm account API token with publish privs.

    - npm publish --access public # from directory that has package.json - and all of the files mentioned in package.json !!!

    - each publish needs to increment the package version number (in package.json)

Show root folder where globally installed npm modules are:

    npm root -g 

install YAML package into the global dir
    npm i yaml -g

    To use globally installed package:

    To use locally installed package:
        need to set environment variable: 
        export NODE_PATH=$(npm root -g)

install YAML locally
    npm i yaml

    module is installed to $PWD/node_modules
    
    To use locally installed package:
        need to set environment variable: 
        export NODE_PATH=$PWD

If you are part of an installed package, then you don't need to do this!!! (can include anything in the current scope)


List all globally installed packages

    npm list -g 


--- 

Viewing ssh logs:

With systemd

    journalctl -fu ssh

otherwise 

    sudo grep sshd /var/log/auth.log

In /etc/ssh/sshd_config put

    LogLevel VERBOSE


======

Formatting text in whatsapp messages

https://faq.whatsapp.com/539178204879377

    ```...```` - makes it fixed font.

======

Python virtual environments

    # create the virtual env
    python -m venv myvenv

    # activate the virtual env
    source myvenv/bin/activate
    #(prompt changes after that)

    # if there is a requirements.txt file:
    pip install -r requirements.txt

    # create requirements (after having installed some pip packages)
    pip freeze > requirements.txt

    Benefit: pip install puts up everything in /site/packages - shared thing, can get clogged up. This way every environment is kind of isolated.

    # deactivate virtual environment myvenv
    deactivate


Pip show installed package version of numpy
     pip show numpy


Show all packages
     pip list

Show all packages as a tree (which one requires which one)

   # need to install the thing
   pip install pipdeptree

   pipdeptree


   # extremely usefull thing - i wonder why they don't have something like that as part of pip, without having to install some package, that is.


Force upgrade a package to latest?

    pip install --upgrade --force-reinstall cryptography

=========

Workign with binary data.

(Perl and Python are similar to this respect - they pack/unpack the fields - according to some printf-like text spec (the spec is called 'the template')
Nodejs/Javascript doesn't do that - they have java like readUint8, writeUint8 functions for binary de-serialisation.


https://perldoc.perl.org/perlpacktut


==

GITHUB

Want to submit a pull request to some repository other than your own stuff?

    https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork

    - need to fork that repository first.


--------

AWS lambdas

    - tutorial: https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html



----

Integration testing approaches :  testing interaction of groups of software models / components.

    https://en.wikipedia.org/wiki/Integration_testing

    Big bang - create a complete system (consisting of all components) ; run test against this configuration

        Problem: 
            the complete system hardly works during development (lot's of downtime for the developers)

            https://www.tutorialspoint.com/software_testing_dictionary/big_bang_testing.htm

            - Defects present at the interfaces of components are identified at very late stage as all components are integrated in one shot.
            - It is very difficult to isolate the defects found.
            - There is high probability of missing some critical defects, which might pop up in the production environment
            - It is very difficult to cover all the cases for integration testing without missing even a single scenario.

    Bottom up - test lower level modules in isolation, then combine them with next upper level - until you test the whole system

        needs test drivers : to call the lower level models (in the absence of a complete system)

        But it is easier to test the system!  https://testsigma.com/blog/difference-between-top-down-and-bottom-up-integration-testing/

            - easier to find problems when limited amount of components are interacting
            - easier to come up with a more complete test scenario
            - less downtime than with big bang system

        Problems
            - you need to develop an army of these test drivers
            - more time to run the test?


    Top - down - reverse direction.

        first test the higher level models, note that lower level models are substituted by test stubs (that simulate the working of the lower level models)

        (says it's more suitable for procedural systems, less so for Object Oriented systems)
        
            
    Sandwich level - combine bottom-up and top-down

        (interesting, which companies use this one? Sounds like a lot of work)

----

Dynamic programming - can be used to problems that have 'optimal substructure' and 'Overlapping subproblems'


    optimal substructure - optimal solution to the problem can be constructed from the optimal solutions of its subproblems. 

    Overlapping subproblems refer - a problem can be broken down into smaller subproblems, and the solutions to these subproblems are used repeatedly to solve the original problem


Solution can be bottom up - table driven (to store the repeated solutions) 
 or top-down - recursive (but need memoization - a hash table to store results of previously solved common subproblems)

- doing the recursive solution with memoization is easier, but the table driven solution is more efficient during runtime.

Doing some examples as test cases here:  https://github.com/MoserMichael/jscriptparse/tree/main/leetcode

Also lots of stuff here: https://en.wikipedia.org/wiki/Category:Dynamic_programming

=====

Asking chatgpt on how it works:

- says the Turing test is regarded as 'too subjective', as it is built on tricking an observer into believing that you talk with a human.
  Instead they have the 'Winograd Schema Challenge' - that's a multi-choice test that asks questions on complicated sentences. https://en.wikipedia.org/wiki/Winograd_schema_challenge
- says as of 19/3/2023 it is built on top of GPT-3 (with 'knowledge cutoff' date of 2021), is being frequently/continuously updated.
  'larger updates' requires months or years.
- says it has access to news sources that re not part of it's language model (like stock news, weather, web articles, current date - from a system clock)  
  some of this info is available via API's. 'if a user asks me about the weather in a certain location, I can use an API to retrieve the current weather data for that location '
- q: 'How can you refer to previous items in this conversation if all that you do is to predict the next word?'
  'One technique I use is called sequence modeling, which allows me to model the entire sequence of words and phrases in a conversation and use that information to generate responses that are coherent and relevant to the current topic. Another technique is called attention-based modeling, which allows me to focus on specific parts of the conversation that are most relevant to the current turn.

In addition, I can use machine learning algorithms to learn from previous turns in the conversation and adapt my responses based on the overall context and history of the dialogue. This allows me to refer back to previous items in the conversation and generate responses that are consistent and relevant to the ongoing dialogue'
- q'tell me more about sequence modelling'
 The basic idea behind sequence modeling is to treat a conversation or any sequence of text as a series of tokens, which can be words, phrases, or even characters.

One common type of sequence modeling technique is called Recurrent Neural Networks (RNNs). RNNs are a type of neural network that can handle sequential data by processing the tokens one at a time and maintaining a hidden state that summarizes the information seen so far. This hidden state is updated at each step of the sequence, and can be used to generate predictions for the next token.

Another more advanced type of sequence modeling technique is called Transformer Networks. Transformers are also a type of neural network that can handle sequential data, but they use a self-attention mechanism to directly model the relationships between all tokens in the sequence, rather than relying on a hidden state like RNNs. This makes them more effective for modeling longer sequences of text.

-are you using a single model, or are you combining the output of several different modelling approaches?

-   based on a single large neural network architecture that is trained on a large corpus of text data. Specifically, my architecture is based on the GPT (Generative Pre-trained Transformer) series developed by OpenAI, which is a type of Transformer-based language model.

However, while I am based on a single neural network architecture, I use a combination of different techniques and approaches to generate responses to user queries. For example, in addition to sequence modeling techniques like RNNs and Transformers that I mentioned earlier, I also use techniques like language modeling, text classification, and sentiment analysis to better understand the content and context of the user's input

- can you tell me more about transformers? How do you disambiguate the meaning of words with several meanings?

 Transformers use a self-attention mechanism to directly model the relationships between all tokens in a sequence of text, rather than relying on a hidden state like RNNs. This makes them more effective for modeling longer sequences of text and capturing complex relationships between different parts of a text.

To disambiguate the meaning of words with several meanings, transformers use a technique called contextual word embeddings. Contextual word embeddings refer to the process of representing words in a way that takes into account the context in which they appear.


In a transformer-based language model, the model is trained on a large corpus of text data, and during training, the model learns to represent words in a high-dimensional vector space. These vectors capture the meaning of the words in the context of the sentences and documents they appear in. For example, the vector representation of the word "bat" would be different in the context of a discussion about baseball compared to a discussion about animals.

When processing new text input, the transformer-based language model can use these contextual word embeddings to disambiguate the meaning of words with multiple meanings. By taking into account the context in which a word appears, the model can determine which meaning of the word is most likely to be correct in that specific context.

In addition, transformers can also use other techniques like part-of-speech tagging, named entity recognition, and dependency parsing to further disambiguate the meaning of words and understand the structure and relationships between different parts of a text

- How are transformers using dependency parsing?

In a transformer-based language model, dependency parsing can be used to generate a dependency tree that represents the grammatical structure of a sentence. The transformer can use this tree to understand the relationships between different words and how they relate to each other in the sentence.

Dependency parsing can be performed in two main ways in a transformer-based language model:

As a preprocessing step: The sentence is first passed through a separate dependency parser that generates a dependency tree for the sentence. This dependency tree is then used as an additional input to the transformer-based language model. The transformer can then use this information to better understand the relationships between different words in the sentence.

As part of the model architecture: The transformer can be modified to incorporate a dependency parsing component as part of its architecture. This allows the model to generate the dependency tree for a sentence directly as part of the forward pass through the model. This approach can be more efficient and can allow the model to learn to perform dependency parsing and language modeling simultaneously.

Using dependency parsing in a transformer-based language model can help the model better understand the structure and relationships between different parts of a sentence. This can be particularly useful for disambiguating the meaning of words with multiple meanings and for understanding the overall meaning and intent of a sentence.

- A dependency parser can result in multiple parse structures for the same sentence. What do you do if you get multiple parse structures?

 a transformer-based language model can use a variety of techniques to handle the ambiguity and choose the most appropriate parse structure. Here are a few possible approaches:

Probability-based selection: One approach is to assign probabilities to each parse structure based on how well it fits with the context of the sentence and the overall distribution of parse structures in the training data. The transformer can then choose the most probable parse structure as the correct one.

Rule-based selection: Another approach is to use a set of hand-crafted rules to select the most appropriate parse structure. For example, the rules could favor parse structures that are more common in the training data or that better match the syntactic or semantic context of the sentence.

Ensemble-based selection: A third approach is to use an ensemble of multiple dependency parsers that generate different parse structures for the same sentence. The transformer can then choose the parse structure that is most commonly agreed upon by the ensemble, or that best fits with the overall distribution of parse structures in the training data.

Active learning: Finally, the model can be trained to actively seek additional information to resolve ambiguity. For example, it could ask follow-up questions or request additional input from the user to clarify the intended meaning of the sentence.

Overall, handling ambiguity in dependency parsing is an active area of research, and there are many different approaches that can be used to address this challenge in a transformer-based language model.

- how is the transformer model processing the output of the dependency parsing stage?

After the dependency parsing stage, the output is typically represented as a graph or a tree structure where the words of the sentence are represented as nodes, and the dependencies between the words are represented as directed edges. The transformer model can then take this graph or tree structure as input and process it using various techniques.

One common approach is to use graph neural networks (GNNs) to encode the graph or tree structure. GNNs are a class of neural networks that operate directly on graphs and have been shown to be effective in various natural language processing tasks, including dependency parsing. Specifically, GNNs can be used to encode the graph or tree structure of the sentence by propagating information from neighboring nodes along the edges of the graph.

Another approach is to use attention mechanisms to weight the importance of each word or dependency relation in the sentence. Attention mechanisms have been a central component of transformer models and have been shown to be effective in various NLP tasks. Specifically, attention mechanisms can be used to attend to the most relevant parts of the sentence based on the dependency relations between the words.

After processing the output of the dependency parsing stage, the transformer model can use this information to make predictions or generate output in various NLP tasks, such as question answering, text summarization, or machine translation.

- is the GNN performing a classification task or a prediction task in this case?

There was an error generating a response ...


- how does the transformer model determine the context of a conversation?

uring this training process, the transformer model learns to recognize patterns and dependencies in the text, which allows it to understand the relationships between words and the context in which they appear. For example, it learns that the word "dog" is more likely to appear in the context of a sentence like "The boy played with his dog" than in a sentence like "The girl ate a sandwich."

In the context of a conversation, the transformer model uses the previous messages in the conversation as context to understand the meaning of the current message. It does this by encoding each message into a vector representation, which it uses to inform its understanding of the current message. The transformer model also has the ability to pay attention to specific parts of the context, which allows it to understand the nuances of the conversation and make more accurate predictions.

- can you distinguish between questions about artificial intelligence and questions about the stock market?

If a question includes terms or phrases that are commonly associated with artificial intelligence, such as "neural networks," "deep learning," or "machine learning," it is likely to be about AI. On the other hand, if a question includes terms or phrases commonly associated with the stock market, such as "stocks," "dividends," or "market capitalization," it is likely to be about the stock market.

However, there may be cases where a question includes terms or phrases that could be associated with both AI and the stock market, such as "algorithmic trading" or "data analysis." In these cases, further clarification or context may be needed to determine the intended topic.

- each context has a different multidimensional vector built by machine learning to disambiguate terms, isn't it ? how many different conceptual contexts are you handling?

Some common conceptual contexts that I am trained on include language and grammar, general knowledge, history, science, technology, social issues, culture, and more. However, my ability to handle specific conceptual contexts depends on the availability and quality of training data related to that context.

- are these different contexts defined by the model or are they defined by the developers of the system?

During the training process, the developers can choose to focus on specific topics or domains by selecting appropriate training data or by fine-tuning the model on specific datasets. For example, a developer might choose to train a language model specifically on medical texts in order to create a system that is particularly adept at understanding medical terminology and concepts.

- how would you sum up this conversation?

In this conversation, we discussed several topics related to artificial intelligence and natural language processing. We talked about how the transformer model determines the context of a conversation, and how different contexts are represented by multidimensional vectors learned through machine learning. We also touched on the different conceptual contexts that an AI language model can handle, and how these contexts are defined by the developers of the system. Finally, we talked about the storage of user profiles in AI systems and the importance of data privacy and security regulations.

- are you learning from the summaries of your conversation?

As an AI language model, I am constantly learning and improving based on the conversations I have with users. This includes learning from the summaries of our conversations, as well as from the original inputs and outputs.

During training, my neural network is adjusted based on patterns and relationships that it identifies in large datasets of text, which includes the text generated by users during interactions with the system. This means that the more conversations I have and the more diverse the input data is, the better I become at understanding and generating human-like responses.

- Do you also employ traditional AI approaches other than machine learning?

- As an AI language model based on the GPT architecture, I primarily rely on deep learning techniques, specifically neural networks and machine learning, to understand and generate natural language. However, some traditional AI approaches may also be used in combination with these techniques to enhance my performance or enable certain functionalities.

For example, rule-based systems can be used to supplement machine learning by providing explicit, hand-coded rules for specific tasks or domains. These rules can help improve the accuracy and efficiency of certain tasks by encoding prior knowledge or constraints into the system.

Another example is the use of knowledge graphs or ontologies, which organize knowledge into a structured representation that can be used to support reasoning and inference. These structured representations can be used to enhance my understanding of complex relationships and concepts, as well as to facilitate information retrieval and question answering tasks.

That being said, the core of my functionality and performance is based on neural networks and machine learning, which enable me to learn and adapt to new data and contexts over time.

===============

CHATGTP on it's api parameters:

    - The "temperature" field in a ChatGPT API request is a parameter that determines the level of randomness or creativity in the generated response.

    Temperature is a measure of the entropy of the system, and in the context of natural language processing, it determines the level of variation in the model's responses. A higher temperature value will result in more varied and unpredictable responses, while a lower temperature value will produce more conservative and predictable responses.

    In the context of a ChatGPT API request, the temperature value can be set to a value between 0 and 1, with 0 being the most conservative and 1 being the most creative. A temperature value of around 0.5 is often used as a default, as it strikes a balance between creativity and coherence in the generated responses.

    Note that the actual temperature value used in generating the response can be slightly different from the value specified in the API request, as the model may adjust the temperature value based on the context and other factors.


    - The maxTokens parameter is important because it allows you to control the length and specificity of the model's output. If you set maxTokens to a low value, the generated text will be relatively short and focused on the most salient aspects of the prompt. If you set maxTokens to a high value, the generated text will be longer and more detailed, potentially covering a wider range of topics and ideas.

    However, setting maxTokens too high can also lead to the model generating text that is unfocused or irrelevant to the prompt. Therefore, it is important to experiment with different values of maxTokens to find the optimal balance between length and relevance for your specific application.

    It is worth noting that different engines may have different maximum values for maxTokens


    >Is there an equation that returns the number of api tokens consumed by the request, depending on the max_token input parameter of the api?

    Yes, there is an equation that can be used to estimate the number of tokens consumed by an API request to ChatGPT, based on the max_tokens parameter that you set in your request.

    The equation is:

        tokens = max(max_tokens, 4) + ceil(log2(num_samples))

        num_samples is the number of samples or responses that you request from the API

        It's worth noting that this equation is an estimate, and the actual number of tokens consumed by a request may vary depending on factors such as the length and complexity of the generated text, as well as the specific parameters and settings used in the request.


-----
IntelliJ

    Command+L  - goto line/column 
        12:5     - move to line 12 column 5
        12         move to line 12 column 1

    Option+Fn+F7  - or from context menu 'Find Usage' - find where symbol under cursor is used in this project


Visual Studio Code

    
    Run commands in terminal
        Command + Shift + P ; type 'Shell Command install 'code' command in path'

        View / Terminal

    Fn+F12  : Goto definition of symbol. (very valuable thing!!!)

    Ctrl+G  : Goto line

-- learning how to do somthing in minecraft: (also looking at mineflayer - nodejs module for bots)

(i am an idiot should have looked it up years ago: 'Parent's guide to minecraft' https://www.youtube.com/watch?v=t9eJ6TLtfd8 ... )

Modes: can switch modes
    Creative mode: the monsters can't hurt you
    Survival mode: start with nothing, have to collect and build stuff. Monsters can hurt you. Have earn stuff.
    Adventure mode: can't place/destroy blocks (need tools). 


How to use it
Navigation (just like in vim ;-)
    a d  : left,right
    w s  : up down
    swipe mouse : turn around

    space space  : (in creative mode) - start flying
    shift : gets you down again

Building
    e : bring up the 'inventory window' - the stuff that you can build with.
        
        chose type of block and put them into the 'inventory' (you can build with it)
        
        press number to switch between the selected slot in the inventory (or mouse wheel)


Messages 
    / - bring up the chat window

    # now the bot will get a 'whisper' event. (the bot will get it!)
    /msg BotyBot fly
        
    # public chat on minecraft with say (they don't seem to have that in creative mode without microsoft authentication)
    # this one results in a event for the following
    # bot.on('chat', (username, text) => { } )

    /say BotyBot fly


Coordinates in minecraft
    https://minecraft.fandom.com/wiki/Coordinates#:~:text=The%20y%2Daxis%20indicates%20how,block%20equals%201%20cubic%20meter

    (x,y,z) 
        x,z - coorinates on the plane
            x-axis indicates the player's distance east (positive) or west (negative) - longitude
            z-axis indicates the player's distance south (positive) or north (negative) of the origin pointâ€”i.e., the latitude,

        y   - the elevation from surface (

In game:
    F3 (mac fn-F3) : dumps all sort of stuff on the screen (including players coordinates)

===========
Interview with Ilya Sutskever on GTP-4  https://en.wikipedia.org/wiki/Ilya_Sutskever

https://www.youtube.com/watch?v=SjhIlw3Iffs

- I.S: on imagenet: says he thought that a big and deep neural net + lots of data (with the desired solutions) can do complicated stuff.
  Why? The Brain can do it, and it works like that.
  
- 8:07 Intuition behind transformers, self supervised learning?
    Even with imagenet the idea is: predicting the next small step is all you need.
    Prediction is compression - the idea is that making the next small step (of predicting the next word) is all you need in order to solve self supervised learning!
    Transformer paper: says they address the problems of recurrent neural networks.
    Then they made the stuff bigger - and it worked (somehow).

- Q: Richard S. Sutton says : you just need to scale neural nets, did he have an influence on gtp?
- I.S: Says no, says it's not just scale, you need to scale some specific feature that can benefit from scaling.
    Deep learning: says it is one of the first thing that would benefit from large scale computations - throwing more data at neural nets is a computation that benefits from scaling!
    
- Q: limitation of language models: all knowledge comes from the text corpus of the training data. Says a lot of knowledge is non linguistic.
- I.S: Objective of language model: optimize the answers for the target metric. 
  Says language knowledge doesn't have an understanding of the things that language relates to:
  Example: ChatGTP invents lots of stuff when talking about the interviewer. 
  How to address the problem?

  Answer: hard to talk about limitations of language model, because the subject (language models) changes quickly.

  15:04
  Q: Do they just learn about statistical regularities, without knowing about the nature of the world?
  I.S: Says doing statistical regularities is a big thing by itself: says that predicting the next step can be interpreted as ... compression. Both are statistical phenomena.
  Also you somehow need to understand the underlying data in order to predict the next item (same for compression).
  /also you need to understand more about the data/

  Says understanding and good performance at prediction goes together. Claims: good performant models will have a shocking degree of understanding of the world - as seen through the lense of text.

  16:53
  Example: Sydney (that's the language model in bing chat), Sydney became aggressive when given a prompt that says that google is the better search engine. WTF?
  says that you can think about that as "psychology of neural networks".

  Limitations: neural networks invent things/hallucinate - because language model is good in learning about the world, but the output isn't great (this has technical reasons)
  (in pre-training)

  18:24
  chatgtp has an additional reinforcement learning process - here it learns: this output is not appropriate, so don't do that again (pre-training doesn't have these constraints, so he says it's great at building internal models about the world)

  19:30  
  https://www.youtube.com/watch?v=SjhIlw3Iffs&t=1068s

  Says hallucinations are a big problem that 'limits the usefulness of chatgtp', says that can be fixed by strengthening the reinforcement learning step. 
  Will it work? let's find out!  

  20:36
  Right now they hire people to teach ChatGTP to behave (to fine-tune the reinforcement learning step). But the ideal would be for the system to incorporate end user feedback into the reinforcement learning step (?)

- Q: ... language model lacks non linguistic knowledge, world model - is that a significant problem?
- I.S:
  
  Says no significant claims.
    - systems should have multi-modal understanding (not just text). Says incorporating multi-model stuff is desirable, but says it's not essential:
        why? You can learn the same things effectively from text, even if the learning process is slower.
        24:00.
        Example: color: says you can learn about color from text only - it just takes more time than learning it by looking at it.
            why? every concept in the neural network is represented by some high dimensional vector (embeddings)
            You can similarities between these high dimensional vectors/embeddings - and can possibly derive facts like 'purple is more similar to the color blue than to color red'
            claims it is possible to learn this fact from the text alone - it just takes more time & effort.
        
   
28:00

- Q: you have an army of people who fine-tune/guide the reinforcement learning step, is there a better way? Ian Kuhn says there are algorithms for that.
- I.S: says pretrained model step already knows about reality - knowledge of language and the 'processes that exist in the world and produce the language'
     says it is a 'compressed representation of the real world processes that produce the data' - and a compressed model of ... everything!
     The 'army of teachers' also are using AI assistants, but the results need to be reviewed by humans!

  32:00
  - so the objective is to make the output more accurate?
    Yes, it's similar to an education process - to tell the model that hallucinations are not ok.

- Q: here do you go now?
- I.S:  make the model more reliable, less hallucinations. 

34:19
- Q: Jeff Hinton says: large language models 'hold tremendous amount of data with a modest amount of parameters' - whereas the Human brain has 'trillions of parameters' (neurons?) and modest amounts of data'. 
- I.S: Says there is a problem: models should be able to learn from less data during data - during the initial stage. In later learning stages the language model needs less data.
  Says we will get there - in terms of learning more from less data.
  Says that will allow to teach us new skills to the language models and how to 'behave better'

36:22
- didn't understand the question.

38:09
- Q: impact of AI on democracy; can it come up with solutions to satisfy everybody - will it help humans to 'manage society?'
- I.S:  Says they want to come up with 'solutions to problems of this kind' - but doesn't know how they will be used by governments.
  Maybe there will be a democratic process where people will provide 'input to the model that will be aggregated' ????

    (me: this song comes to mind.... 'The Doors' https://www.youtube.com/watch?v=4YWPhmduSDQ  

        When I was back there in Seminary School
        There was a person there
        Who put forth the proposition
        That you can petition the Lord with prayer
        Petition the Lord with prayer
        Petition the Lord with prayer
        You cannot petition the Lord with prayer!
   )      

Bla-bla - (the music starts here)


--------

https://www.youtube.com/watch?v=NWqy_b1OvwQ

Geoffrey Hinton: Unpacking The Forward-Forward Algorithm



--------

Not entirely sure: GPT-4 is still inventing stuff, fixing it may be harder than they admit - for example you can still trick an ML vision model with a random image. (also: there must be *some* FUD in the marketing...) See:

https://www.youtube.com/watch?v=SjhIlw3Iffs&t=1068s

--------
https://philbooth.me/blog/nine-ways-to-shoot-yourself-in-the-foot-with-postgresql

Postgress perfrmance tips
    
    - it has work_mem parameter (size of memory reserved for a DB query)
    - if it's too small it starts paging ON EACH QUERY (OMG)
    - says the following formula does the approximation:

        work_mem = ($YOUR_INSTANCE_MEMORY * 0.8 - shared_buffers) / $YOUR_ACTIVE_CONNECTION_COUNT 


--------

https://www.infoq.com/news/2023/04/virtual-threads-arrives-jdk21/

JEP 444: Virtual Threads Arrive in JDK 21
    
    - idea is to add co-thread to JDK (now golang is lossing it's big advantage over JDK - goroutines?)

    - JEP444 is now 'Proposal' status in JDK21 (used to be candidate in JDK20)

    - 'The JDK can now run up to 10,000 concurrent virtual threads on a small number of operating system (OS) threads'
    
    - 

-----

Python turtle package on OSX

    - install python via brew

    brew install python3

    - get version
    
    /opt/homebrew/bin/python3 --version
    Python 3.11.4

    - install python tkinter binding for the same version!

    brew install python-tk@3.11
