g--progress=plainit tricksp#

        Get all commits by user (that can be useful during performance reviews...)

        # select all commits done by user
        git log --author=<user name that appears in commit messages>

        # get the commit message of all commits by user.
        git log --author=<user name that appears in commit messages> --format=%s 

        # in case that the first string is the name of the issue - get all the issues that the user did.
        git log --author=<user name that appears in commit messages> --format=%s | awk '{print $1}' | sort | uniq

OSX hacks

? built-in something is very different from what you expect ?

    # very true for sed - will install gsed 
    brew install gnu-sed

    gsed -i 's/blabla/blublu/g' foo.txt

    # to set it up as sed - instead of gsed (still figure the path)
    brew install --default-names gnu-sed

? force quit an application

    command + option + escape - brings up a 'task manager'

    'Activity monitor' - a better 'task manager' that shows cpu utilization, etc. (or use top)

? can't install an old php version - because it's not supported by brew ?

    brew install shivammathur/php/php@7.4

    this installs it based on https://github.com/shivammathur/homebrew-php (got a list of supported versions).

? Always show the scroll bar ?
    In the Menu bar, click Apple Menu > System Preferences.
    Click General.
    Next to the "Show scroll bars" heading, select "Always."

? Mirror displays - on Monterey ?
    
    Apple > System Preferences > Displays > select Display settings > click on Use as: "Choose Mirror As"

? who is listening on port ?

    # on Mnterey (from here: https://stackoverflow.com/questions/4421633/who-is-listening-on-a-given-tcp-port-on-mac-os-x )

    sudo lsof -iTCP -sTCP:LISTEN -n -P

? osx - find all executable blabla files (not symlinks ?

    find . -perm +111 -type f  -name blabla

? osx - find all executable blabla files or symlinks ?

    find . -perm +111 -type f -or -type l -name blabla

? sort processes by virtual memory consumption /low memory does strange things on the mac.../

    top -o mem 

? regular expression editor (online) ?

    https://regex101.com/

? show process tree (on both osx and linux) ?

    htop -t

? netstat - show process names, who is listening on a given port ?

    sudo lsof -iTCP -sTCP:LISTEN -n -P
    sudo lsof -iUDP -sTCP:LISTEN -n -P

? disk space
    
    Show disk usage:
        df -a -H

    Show disk usage in directory
        du -ah  /usr/ | tail -1


    ? https://apple.stackexchange.com/questions/267165/why-is-devfs-full    
        devfs - is always full

? show memory usage in friendly terms

        # option -h is for Human friendly output !!!
        free -h


? http server with nc

    in bash shell:
        while true; do (echo -e 'HTTP/1.1 200 OK\r\n'; echo -e "\n\tMy website has date function" ; echo -e "\t$(date)\n") | nc -l 8081; done
        curl http://localhost:8081/


? ssh access keys ?

    # generate the keypair. (entery empty keyphrase
    ssh-keygen -t rsa -b 4096 -f id_rsa -C "mmoser@perforce.com"
    # you get id_rsa (private key) id_rsa.pub (public key)

    # put the public key in authorized keys
    cat id_rsa.pub >>${HOME}/.ssh/authorized_keys

    # check that permission is 0600
    stat $HOME/.ssh/authorized_keys

    # save the keys somewhere in your reach. (id_rsa and id_rsa.pub)

    # write down the ip admin address - (gcp tells that in the console), hostname and user $USER

    # can connect with 
    ssh -i id_rsa <user>@<admin_ip>

? install docker on ubuntu ?  https://docs.docker.com/engine/install/ubuntu/

    sudo apt-get update -y
    sudo apt-get install -y \
        ca-certificates \
        curl \
        gnupg \
        lsb-release

    # docker gpg key    
    sudo mkdir -p /etc/apt/keyrings
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

    # set up the repository:
    echo \
 "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \

    sudo apt-get update -y
    sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

    # make current user able to access docker 
    sudo setfacl -m user:$USER:rw /var/run/docker.sock

    #check that it is alive from non-root user
    docker run hello-world
    docker ps -a
    docker container prune -f






? tcpdump ?

    # just show the packets on screen (without payload)
    sudo tcpdump -i any port 9010

    # -X adds payload
    sudo tcpdump -X -i any port 9010

    # dump packets into pcap file (can view them in wireshark, or read the file with tcpdump)
    sudo tcpdump -i any port 9010 -s 65535 -w out.pcap


? wireshark ?
    don't install via brew (goes wrong) - there is an installer on https://www.wireshark.org/


Linux

    # get netstat on ubuntu
    apt-get install net-tools

    # fedora: they all call that package net-tools!
    dnf install net-tools

    # get external ip address
    dig +short txt ch whoami.cloudflare @1.0.0.1

    #get external ipv6
    dig -6 TXT +short o-o.myaddr.l.google.com @ns1.google.com


OS agnostic

    # also gets external ip address
    curl ifconfig.me
    curl icanhazip.com
    curl ipecho.net/plain
    curl ident.me
    curl bot.whatismyipaddress.com
    curl https://diagnostic.opendns.com/myip
    curl http://checkip.amazonaws.com
    curl http://whatismyip.akamai.com

======
Linux

ALPINE - list all installed packages

    apk info -vv

=======
Ansible

    https://www.digitalocean.com/community/tutorial_series/how-to-write-ansible-playbooks

Terminology:
    Playbook: yaml file that defines the script of what is being done. Includes a sequence of Tasks (unit of execution)

    Modules: reusable entities for doing common tasks

    Tasks: you write these to specify what the playbook is doing.

Examples:

    Playbook yaml that sets up variables, the first task prints them ('hosts: all' - the task act on all defined 'hosts' - these are machines that we can control)

        ---
        - hosts: all
          vars:
            - username: sammy
            - home: /home/sammy   
          tasks:
            - name: print variables
              debug:
                msg: "Username: {{ username }}, Home dir: {{ home }}"


======

Get all artifact names for a public repository using the GITHUB rest api

    USER="robxu9"
    REPO="bash-static"

    curl -L -s -H "Accept: application/vnd.github+json" https://api.github.com/repos/${USER}/${REPO}/actions/artifacts | jq '.artifacts[].name' | sort | uniq

Get names of all artifacts that are not of expired state

    curl -L -s -H "Accept: application/vnd.github+json" https://api.github.com/repos/${USER}/${REPO}/actions/artifacts | jq --raw-output '.artifacts[] | select(.expired==false) | .name'


Get the latest release for a public repository

    curl -L -s  -H "Accept: application/json" https://github.com/${USER}/${REPO}/releases/latest | jq --raw-output .tag_name
======

Kubernetes:
    - extracting interesting fields from a set of pods:

         An overview of what the containers in pods are running: (name,image,command,arguments of commands, environment)

         kubectl get pods -o json | jq '.items[] | {name: .metadata.name, image: .spec.containers[].image, command: .spec.containers[].command, args: .spec.containers[].args, env: .spec.containers[].env }'


    - https://github.com/ahmetb/kubectx 
        set kubectl context (kubectx) and namespace (kubens) /makes it easier to work with big clusters that have many of them/


=======

Show logs of all pods (name of pod/container is in each line) - and grep through them

    kubectl get pods | tail -n +2  | awk '{ print $1 }' | xargs -I{} kubectl logs --all-containers=true --ignore-errors=true --prefix=true {} | grep -i error

    kubectl get pods | tail -n +2  | awk '{ print $1 }' | xargs -I{} kubectl logs --prefix=true {} | grep -i error


======

Docker reference

https://docs.docker.com/engine/reference/commandline/docker/

---

How to build with the Dockerfile in the current directory

    docker build --progress=plain --no-cache . -t resulting-image-tag
       --progress=plain : show output on the screen! old-style! you see what is happening!
       --no-cache       : don't use the cache to pick up something old
       .                : current dir
       -t blalba        : tag the resulting image as blabla

---

docker image ls     :: shows docker images (except for intermediate images)
                    :: what is an intermediate image? Each step in docker build creates an intermediate images (these are cached to improve build time!)

docker image ls -a  :: shows all docker images (including intermediate images)


    docker image ls
        REPOSITORY               TAG       IMAGE ID       CREATED        SIZE
        docker/getting-started   latest    157095baba98   5 months ago   27.4MB


    # you have more fields in the json output (no idea what they all mean).
    docker image ls --format='{{json .}}'
        {"Containers":"N/A","CreatedAt":"2022-04-11 18:25:34 +0300 IDT","CreatedSince":"5 months ago","Digest":"\u003cnone\u003e","ID":"157095baba98","Repository":"docker/getting-started","SharedSize":"N/A","Size":"27.4MB","Tag":"latest","UniqueSize":"N/A","VirtualSize":"27.37MB"}



---
running a container - with a given image (if image is not present then they try to pull it from the docker registry)

    docker run fedora:latest ls / 

        - run $(ls /) command in the container, and print the output of the command.
        - the command exits and the container is no longer running

            docker ps -a    
            CONTAINER ID   IMAGE           COMMAND   CREATED              STATUS                          PORTS     NAMES
            d35b0813ce79   fedora:latest   "ls /"    About a minute ago   Exited (0) About a minute ago             eager_mcclintock

        - the entry of the exited docker container remains - until it is cleaned up by running $(docker container prune -f) 

    docker run --rm fedora:latest ls /  

        - same as before, just that the entry for the exited docker container gets cleaned up, when the docker exits. (--rm says so)


    docker run -d fedora:latest ls /
8bd31381ae93712414e02c58589a8562be9739ccc01d0d9baf93f39a6ab32505

        -d option says not to show the docker command output to screen, instead it prints the id of the docker container. You can now use that id to stop/resume/kill the container (or to $(docker wait CONTAINER_ID) until it exits)


    docker run -d -p 9000:8000 -v $PWD:/mnt/loc fedora:latest /bin/sleep infinity

        -d                  : runs as daemon (you get the docker container id in stdout)
        -p 9000:8000        : maps tcp port 9000 (outside the container) to port 8000 (inside the container)
        -v $PWD:/mnt/loc    : mounts current directory to /mnt/loc inside container

        /bin/sleep infinity : the container does nothing, just keeps running (but you can attach a terminal to it that runs in the container, and it enjoys the mapped port and mounted directory!)

        ## !!! sleep infinity - works on Linux, this is a GNU extension. !!!
---

Running a container, and overriding the entry point of the container (--entrypoint can tell what to run, provided the process is installed on the image)

    docker run  -it --entrypoint /bin/sh php:7.4-cli

In addition: mount the current directory on the host into the container - to path /mnt/loc ; ls /mnt/loc - just like the local directory when running the command!

    docker run  -v $PWD:/mnt/loc -it --entrypoint /bin/sh php:7.4-cli

You can run the whole thing in detached mode ! (and attach to it later)
This has the advantage that the docker keeps running, until the system is rebooted. (you can get a similar effect with tmux :- ) 

    docker run -d  -v $PWD:/mnt/loc -it --entrypoint /bin/sh php:7.4-cli


---

docker exec -it <container name|container id> /bin/sh   :: The classic: running a shell in a running container 

[Docker guides] See:  https://docs.docker.com/get-started/overview/) It's all hidden under: Running your app in production / Configure containers 


----

docker ps     ::: shows only running containers
docker ps -a  ::: shows running and stopped containers !!!

docker container ls ::: exactly the same as docker ps 

Docker commands can display it's stuff as json!!

    docker ps --format='{{json .}}'
        {"Command":"\"/docker-entrypoint.â€¦\"","CreatedAt":"2022-10-08 07:45:28 +0300 IDT","ID":"f99693d80146","Image":"docker/getting-started","Labels":"maintainer=NGINX Docker Maintainers \u003cdocker-maint@nginx.com\u003e","LocalVolumes":"0","Mounts":"","Names":"keen_darwin","Networks":"bridge","Ports":"0.0.0.0:80-\u003e80/tcp","RunningFor":"6 minutes ago","Size":"1.09kB (virtual 27.4MB)","State":"running","Status":"Up 6 minutes"}

-----

## moving a docker image to another machine

    docker save FULL_SHA -o tarfile.tar

    /COPY THE TARFILE.TAR to the other machine/

    docker load FULL_SHA -i tarfile.tar

    /Note that it often doesn't move the TAGS!!! so you need to tag the resulting image on your own/

    docker tag FULL_SHA tag_name


## shows properties of the docker engine

docker info         

## shows more stuff in json output (again, no idea what they all mean) 

docker info --format='{{json .}}' 

-----

### shows container logs (shows both stdout and stderr)

docker logs <container name or id>  

### shows logs with timestamp

docker logs  --timestamps <container name or id> 

### shows both stdout and stderr (they need to be sorted by timestamp, otherwise things get mixed up)

docker logs  --timestamps <container name or id>  2>&1 | sort -k 1  

### !!! Attention !!! docker logs writes to both STDOUT and STDERR. Error info is supposed to be written to stderr!!! ###

-----
## spills out details of the object. (all different format, depending on object type)

docker inspect <container-id>|<container-name>|<image-id>|<image-name>      


# You can extract several fields from the result!

docker inspect --format='{{json .Id}} {{json .Config.Cmd}}' docker/getting-started
    "sha256:157095baba98513dfef4ea00423767d8dae10edfeb629e9d39ea456e53f51e6a" ["nginx","-g","daemon off;"]

# strings in the template are shown as is (between the {{...}} query construct)

docker inspect --format='{{json .Id}} <=> {{json .Config.Cmd}}' docker/getting-started
"sha256:157095baba98513dfef4ea00423767d8dae10edfeb629e9d39ea456e53f51e6a" <=> ["nginx","-g","daemon off;"]


Listing all tags/info for an image:

    https://www.googlinux.com/how-to-list-all-tags-of-a-docker-image/

    curl 'https://registry.hub.docker.com/v2/repositories/library/debian/tags/'|jq . 


------

docker swarms  - they have their own cluster thing (i think docker tries to be some alternative to Kubernetes; that's more than docker-compose - which runs on a single node only)


--------
Get info on a specific image and tag - from dockerhub (including the os/platforms of all possible images)

    docker manifest inspect php:8.2-apache

------

PORTS

    - you can expose ports from a container -  that does nothing (it's a form of documenting which ports will be listened at within the container)
        - either by having EXPOSE in the Dockerfile (during build)
        - $(docker run --expose <port> )

    - to access a daemon running within the container: you need to map the port from outside the network to a port within the network  (docker run -p 8000:80)

    - you can do $(docker run -P .... ) - this will create a mapping between all 'exposed' ports to a temporarily assigned port number outside the network.

------

Trying to run a subprocess $(docker exec -ti <docker_id> /bin/bash) and passed the process pipes for the stdin/stdout/stderr streams.
Got error "the input device is not a TTY".
 
Now there i a lower level docker REST API! https://github.com/docker-php/docker-php 


https://docs.docker.com/engine/api/sdk/examples/ 
    For $(docker ps)
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json`
That means write raw HTTP to a unix domain socket, genius!


https://stackoverflow.com/questions/53781590/how-to-use-docker-api-engine-to-exec-cmd-in-containera

    - first create an "exec instance"
        POST /containers/{{id/name}}/exec
        {
            "AttachStdin": true,
            "AttachStdout": true,
            "AttachStderr": true,
            "DetachKeys": "ctrl-p,ctrl-q",
            "Tty": true,
            "Cmd": [
            "bin/bash","-c","touch appa.py"
            ],
            "Env": [
            "FOO=bar",
            "BAZ=quux"
            ],
            "Privileged":true,
            "User":"root"
        }


    = messages received after upgrade

        ```go
        header := [8]byte{STREAM_TYPE, 0, 0, 0, SIZE1, SIZE2, SIZE3, SIZE4}
        ```

        `STREAM_TYPE` can be:

        - 0: `stdin` (is written on `stdout`)
        - 1: `stdout`
        - 2: `stderr`

        `SIZE1, SIZE2, SIZE3, SIZE4` are the four bytes of the `uint32` size
        encoded as big endian.

        Following the header is the payload, which is the specified number of
        bytes of `STREAM_TYPE`.


    - then 

----
Mapping some docker commands to the docker engine rest api

Beware! The json output of curl vs docker commands is very different!

    docker ps --format='{{json .}}

    # doesn't show all containers (only running ones)
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json | jq . | less

    # show all containers
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json?all=true | jq . | less

    # show all containers and the size of containers field.
    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/containers/json?size=true\&all=true


    ---

    docker image ls -a --format='{{json .}}'

    curl --unix-socket /var/run/docker.sock http://localhost/v1.41/images/json?all=true\&digest=true | jq .

---

Docker layered file system

https://jessicagreben.medium.com/digging-into-docker-layers-c22f948ed612#:~:text=What%20are%20the%20layers%3F,during%20the%20Docker%20image%20build. 

========================

on OSX and linux - the same output for:

    docker info --format='{{json .Runtimes}}' | jq .
    {
      "io.containerd.runc.v2": {
        "path": "runc"
      },
      "io.containerd.runtime.v1.linux": {
        "path": "runc"
      },
      "runc": {
        "path": "runc"
      }
    }


====================

https://www.mongodb.com/docs/manual/reference/command/

Mongo (just like Elasticsearch) is a document oriented database. 


Mongo DB - holds a set of "collections"
    DB identified by name 
        - may not have following chars in it:  /, \, ., ", *, <, >, :, |, ?, $, space, \0, 
        - limited to 64 chars length
        - case sensitive

Mongo "collection" - similar to SQL table (but you don't have to define a schema in Mongo).
     - "collection" identified by its name ( "" is not a valid collection name, no $ chars or \0 chars in the name; may not start with prefix "system.")
     - can store very different json documents in a collection, but that's not a good idea!
     - Recommendation: store json docs of similar structure in the same collection (this allows you to do search)
        
    
A "document" in Mongo is an entry in a "collection" (it's a JSon file)- equivalent to a db "Row"; 
    - each "document" gets it's generated special _id field (12 bytes long - binary value)
    - id structure: 
            [4 bytes timestamp]
            [4 bytes - machine id]
            [2 bytes - pid]
            [3 bytes - incremented counter]
    - id field is always indexed, so you can lookup a document by it's id value!

Didn't know that mongo-db is used for search; so in what sense there is a equivalence with ElasticSearch ?

        - https://cloud.netapp.com/blog/cvo-blg-elasticsearch-vs-mongodb-6-key-differences

            - inserts are faster in mongo,  (mongo is written in C++, whereas elasticsearch is java)
                - better suited for document structured data (? don't you have a schema in elasticsearch too?)
            - text search is faster in elasticsearch

- mongo daemon (by default it listens on port 27017) 
  HTTP admin UI is listening port + 1000 (default: 28017)

- mongo shell: can enter expressions in json - that's very natural, as you can look at json documents as javacript maps/arrays. 
    - the document _id is represented as ObjectId class,.

    (or you can use alternative GUI based clients - like "Robo 3T")

    - shell commands
        var coll = db.getCollection("collectionName");  // lookup of collection object; or you can do as db.collectionName - but get an error if hte collection does not exist.
        
        - collection thing has all sorts of manipulation methods 
                - coll.insert({"person","Bob", "profession" : "Drummer"})  // insert doc, _id field is added upon insertion
                - var existingDoc = coll.findOne({"person" : "Bob"});    // find one doc, given a field
                - coll.update({"person":"Bob"}, { '$set' : { "profession":"Teacher" }}) ;  // first argument: query for doc like this, second argument: how to change the entry, once it was found. 
                - coll.find({}) // find all of them
    
        - update expressions can have all sorts of operators,
                $set - set the value of a field

                $inc, 

                $rename (rename field) 

                $push (adding to array)
                    coll.update({"person","Aob"}, {"$push", {"children" : { "Andrew" } } });

                $addToSet - like push, but prevents duplication of entries

                {$pop : { "key": 1}} - remove element from beginning of array "key", {$pop : { "key": -1}} - remove from the end 
                
                $pull - remove matching documents

                $each - push more than one entry (in an array)

                $slice - limit the size of an array to maximum n elements (or remove n - if n is negative)
                
                $sort - sort array by given key entry (provided that array has objects where all objects have the given key attribute)

            https://www.mongodb.com/docs/manual/reference/operator/update/
            https://www.mongodb.com/docs/manual/reference/operator/

        - find queries can have a lot of these (that one tries to be similar to sql SELECT...)  https://www.mongodb.com/docs/manual/reference/operator/query/    

    - Example session:

mongo 

> c=db.createCollection("tst")
{ "ok" : 1 }

> c=db.getCollection("tst")

> c.update({_id: 123}, { '$addToSet': {'a': 1, 'b': 2, 'c' :3} }, { 'upsert': 'true'})
WriteResult({ "nMatched" : 0, "nUpserted" : 1, "nModified" : 0, "_id" : 123 })

> c.find({_id: 123})
{ "_id" : 123, "a" : [ 1 ], "b" : [ 2 ], "c" : [ 3 ] }

> c.update({_id: 123}, { '$addToSet': {'d': 4, 'e': 5, 'e' :5} }, { 'upsert': 'true'})
WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })

> c.find({_id: 123})
{ "_id" : 123, "a" : [ 1 ], "b" : [ 2 ], "c" : [ 3 ], "d" : [ 4 ], "e" : [ 5 ] }

    - mongo operatator reference (very important link) https://www.mongodb.com/docs/manual/reference/operator/

    - now mongo is not the real shell (old and obsolete. use mongosh !!!!
        (installation instructions: https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-os-x/ )

    - in mongosh every map on a collection will return a Cursor object. You can convert it into a javascript array with toArray (big difference!!!)
        - but toArray() will take a lot of time - the cursor is evaluated just as it is needed, whereas toArray() will walk the whole query!!!

        - (i had to do it, when translating an index from one db into another one, they were using the same index field values, so I had to get them all...)

mongosh
    working with really large datasets

    - makes sense to save a result to file.
        fs.writeFileSync('file_big_array.json', JSON.stringify( variable_you_want_to_store ) )


    - reading the file back (in the old shell: mongo)
        a=cat('file_big_array.json')
        big_arr = eval(a)

    - in the new shell
        a=fs.readFileSync('file_big_array.json').toString()


aggregation queries: (that's what they do for 'join')
    
https://www.mongodb.com/docs/manual/core/aggregation-pipeline/#std-label-aggregation-pipeline

==========================

Reddis
    - does all sort of stuff (not just key value storage), 
        - they do work queues, and locks (with a specified time to life period). /what about reliability when usingg the work queue thing?/

        - https://github.com/resque/php-resque - that's a php library on top of redis lists (port of ruby library), Requests are queued as json objects. 
            Library has as a queue runner that listens on the list for jobs (maintained by github)

    
===========================

https://en.wikipedia.org/wiki/WebSocket

    - bi-directional communication
         https://www.rfc-editor.org/rfc/rfc6455 - the doc

    - initially over http, then client sends header: 'Connection: Upgrade' 'Upgrade: websocket' + additional headers, in order to change from http to websocket protocol.

        Request:

            GET /chat HTTP/1.1
            Host: server.example.com
            Upgrade: websocket
            Connection: Upgrade
            Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
            Origin: http://example.com
            Sec-WebSocket-Protocol: chat, superchat
            Sec-WebSocket-Version: 13

        Response:  Sec-WebSocket-Accept value is some sha signature over the request value Sec-WebSocket-Key + <some uuid>
    
            HTTP/1.1 101 Switching Protocols
            Upgrade: websocket
            Connection: Upgrade
            Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=

 
        Further requests in WebSocket are in frames

        - php Ratchet: 

            // bad news: you can't just upgrade a http connection of you php web request handler. You need to make a standalone script that listens to incoming WebSocket request.

            // simple chat server (that's how they do event driven stuff)

            $server =  Ratchet\Server\IoServer::factory(     // creates listening socke0wwt and event loop wrapper (IoServer), 
                                                             // the connect events are handled and create IoConnection)
                new Ratchet\Http\HttpServer(                 // parses http messages             
                    new Ratchet\WebSocket\WsServer(          // parsing of websocket handshake and frame messages 
                        new Chat()                           // application level class that handles messages
                    )
                ),
                8080
             );
             $server->run(); // run event loop.

            // server side action goes here
            class WsServer implements HttpServerInterface {

                
                // ComponentInterface - the application that consumes messages
                public function __construct(ComponentInterface $component) {

                public function onOpen(ConnectionInterface $conn, RequestInterface $request = null) {

                    // handle handshake
                    \Ratchet\RFC6455\Handshake\ServerNegotiator - handshake method ::: does the handshake.


        
        - Ratchet: working with sockets:
            
            Ratchet\Server\IoConnection - asynchronous socket handler
                - constructor receives derived class of \\React\\Socket\\ConnectionInterface

                - \React\Socket\Connection // wraps a stream/socket so that it does the ConnectionInterface!
                        public function __construct($resource, LoopInterface $loop)

        - every server passed to a ratched event loop handles:

            interface MessageInterface {

                function onMessage(ConnectionInterface $clientConnection, $msg);

----

    List of libraries for working with the docker engine api  : https://docs.docker.com/engine/api/sdk/

----

https://realpython.com/pytest-python-testing/ (and others)

    - standard library comes with unittest framework ( https://docs.python.org/3/library/unittest.html ) - very much like JUnit (with setUp and tearDown methods)

        - but they have pytest : 
        - here you don't have to import/derive from base class and call it's assertion methods; most simple test is a function with test_ prefix and calls assert
        - pytest is an exe, it's the test runner - it gathers files with test_ prefix and calls all the test.

    - so how do I set up the test?
        - test function has parameter, the parameter is a function that is declared with @pytest.fixture decorator ; FIXTURE MEANS it produces a value used for the test.

        import pytest

        @pytest.fixture
        def example_fixture():
            return 1

        def test_with_fixture(example_fixture):
            assert example_fixture == 1

    - now functions with the @pytest.fixture annotation can also have parameters, these are most probably nested fixtures, so they call themselves in turn upon the setup of the value that is returned by the fixture.

    - great, now how do I tear down the test? (here things get complicated - you tear down the fixture!) generators!

      One way of tearing the fixture down.

        import pytest

        @pytest.fixture()
        def resource():
            print("setup")
            yield "resource"
            print("teardown")

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

        
    Another way: register a callback!

        @pytest.fixture()
        def resource(request):
            print("setup")

            def teardown():
                print("teardown")
            request.addfinalizer(teardown)
            
            return "resource"

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

    - lifetime of a fixture return value.

        When you use a fixture in several tests, then the returned value may be cached (similar to unittest - you don't want to have a setUp per test!)
        Now the value returned by the fixture has a lifetime, depending on the scope parameter of the fixture annotation!!!

        
        # scope="function" - if you use the resource parameter as a fixture, then the nested teardown method will be called FOR THE LAST TEST OF THIS SOURCE FILE!
        # scope="session"  - teardown called after the end of the run of the pytest test runner !
        # scope="module"   - teardown called after the last test in the current module
        # scope="package"  - teardown called after the last test in the current package.

        !!! BECAUSE OF ALL THE CACHING: YOU CAN'T INVOKE A FIXTURE TWICE FOR A TEST !!!
       
        @pytest.fixture(scope="function")
        def resource(request):
            print("setup")

            def teardown():
                print("teardown")
            request.addfinalizer(teardown)
            
            return "resource"

        def test_that_depends_on_resource(self, resource):
            print("testing {}".format(resource))

    - can parametrize tests. Each entry in the array will be passed in a separate test run of test_in_palindrome
      YOU ALSO PASS PAREMTERS TO FIXTURES THIS WAY !!!

        @pytest.mark.parametrize("palindrome", [
            "",
            "a",
            "Bob",
            "Never odd or even",
            "Do geese see God?",
        ])
        def test_is_palindrome(palindrome):
            assert is_palindrome(palindrome)
                
    - multiple parametrize annotations: will result in all combinations (cartesian product) of all test values!
        
            @pytest.mark.parametrize('foo', ['a', 'b', 'c'])
            @pytest.mark.parametrize('bar', [1, 2, 3])
            def test_things(foo, bar):
                assert foo in ['a', 'b', 'c']
                assert bar in [1, 2, 3]

      test report now looks as:

            test_foo.py::test_things[1-a] PASSED
            test_foo.py::test_things[1-b] PASSED
            test_foo.py::test_things[1-c] PASSED
            test_foo.py::test_things[2-a] PASSED
            test_foo.py::test_things[2-b] PASSED
            test_foo.py::test_things[2-c] PASSED
            test_foo.py::test_things[3-a] PASSED
            test_foo.py::test_things[3-b] PASSED
            test_foo.py::test_things[3-c] PASSED


-----
BNF grammars 

    Python got it's syntax in the reference
        https://docs.python.org/3/reference/grammar.html

    BNF grammars for javascript?

    You got a lot of them in ANTLR!
        - https://github.com/antlr/grammars-v4/tree/master/javascript/javascript

    C syntax BNF
        - https://cs.wmich.edu/~gupta/teaching/cs4850/sumII06/The%20syntax%20of%20C%20in%20Backus-Naur%20form.htm

    Pascal syntax BNF
        - https://condor.depaul.edu/ichu/csc447/notes/wk2/pascal.html

===

Javascript - iterating over map entries (node session)

    > a={a:1,b:2,c:3}
    { a: 1, b: 2, c: 3 }
    > Object.entries(a).map(([k,v],i) => { console.log("k: " + k + " v: " +v + " i :" +i); return k } )
    k: a v: 1 i :0
    k: b v: 2 i :1
    k: c v: 3 i :2
    [ 'a', 'b', 'c' ]

Javascript generators/iterators - they are needed to provide values for a 'for' loop.

Here is how you do an iterable object (that's an object that has special function to return an iterator, so that it can be used in a for loop)

    class Foo {
         constructor() {
             this.data = [1,2,3];
         }
    }

    //
    // class Foo is 'iterable' if it has a member named Symbol.iterator - that returns an iterator object.
    //
    // need to add the function named as special symbol: Symbol.iterator to protoype
    // can't just define a member function with that name ???!
    // now a new Foo object will bet 'iterable' as it has an iterator.
    //
    Foo.prototype[Symbol.iterator] = function() {
         let pos = 0;
         let data = this.data;

         // iterator is an object with the next function, that returns an object upon each call
         // returned object/dict has done: - are we done iterating? and value: - value to return to for loop
         return {
             next: function() {
                     return {
                         done: !(pos in data), // are we done with the iteration?
                         value: data[pos++]    // value returned by iterator
                     }
             }
         }
     }


    f = new Foo();
    for(let c of f) {
        console.log(c);
    }

Here is how you do a generator

    function *range(from,to,step) {
        let val = from;
        while(val < to) {
            yield val;
            val += step;
        }
    }

    for(let i of range(1,10,2)) {
        console.log(i);
    }

Calling generators directly

    > a=range(1,3,1)
    Object [Generator] {}
    > a.next()
    { value: 1, done: false }
    > a.next()
    { value: 2, done: false }
    > a.next()
    { value: undefined, done: true }
    > a.next()
    { value: undefined, done: true }


Generators can call other generators - but they have to do the call via yield* 

    function *blabla() {
        return 3;
    }

    function* func1() {
      yield 42;
    }

    function regFunc() {
        return 43;
    }

    function* func2() {

        // can call regular function from generator - works as expected
        let bl = regFunc();
        console.log(bl);

        // calling a generator function returns a generator object
        let er = blabla();
        console.log("blabla returns: " + er);

        // this way it returns 3
        er = yield *blabla();
        console.log("blabla returns: " + er);

        yield* func1();
        yield* func1();
        yield* func1();
    }

    for(a of func2()) {
        console.log(a);
    }


It turns out that python has a similar thing to delegate form one generator to the other. Welcome to 'yield from' - beginning with python 3.8

    def my_range(f,t,st):
        while f < t:
            yield f
            f += st

    # yield from - to delegate to another generator !!! (from python 3.8)
    def range_three(f,t):
        yield from my_range(f,t,3)

    for n in my_range(1,10,1):
        print(n)

    print("generator calling generator")

    for n in range_three(1,10):
        print(n)


===

    javascript express for http server:

        const express = require("express"),
              app = express()

        app.get("/", (req, res) => {
            res.send("Hello, TREND OCEANS!")
            console.log("finished handling request");
        })

        console.log("before listen");
        app.listen(3000, console.log(`Server started on port 3000`))
        console.log("after listen");

    output:

        node t1.js
        before listen
        Server started on port 3000
        after listen
        finished handling request

=== 

    javascript sleep:

        function onTm() {
            console.log("onTm");
        }

        console.log("before sleep");
        setTimeout(onTm,1000);
        console.log("after sleep");

    output:
        node t.js
        before sleep
        after sleep
        onTm

====
http in node:
    node has in-built http module
        

    express - server
        http://expressjs.com/en/5x/api.html


====

NodeJS - event loop

    https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/

    Phases of the event loop
        - timers: (timers are best effort - not exactly scheduled)
        

-----

npm - creating package

    - search if package exists in: https://www.npmjs.com/search?q=prs
    - npm init new-package-name  # makes a default package.json ( package.json reference: https://docs.npmjs.com/cli/v9/configuring-npm/package-json )

    - npm adduser  # use this to log into your npm account. before publishing: need to be logged into your account. 

    - set NPM_TOKEN environment variable to an npm account API token with publish privs.

    - npm publish --access public # from directory that has package.json - and all of the files mentioned in package.json !!!

    - each publish needs to increment the package version number (in package.json)

Show root folder where globally installed npm modules are:

    npm root -g 

install YAML package into the global dir
    npm i yaml -g

    To use globally installed package:

    To use locally installed package:
        need to set environment variable: 
        export NODE_PATH=$(npm root -g)

install YAML locally
    npm i yaml

    module is installed to $PWD/node_modules
    
    To use locally installed package:
        need to set environment variable: 
        export NODE_PATH=$PWD

If you are part of an installed package, then you don't need to do this!!! (can include anything in the current scope)


--- 

Viewing ssh logs:

With systemd

    journalctl -fu ssh

otherwise 

    sudo grep sshd /var/log/auth.log

In /etc/ssh/sshd_config put

    LogLevel VERBOSE


======

Formatting text in whatsapp messages

https://faq.whatsapp.com/539178204879377

    ```...```` - makes it fixed font.

======

Python virtual environments

    # create the virtual env
    python -m venv myvenv

    # activate the virtual env
    source myvenv/bin/activate
    #(prompt changes after that)

    # if there is a requirements.txt file:
    pip install -r requirements.txt

    # create requirements (after having installed some pip packages)
    pip freeze > requirements.txt

    Benefit: pip install puts up everything in /site/packages - shared thing, can get clogged up. This way every environment is kind of isolated.

    # deactivate virtual environment myvenv
    deactivate


Pip show installed package version of numpy
     pip show numpy


Show all packages
     pip list

Show all packages as a tree (which one requires which one)

   # need to install the thing
   pip install pipdeptree

   pipdeptree


=========

Workign with binary data.

(Perl and Python are similar to this respect - they pack/unpack the fields - according to some printf-like text spec (the spec is called 'the template')
Nodejs/Javascript doesn't do that - they have java like readUint8, writeUint8 functions for binary de-serialisation.


https://perldoc.perl.org/perlpacktut


==

GITHUB

Want to submit a pull request to some repository other than your own stuff?

    https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork

    - need to fork that repository first.


--------

AWS lambdas

    - tutorial: https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html


