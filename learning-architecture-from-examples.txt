https://www.youtube.com/watch?v=lG90LZotrpo

https://alexwlchan.net/a-plumbers-guide-to-git/

How GIT works UNDER the HOOD?

    every object in git:
        key:    sha1 hash over the content of the content (this identifies the object)
                This helps with distributed version control:
                    - other version control systems have version numbers 1, 2, 3 - here the identity is based on the data of the object, so the ids' won't overlap


        value:  object content (object data) - is stored zlib compresseed



 .git/objects - stores the object database

 .git/objects/<hexa value of first byte of sha1 hash>/<file named after remaining hexa value of sha1 hash>  - that's the file that holds object of given hash key   
    
    - you don't want to have all of the objects in one directory, accessing files in very big directory is sequential search, gets very slow for very big directories.
 
Git command types

    "Porcelain" commands - high level, user friendly commands
        like:
            git add/checkout/commit/branch 

    "Plumbing" commands  - low level commands, commands to directly manipulate the git object store.
        like:
            # show type of git object from sha
            git cat-file -t <sha> 

            # pretty print git object 
            git cat-file -p <sha>

Session:

    # create new git repo with name 'try' try.git/objects - contains all git objects.
    > git init try

    # watch changes of .git folder as they occur
    > fswatch  -d -r try/.git/

    > cd try
    > echo "ha" >ha.txt

    # add new file into 'stagin' area (a new BLOB object is created for the file)
    > git add ha.txt

    # file appears for the new BLOB object
    # (piece the numbers togather to get the object id - won't repeat that again and again)
    try/.git/objects/05/a98d59e6f083f194480efd00062071a01e54d8

    # show object type (blob)
    > git cat-file -t 05a98d59e6f083f194480efd00062071a01e54d8  
    blob

    # show object content - a blob object only stores the contents of a file, not it's name !!!
    # each revision of a file creates a complete BLOB objects that holds the whole file content (what about duplicated data?)
    # that's why git checkout is fast - as compared to SVN (where they have to get the top of the branch from applying all diffs for each files)
    # also that helps distributed colaboration

    > git cat-file -p  05a98d59e6f083f194480efd00062071a01e54d8
    ha

    # make a commit
    > git commit -am "first commit"

    # now what is created behind the scenes?

    # a tree object - that is listing the file name and referencing the blob object

    # the tree is always listing the top level directory of a git repository
    # a subdirectory of the top level directory will be represented by another tree object,
    # the tree referenced by a commit is therefore a SNAPSHOT - you can get the state of the branch by examining tree of the top level commit only !!!
    # (in git the checkout command is much faster than in SVN - where they have to replay the whole repo history in the process !!!)

    
    > git cat-file -t  4c464b23effe5c40149ccffb7aadc4afbf3b7738
    tree

    # the content of the tree object, each line defines a listed file/directory, the fields: 
        - file permissions
        - object type of entry
        - the content (for files this is a blob object, for directories that's another tree object)
        - the key of the referenced object
        - the name of the file system object

    > git cat-file -p  4c464b23effe5c40149ccffb7aadc4afbf3b7738
    100644 blob 05a98d59e6f083f194480efd00062071a01e54d8	ha.txt
    

    # if you have a repo with nested directories: $(git ls-tree -r <SHA OF TREE OBJECT>) will traverse all referenced sub trees/directories recursively
    # that's another plumbing command for you!
    git ls-tree -r  4c464b23effe5c40149ccffb7aadc4afbf3b7738

    # a commit object - that references the tree. (reminder: the tree is the snapshot of the current state of a branch - for a given point in the commit history)
    > git cat-file -t c64d35748b2b211f188e01149a538806fbb373f3
    commit

    # the commit object - references the tree. also has metada on commit (this metadata is displayed by git log)

    > git cat-file -t d9b480dd708c4635c31c76adea6f899989a573a8
    commit

    > git cat-file -p d9b480dd708c4635c31c76adea6f899989a573a8
    tree 4c464b23effe5c40149ccffb7aadc4afbf3b7738
    author mmoser-blaze <mmoser@perforce.com> 1693612392 +0300
    committer mmoser-blaze <mmoser@perforce.com> 1693612392 +0300

    first commit

    # a branch just points to the last commit.

    cat HEAD
    ref: refs/heads/main

    cat .git/refs/heads/main
    d9b480dd708c4635c31c76adea6f899989a573a8


    # lets add a second commit that changes the first file !
    echo  "aaa" >>ha.txt

    git add ha.txt

    # new blob appears for the new revision of the file ,  - new copy of the file in object store!
    
    git cat-file -t 1b3e02e5d3ddd7dc5b18511fa4be5479b7773a7f
    blob
    
    git cat-file -p 1b3e02e5d3ddd7dc5b18511fa4be5479b7773a7f
    ha
    aaa

    # adding a second commit
    > git commit -am "second commit"

    # again a tree object appears, which is referencing the blob, and listing it with it's file name
    > git cat-file -t 8cb4db8de90adf4ef1f344451c840d7d6af659f8
    tree

    > git cat-file -p 8cb4db8de90adf4ef1f344451c840d7d6af659f8
    100644 blob 1b3e02e5d3ddd7dc5b18511fa4be5479b7773a7f	ha.txt

    # and

    > git cat-file -t 9ec26f9793513e4cc6396e3ac3da45fab915e508
    commit

    # and the second commit object
    # !!! the second commit object has a pointer to the parent commit !!!
    #  - that's how git log command is following all of the commit objects
    #  - also note: each git object is immutable, and the key is defined only by the object content - it's the sha1 hash 
    #    - that's why git is distributed, each node can work independently, as they don't have a shared numbering schema that has to be coordinated (SVN/CVS revisions are numbered 1,2,3, so the server has to determine and merge them)
    #    - also: each git object is a merkel tree : it contains the hashes of refererred to objects (which are also immutable), so that they can't change underneath.
    #      In a way that is similar to the bitcoin blockchain !!!


    > git cat-file -p 9ec26f9793513e4cc6396e3ac3da45fab915e508
    tree 8cb4db8de90adf4ef1f344451c840d7d6af659f8
    parent d9b480dd708c4635c31c76adea6f899989a573a8
    author mmoser-blaze <mmoser@perforce.com> 1693613310 +0300
    committer mmoser-blaze <mmoser@perforce.com> 1693613310 +0300

    # and the branch is updated to point to the latest commit.
    > cat .git/refs/heads/main
    9ec26f9793513e4cc6396e3ac3da45fab915e508


---
Hussein Nasser - nice lectures you can listen to, while on the train.

POSTGRES SQL SERVER ARCHITECTURE

    There is a book on the subject: 
        
        https://edu.postgrespro.com/postgresql_internals-14_en.pdf
        Egor Rogov
        PostgreSQL 14 Internals

    https://www.youtube.com/watch?v=Q56kljmIN14 

    - data in DB tables is stored in tuple format, each tuple has a version.
    - upon update an new row with a new tuple version is being written (POSTGRES is append only).
    - the old versions will eventually be cleaned up by the VACUUM process

    Process structure:

        Main postgress: POSTMASTER process, it is listening for new database connection (and it starts all the other worker processes)
        
        Upon new connection a BACKEND child process is spawned (that services the DB connection) - this is to isolate processing of client connections from each other. The processes share a lot of stuff via shared memory.

        Per connection child processes does the query processing by default, if PARALLEL PLAN is enabled, then it passes processing of query to BACKGROUND WORKER process (there is a pool of them)

        SQL statement execution involves building an  execution plan, then running it. In any event the output is written to shared memory called WORK-MEM (no disk-IO yet)
            

        AUXILIARY PROCESSES - helper processes. 
            
            BACKGROUND-WRITER - has access to WORK-MEM shared memory. It writes the updated pages to file. (now OS write does not OS flush !!1)
                (OS write only updates the OS FILE SYSTEM CACHE (inside the OS), made persistent only upon flush)

            CHECKPOINTER - does OS flush, this creates a CHECKPOINT RECORD (all older data must be replayed, if necessary.

            LOGGER PROCESS - also get's it's data from shared memory

            VACUUM - pages that consist of obsolete tuples (which have been overwritten. (wow. Big potential for disk fragmentation?)

            AUTO-VACUUM - watcher or VACUUM workers

            WALL-ARCHIVER - backs up the WALL RECORDS (if a CHECK-POINT is earlier, them upon restart the entries in WALL_ARCHIE are applied to shared memory WORK-MEM shared memory)

            CHECK-POINTER - creates CHECKPOINT (that creates a record that we are consistent)

POSTGRES 
    Peter Eisentraut
    https://www.youtube.com/watch?v=OeKbL55OyL0&list=LLjHqZSJGoBvKUdYF6hw6xoA&index=2

    DB client libpq library 

        - general format of messages:
            [one letter request code]  [NUM-OF-BYTES] [data]

            short sqeuence of messages

                client2Server   Q [num] 'SELECT .... "
                server2Client   T [num] <tuple defines rows: field and field types>
                                D [num] <each row is a message>
                                Z [num] 

        - stages of processing request
            Parser  https://github.com/postgres/postgres/tree/master/src/backend/rewrite
                - build parse tree/Abstract syntax tree(AST) with flex/bison
                - Validation of AST: doe the table names exist? Do column names exist? Do the types of columns match?

            Rewrite https://github.com/postgres/postgres/tree/master/src/backend/rewrite

                - modify AST does stuff like expanding views

        - Planning Optimization - build an execution plan https://github.com/postgres/postgres/tree/master/src/backend/optimizer
            (EXPLAIN command shows a yaml of this - you put EXPLAIN before the SQL query in the PSQL shell)

            (EXPLAIN with analyse option - this does more than planning, it runs the query and collects statistics to show the cost of each step)

                Uses statistics gathered so far to build an optimal sequence of actions required to service the requst
                
                stages of execution plan actually do something with the tables, operations like 
                    Sort
                    Aggregate
                    Hash Join
                    Seq Scan 

        - execute the plan (prepared statements do the parsing and optimization stages, so that running a prepared statements starts with the execution stage)
            https://github.com/postgres/postgres/tree/master/src/backend/executor

            says it does a pipeline of stages, and tuples pass through the stages one by one (says there is an effort to batch these, so as to reduce cache misses)

        
            - Access - lower level to implement the execution stages https://github.com/postgres/postgres/tree/master/src/backend/access


             How do filters work in sequntial scan? Need to compare operators, let's say the want to process '>=' on int2 typed values.

               You can find the names of the function in the psql code base - the system catalog tables know this. (says plan processing does some kind of lookup like this!!!)
        
                # show all internal tables
                SELECT * FROM pg_catalog.pg_tables;

                # show all types
                SELECT * FROM pg_catalog.pg_type;

                #
                SELECT oid FROM pg_catalog.pg_type WHERE pg_type.typname = 'int2';
                 oid
                -----
                  21
                (1 row)


                postgres=# SELECT * FROM pg_catalog.pg_operator WHERE oprname = '>=' AND oprleft=21 AND oprright=21;
                 oid | oprname | oprnamespace | oprowner | oprkind | oprcanmerge | oprcanhash | oprleft | oprright | oprresult | oprcom | oprnegate | oprcode |   oprrest   |     oprjoin
                -----+---------+--------------+----------+---------+-------------+------------+---------+----------+-----------+--------+-----------+---------+-------------+-----------------
                 524 | >=      |           11 |       10 | b       | f           | f          |      21 |       21 |        16 |    522 |        95 | int2ge  | scalargesel | scalargejoinsel
                (1 row)

                # int2ge isa C function in the code base, that acts as operator to compare two integer fields.
            
                SELECT * FROM pg_catalog.pg_proc WHERE proname = 'int2ge';

                oid | proname | pronamespace | proowner | prolang | procost | prorows | provariadic | prosupport | prokind | prosecdef | proleakproof | proisstrict | proretset | provolatile | proparallel | pronargs | pronargdefaults | prorettype | proargtypes | proallargtypes | proargmodes | proargnames | proargdefaults | protrftypes | prosrc | probin | prosqlbody | proconfig | proacl
-----+---------+--------------+----------+---------+---------+---------+-------------+------------+---------+-----------+--------------+-------------+-----------+-------------+-------------+----------+-----------------+------------+-------------+----------------+-------------+-------------+----------------+-------------+--------+--------+------------+-----------+--------
 151 | int2ge  |           11 |       10 |      12 |       1 |       0 |           0 | -          | f       | f         | t            | t           | f         | i           | s           |        2 |               0 |         16 | 21 21       |                |             |             |                |             | int2ge |        |            |           |
(
                
        - storage management             
             file: <POSTGRESS DATA DIR>/base/<oid of DB>/<oid of table>
             file is divided into 8k pages/block (tuple data for DB rows is stored here), these are mapped to shared memory - .

          pgfiledump tool parses these raw database files.    


        - WAL logging, each update is changing the audit log (WAL) - if the thing crashes before the shared memory is synced to disk, then it restores the changes from the WAL audit log.   
          (function XLogInsert - writes to WALL)

          WAL is also used for DB replication.

---

https://www.youtube.com/watch?v=URwmzTeuHdk&list=LLjHqZSJGoBvKUdYF6hw6xoA&index=2

Locks in PSQL (reads out this doc: https://www.postgresql.org/docs/current/explicit-locking.html )

    General about locks (true for any locks)
        - shared locks (reader locks - multiple readers get it - but you can'tget exclusive lock while any one is active)
        - exclusive locks (writer locks - only one writer gets it, if no readers have it)
        
            - usually the DB must lock stuff implicitly, when it does something
            - turns out that postgress has a LOCK command to get these explicitly

    Postgress type of locks
    
        - advisory locks (application locks, this lock is represented by DB object, used to coordinate a whole transaction (or multiple transactions)

        Table locks - so many lock modes!!! (not listing them by order of severity, see document for that)

            ACCESS EXCLUSIVE  - DROP/TRUNCATE table - needs to be sure that no one is touching the table
                                  (highest level lock, can't get it if any other lock is active)
            EXCLUSIVE            REFRESH MATERIALIZED VIEW CONCURRENTLY - when the results of a completed query need to be changed from scratch (?)
                                  (second highest level lock)   
            ACCESS SHARED     - even SELECT needs it, to access an index (make sure the index disappears if table is dropped)
                                  (lowest level lock)

            ROW SHARED        - used with modes like FOR UPDATE (that's used when you make sure that no multiple rows are changed in a transaction, and you want to make sure they are not changed)

            ROW EXCLUSIVE     - on changing a row (UPDATE/DELETE/INSERT/MERGE need to take this lock) 

            SHARE UPDATE EXCL - VACCUM (not full) - here only pages where all records are superseeded by newer ones get freed (?)

            SHARE             - CREATE INDEX needs one (but not CREATE INDEX CONCURRENTLY - but that's a tricky option!!)

            SHARE ROW EXCL    - sometimes ALTER TABLE need it.

            Exclusive - if you want to drop/truncate/reindex table/create index/cluster VACCUM FULL (here the index is reordered, as existing blocks are reordered to clean out of old versions of records)
        
        Row level locks
            FOR KEY SHARE - lowest level lock.      A key-shared lock blocks other transactions from performing DELETE or any UPDATE that changes the key values, but not other UPDATE, and neither does it prevent SELECT FOR NO KEY UPDATE, SELECT FOR SHARE, or SELECT FOR KEY SHARE.

            FOR SHARE       - A shared lock blocks other transactions from performing UPDATE, DELETE, SELECT FOR UPDATE or SELECT FOR NO KEY UPDATE on these rows, but it does not prevent them from performing SELECT FOR SHARE or SELECT FOR KEY SHARE. (?)

            FOR NO KEY UPDATE ... This lock mode is also acquired by any UPDATE that does not acquire a FOR UPDATE lock (transaction stuff)

            FOR UPDATE    - highest level lock  ... acquired by any DELETE on a row, and also by an UPDATE that modifies the values of certain columns. 
        

        Page level locks
            also has read/write level locks per page. But the docks say 'don't wory - this is so low level'

-----
Hussein Nasser 

https://www.youtube.com/watch?v=_E43l5EbNI4

uber moving from postgress to mysql (that one happened in 2016 - quite some time ago)

    - say they use microservices, and use rdms for schemaless data format (? Why? Maybe they still needed some combination of ACID not provided by nosql?)

limitations given by uber in article:
    - on disk format
        in article example : they have lots of indexes (do they really need so many indexes?)
        P. writes a new tuple record for any modified record, os this results lots of index updates - so lots of locking on indexes after any record updates (meaning lot's of flushes on indexes)
        The WAL - write ahead log gets large (because of index updates)

    - replication problems
        says big WAL - write ahead log makes things bad, and P. does not have write ahead replication out of the box, so locks are retained for longer in PW.

    - consequences of postgres design
        SSD write amplification because lots of index writes (bad with SSD performance and longivity)

    - claim data corruption during replication of P:9.2
        ?

    - btree rebalancing (adds to write amplification)

    - no mvcc replication (multi version concurrency control)
        <didn't understand>
    
    - connections: P. keeps process per connection while M. spins up a thread
        he is right here, but you should not have lots of transient DB .connections in the first place.

    - upgrading postgres 
        it is a pain..


(rocks db - log structured merge tree - special to minizes writes, to keep SSD's alive )


---
Hussein Nasser

MONGODB ARCHITECTURE https://www.youtube.com/watch?v=ONzdr4SmOng
    
    Evolution of MONGODB storage engine - NOSQL (SCHEMALESS) DB

    - JSON records are called 'documents' and stored in 'collections'
    - each 'document' get's a unique id (globally unique: The first nine bytes in a MongoDB _ID guarantee its uniqueness across machines and processes, in relation to a single second; the last three bytes provide uniqueness within a single second in a single process)
        
    - first version: 

        - had a single btree index that maps the document id to a storage location. Storage location consists of two parts, first one identifies the storage file, second part is the index in the file (a bit like ISAM)
        - they used to have a single lock in the system !!!

    - second version
        - had a lock per collection

    - third version : wired tigre

        - had a lock per document/record (like almost everyone else)
        - adds compression of record prior to storage
        - hidden clustered indexes (clustered index is a btree index, where the leaf nodes of the index store the data of the records, also the leaf node blocks are linked in linked list - so finding a range of records by id is fast.
            - in a clustered index the btree index nodes are stored on disk, so no more in memory index???       
        
            - also they have a secondary index:
                - first record _id is mapped to internal recordId (64 bits) in it's B-TREE
                - the record _id is mapped to the record in it's clustered B-TREE index

                Why do they need a secondary index?
                    - the _id field is bigger than secondary index value (by default 12 bytes) - so they want to safe space.
                    - the _id field can actually be any kind of data, having that in the index is a problem
                
    
        - afteer 5.3 they did away with the hidden index, now the _id is mappped to record via one B-TREE (but you can still have the double index as an option) 



-----

https://www.youtube.com/watch?v=NymIgA7Wa78&list=LLjHqZSJGoBvKUdYF6hw6xoA

REDIS

    - REDIS is single threaded, tens of thousand requests per second (depending on data structure?)

    - is a kind of better MEMCACHED -  has replication built-in.
        MEMCACHED does o(1) set/get/lookup
        REDIS has more data structures (sorted sets, bit fields, queues), 
            for complex stuff that is more than O(1) there are upper bounds on performance
            has persistence options (configurable)
                - has optional append only journal file for changes.
                - options for FSYNC (flushing to disk)
                    - always (each op)
                    - once per second, (N seconds)
                    - no flushing - OS flushes each n-seconds (by cfg)
                 - RDB saves - current state saves (does fork of service, they do that on the second tiers only)

            Replication
                - can initiate replication to another node (so that given node can be taken off-line)
                - primary / secondary tiers, (primary for read/write - then stuff replicated to secondary, so that it stands buy to take over)
                    - problem: still no guaranteed consistency between tiers
                    - race conditions, if replication did not complete.
                
            REDIS cluster 
                - horizontally scalable (can add nodes),
                - has sharding
                    - each key is hashed, and divided module by 60383 to get hash slot.
                        (so they can ad nodes and partial replication, without rebuilding all of the shards)    
                        makes an effort to keep shards with more or less equal amount of keys stored 
                - fault tolerant (replication and hot swap between nodes)
                - no single admin node (the REDIS instances exchange configuration through gossip protocol)
                - need special client lib for cluster mode
                    - can be configured so that reads/writes go through master nodes only, then these master nodes are replicated to secondary nodes (for active standby)

        - latency issues: sometimes there are peaks (some operation take much longer) 


            - there is latency doctor tool in REDIS
            - redis cli option --intrinsic-latency - reports latencies (sort of like DB EXPLAIN)
                               --latency option: reports network latencies
            - redis cfg option: selectively log operations slower than N microseconds
                REDIS config slowlog-log-slawer-than 10000 
            
              they saw slow commands: 
                PSYNC - partial sync (not done often, done when a replicate subscribes to a master)
                CLUSTERSLOTS - was slow. This requests gets the layout, which range of cluster slots (HASH(KEY) % 60383) is serviced by which cluster node.

                   They had short lived client processes, so they need to do that often! 
                   also cluster can reconfigure itself, so that the cluster slot distribution changes and needs to be sent again.

                How they solved it: kept the cluster slots in some local cache accessible from PHP (between the runs of the application?)

        - rate limiting with REDIS - do it by using the 'leaky bucket' algorithm, Doing that on the client library is not atomic, so they uploaded a LUA stored procedure to REDIS, that does the counter updates and comparison in one step.      
            See https://www.linkedin.com/pulse/lua-script-redis-using-noderedis-vinayak-sharma/



                
        
        

            



            
